{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Plots and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, wilcoxon\n",
    "import pickle\n",
    "\n",
    "from calibration.calibrators import *\n",
    "from validate import calibration_metrics, validation_metrics\n",
    "from calibration.metrics import *\n",
    "from utils.plots import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "mode_metrics = 'quantile'\n",
    "mode_plot = 'uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model vs Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'vgggace': f'experiments\\\\VGGFace\\\\results_MCDP_50_0.5.pkl',\n",
    "    'ncnn': f'experiments\\\\NCNN\\\\results_MCDP_50_0.3.pkl',\n",
    "    'vitb32': f'experiments\\\\ViT_B_32_ENSEMBLE\\\\ensemble_10_results.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'MCDP_0.1': f'experiments\\\\ViT_B_32\\\\results_MCDP_50_0.1.pkl',\n",
    "    'MCDP_0.3': f'experiments\\\\ViT_B_32\\\\results_MCDP_50_0.3.pkl',\n",
    "    'MCDP_0.5': f'experiments\\\\ViT_B_32\\\\results_MCDP_50_0.5.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'ENSEMBLE 3': f'experiments\\\\{model_name}_ENSEMBLE\\\\ensemble_3_results.pkl',\n",
    "    'ENSEMBLE 5': f'experiments\\\\{model_name}_ENSEMBLE\\\\ensemble_5_results.pkl',\n",
    "    'ENSEMBLE 10': f'experiments\\\\{model_name}_ENSEMBLE\\\\ensemble_10_results.pkl',\n",
    "    'MCDP_0.1': f'experiments\\\\{model_name}\\\\results_MCDP_50_0.1.pkl',\n",
    "    'MCDP_0.3': f'experiments\\\\{model_name}\\\\results_MCDP_50_0.3.pkl',\n",
    "    'MCDP_0.5': f'experiments\\\\{model_name}\\\\results_MCDP_50_0.5.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    'vgggace': f'experiments\\\\VGGFace\\\\results_MCDP_50_0.5.pkl',\n",
    "    'ncnn': f'experiments\\\\NCNN\\\\results_MCDP_50_0.3.pkl',\n",
    "    'vitb32': f'experiments\\\\ViT_B_32_ENSEMBLE\\\\ensemble_10_results.pkl'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = ''\n",
    "model_name = 'VGGFace' # change model here\n",
    "\n",
    "filename = 'results_MCDP_50_0.5.pkl'\n",
    "\n",
    "original = f'experiments\\\\{folder}\\\\{model_name}\\\\{filename}'\n",
    "LS_01 = f'experiments\\\\{folder}\\\\{model_name}_LS_01\\\\{filename}'\n",
    "LS_03 = f'experiments\\\\{folder}\\\\{model_name}_LS_03\\\\{filename}'\n",
    "LS_05 = f'experiments\\\\{folder}\\\\{model_name}_LS_05\\\\{filename}'\n",
    "LINEAR = f'experiments\\\\{folder}\\\\{model_name}_LINEAR\\\\{filename}'\n",
    "SIGMOID = f'experiments\\\\{folder}\\\\{model_name}_SIGMOID\\\\{filename}'\n",
    "STEP = f'experiments\\\\{folder}\\\\{model_name}_STEP\\\\{filename}'\n",
    "HIST = f'experiments\\\\{folder}\\\\{model_name}_HIST\\\\{filename}'\n",
    "ISOTONIC = f'experiments\\\\{folder}\\\\{model_name}_ISOTONIC\\\\{filename}'\n",
    "PLATT = f'experiments\\\\{folder}\\\\{model_name}_PLATT\\\\{filename}'\n",
    "TEMPERATURE = f'experiments\\\\{folder}\\\\{model_name}_TEMP\\\\{filename}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = {\n",
    "    \"ORIGINAL\": original,\n",
    "    \"HIST\": HIST, \n",
    "    \"ISOTONIC\": ISOTONIC, \n",
    "    \"LINEAR\": LINEAR,\n",
    "    \"LS_01\": LS_01,\n",
    "    \"LS_03\": LS_03,\n",
    "    \"LS_05\": LS_05,\n",
    "    \"PLATT\": PLATT,\n",
    "    \"SIGMOID\": SIGMOID,\n",
    "    \"STEP\": STEP,\n",
    "    \"TEMPERATURE\": TEMPERATURE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These dictionaries will store metric values across folds\n",
    "metric_keys = ['Accuracy', 'F1 Score', 'Precision', 'Sensitivity', 'Specificity', 'AUC', 'ECE', 'MCE', 'NLL', 'Brier']\n",
    "metrics_dict = {model: {key: [] for key in metric_keys} for model in model_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and extract metrics for each model\n",
    "for model, filepath in model_files.items():\n",
    "    with open(filepath, 'rb') as f:\n",
    "        results_df = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "    for fold in results_df['fold'].unique():\n",
    "        fold_df = results_df[results_df['fold'] == fold]\n",
    "        metrics_cls = validation_metrics(fold_df['preds'], fold_df['probs'], fold_df['labels'])\n",
    "        metrics_calib = calibration_metrics(fold_df['probs'], fold_df['labels'], n_bins=n_bins, mode=mode_metrics)\n",
    "\n",
    "        for key in metrics_cls:\n",
    "            metrics_dict[model][key].append(metrics_cls[key])\n",
    "        for key in metrics_calib:\n",
    "            metrics_dict[model][key].append(metrics_calib[key])\n",
    "\n",
    "# Run Friedman test for each metric\n",
    "friedman_results = {}\n",
    "for key in metric_keys:\n",
    "    data = [metrics_dict[model][key] for model in model_files]\n",
    "    stat, p = friedmanchisquare(*data)\n",
    "    friedman_results[key] = {'Friedman χ²': stat, 'p-value': p}\n",
    "\n",
    "friedman_df = pd.DataFrame(friedman_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Friedman χ²</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.076923</td>\n",
       "      <td>0.583645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>2.205128</td>\n",
       "      <td>0.332019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.716531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>2.114286</td>\n",
       "      <td>0.347447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.971017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.149569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.082085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCE</th>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.122456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLL</th>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.301194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brier</th>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.301194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Friedman χ²   p-value\n",
       "Accuracy        1.076923  0.583645\n",
       "F1 Score        2.205128  0.332019\n",
       "Precision       0.666667  0.716531\n",
       "Sensitivity     2.114286  0.347447\n",
       "Specificity     0.058824  0.971017\n",
       "AUC             3.800000  0.149569\n",
       "ECE             5.000000  0.082085\n",
       "MCE             4.200000  0.122456\n",
       "NLL             2.400000  0.301194\n",
       "Brier           2.400000  0.301194"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friedman_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = ''\n",
    "model_name = 'ViT_B_32_ENSEMBLE' # change model here\n",
    "\n",
    "filename = 'ensemble_10_results.pkl'\n",
    "\n",
    "original = f'experiments\\\\{folder}\\\\{model_name}\\\\{filename}'\n",
    "LS_01 = f'experiments\\\\{folder}\\\\{model_name}_LS_01\\\\{filename}'\n",
    "LS_03 = f'experiments\\\\{folder}\\\\{model_name}_LS_03\\\\{filename}'\n",
    "LS_05 = f'experiments\\\\{folder}\\\\{model_name}_LS_05\\\\{filename}'\n",
    "LINEAR = f'experiments\\\\{folder}\\\\{model_name}_LINEAR\\\\{filename}'\n",
    "SIGMOID = f'experiments\\\\{folder}\\\\{model_name}_SIGMOID\\\\{filename}'\n",
    "STEP = f'experiments\\\\{folder}\\\\{model_name}_STEP\\\\{filename}'\n",
    "HIST = f'experiments\\\\{folder}\\\\{model_name}_HIST\\\\{filename}'\n",
    "ISOTONIC = f'experiments\\\\{folder}\\\\{model_name}_ISOTONIC\\\\{filename}'\n",
    "PLATT = f'experiments\\\\{folder}\\\\{model_name}_PLATT\\\\{filename}'\n",
    "TEMPERATURE = f'experiments\\\\{folder}\\\\{model_name}_TEMP\\\\{filename}'\n",
    "\n",
    "models = [HIST, ISOTONIC, LINEAR, LS_01, LS_03, LS_05, PLATT, SIGMOID, STEP, TEMPERATURE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_original = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "                'Sensitivity': [], 'Specificity': [], 'AUC': [], \n",
    "                'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "metrics_cls_aux = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "            'Sensitivity': [], 'Specificity': [], 'AUC': []}\n",
    "\n",
    "metrics_calib_aux = {'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "\n",
    "with open(original, 'rb') as f:\n",
    "    results_originais = pd.DataFrame(pickle.load(f))\n",
    "    \n",
    "    # Collect metric values for each fold\n",
    "for fold in results_originais['fold'].unique():\n",
    "    metrics_cls = validation_metrics(results_originais[results_originais['fold']==fold]['preds'], results_originais[results_originais['fold']==fold]['probs'], results_originais[results_originais['fold']==fold]['labels'])\n",
    "    metrics_calib = calibration_metrics(results_originais[results_originais['fold']==fold]['probs'], results_originais[results_originais['fold']==fold]['labels'], n_bins=n_bins, mode=mode_metrics)\n",
    "\n",
    "    for metric in metrics_cls_aux.keys():\n",
    "        metrics_dict_original[metric].append(metrics_cls[metric])\n",
    "\n",
    "    for metric in metrics_calib_aux.keys():\n",
    "        metrics_dict_original[metric].append(metrics_calib[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments\\\\ViT_B_32_ENSEMBLE_HIST\\ensemble_10_results.pkl\n",
      "\n",
      "0,469\n",
      "0,195\n",
      "0,312\n",
      "0,031\n",
      "0,188\n",
      "0,193\n",
      "0,014\n",
      "0,02\n",
      "0,557\n",
      "0,557\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 6,768284781429555e-05\n",
      "K-S test p-value: 0,00016146159705177112\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_ISOTONIC\\ensemble_10_results.pkl\n",
      "\n",
      "1,0\n",
      "1,0\n",
      "0,812\n",
      "0,5\n",
      "0,875\n",
      "0,426\n",
      "0,037\n",
      "0,492\n",
      "0,695\n",
      "0,695\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,002633864527092048\n",
      "K-S test p-value: 7,055727434101373e-05\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_LINEAR\\ensemble_10_results.pkl\n",
      "\n",
      "0,004\n",
      "0,004\n",
      "0,91\n",
      "0,004\n",
      "0,688\n",
      "0,006\n",
      "0,004\n",
      "1,0\n",
      "0,006\n",
      "0,002\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,0013697249281084726\n",
      "K-S test p-value: 2,340748933183906e-07\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_LS_01\\ensemble_10_results.pkl\n",
      "\n",
      "0,5\n",
      "0,75\n",
      "0,5\n",
      "1,0\n",
      "0,5\n",
      "0,312\n",
      "0,049\n",
      "0,432\n",
      "0,375\n",
      "0,432\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,002512967563672408\n",
      "K-S test p-value: 0,004834951257530529\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_LS_03\\ensemble_10_results.pkl\n",
      "\n",
      "0,148\n",
      "0,461\n",
      "0,078\n",
      "1,0\n",
      "0,156\n",
      "0,312\n",
      "0,002\n",
      "0,695\n",
      "0,02\n",
      "0,01\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,002856262935698375\n",
      "K-S test p-value: 8,873474658347811e-14\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_LS_05\\ensemble_10_results.pkl\n",
      "\n",
      "0,064\n",
      "0,16\n",
      "0,047\n",
      "0,5\n",
      "0,066\n",
      "0,193\n",
      "0,002\n",
      "0,232\n",
      "0,006\n",
      "0,004\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,0063777769458052865\n",
      "K-S test p-value: 4,845751086717138e-27\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_PLATT\\ensemble_10_results.pkl\n",
      "\n",
      "0,25\n",
      "0,25\n",
      "0,25\n",
      "1,0\n",
      "0,25\n",
      "1,0\n",
      "0,922\n",
      "0,049\n",
      "0,193\n",
      "0,322\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,04491480619374238\n",
      "K-S test p-value: 1,808308298984133e-06\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_SIGMOID\\ensemble_10_results.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonardo\\anaconda3\\envs\\doutorado\\lib\\site-packages\\scipy\\stats\\_wilcoxon.py:172: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  z = (r_plus - mn) / se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,109\n",
      "0,039\n",
      "0,496\n",
      "0,027\n",
      "0,688\n",
      "0,014\n",
      "0,002\n",
      "0,275\n",
      "0,004\n",
      "0,002\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,7674771710735947\n",
      "K-S test p-value: 5,4089853241471996e-14\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_STEP\\ensemble_10_results.pkl\n",
      "\n",
      "0,049\n",
      "0,049\n",
      "0,492\n",
      "0,031\n",
      "0,328\n",
      "0,037\n",
      "0,004\n",
      "1,0\n",
      "0,006\n",
      "0,006\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,893903061219879\n",
      "K-S test p-value: 5,312547715088182e-05\n",
      "\n",
      "experiments\\\\ViT_B_32_ENSEMBLE_TEMP\\ensemble_10_results.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonardo\\anaconda3\\envs\\doutorado\\lib\\site-packages\\scipy\\stats\\_wilcoxon.py:172: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  z = (r_plus - mn) / se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0\n",
      "1,0\n",
      "1,0\n",
      "1,0\n",
      "1,0\n",
      "1,0\n",
      "0,105\n",
      "0,02\n",
      "0,432\n",
      "0,77\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 8,537708855959898e-07\n",
      "K-S test p-value: 5,312547715088182e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "\n",
    "    print(model)\n",
    "    print()\n",
    "\n",
    "    metrics_dict = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "                'Sensitivity': [], 'Specificity': [], 'AUC': [], \n",
    "                'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "    \n",
    "    metrics_cls_aux = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "            'Sensitivity': [], 'Specificity': [], 'AUC': []}\n",
    "\n",
    "    metrics_calib_aux = {'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "\n",
    "    with open(model, 'rb') as f:\n",
    "        results = pd.DataFrame(pickle.load(f))\n",
    "        \n",
    "        # Collect metric values for each fold\n",
    "    for fold in results['fold'].unique():\n",
    "        metrics_cls = validation_metrics(results[results['fold']==fold]['preds'], results[results['fold']==fold]['probs'], results[results['fold']==fold]['labels'])\n",
    "        metrics_calib = calibration_metrics(results[results['fold']==fold]['probs'], results[results['fold']==fold]['labels'], n_bins=n_bins, mode=mode_metrics)\n",
    "\n",
    "        for metric in metrics_cls_aux.keys():\n",
    "            metrics_dict[metric].append(metrics_cls[metric])\n",
    "\n",
    "        for metric in metrics_calib_aux.keys():\n",
    "            metrics_dict[metric].append(metrics_calib[metric])\n",
    "        \n",
    "\n",
    "    for metric in metrics_dict.keys():\n",
    "        \n",
    "        values_originais = np.asarray(metrics_dict_original[metric])\n",
    "        values = np.asarray(metrics_dict[metric])\n",
    "\n",
    "        try:\n",
    "            stat, p_value = wilcoxon(values_originais, values)\n",
    "            print(str(round(p_value,3)).replace('.',','))\n",
    "        except ValueError:\n",
    "            print(f'####')\n",
    "\n",
    "    print(\"Probability Distribution Comparison (Wilcoxon and K-S Tests)\")\n",
    "\n",
    "    # Flatten all probabilities from all folds\n",
    "    probs_model = results['probs'].values\n",
    "    probs_originais = results_originais['probs'].values\n",
    "\n",
    "    try:\n",
    "        # Wilcoxon signed-rank test (paired comparison, only if lengths match)\n",
    "        if len(probs_originais) == len(probs_model):\n",
    "            stat_w, p_w = wilcoxon(probs_originais, probs_model)\n",
    "            print(f\"Wilcoxon p-value: {str(round(p_w, 50)).replace('.', ',')}\")\n",
    "        else:\n",
    "            print(\"Wilcoxon skipped: lengths of probability arrays do not match.\")\n",
    "\n",
    "        # Kolmogorov–Smirnov test (doesn't require same length)\n",
    "        \n",
    "        stat_ks, p_ks = ks_2samp(probs_originais, probs_model)\n",
    "        print(f\"K-S test p-value: {str(round(p_ks, 50)).replace('.', ',')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in probability tests: {e}\")\n",
    "\n",
    "\n",
    "        \n",
    "    print()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ORIGINAL VS MCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, wilcoxon\n",
    "import pickle\n",
    "\n",
    "from calibration.calibrators import *\n",
    "from validate import calibration_metrics, validation_metrics\n",
    "from calibration.metrics import *\n",
    "from utils.plots import *\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "mode_metrics = 'quantile'\n",
    "mode_plot = 'uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = ''\n",
    "model_name = 'NCNN' # change model here\n",
    "\n",
    "filename = 'results.pkl'\n",
    "filename_MCDP = 'results_MCDP_50_0.5.pkl'\n",
    "\n",
    "original = f'experiments\\\\{folder}\\\\{model_name}\\\\{filename}'\n",
    "new = f'experiments\\\\{folder}\\\\{model_name}\\\\{filename_MCDP}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT_B_32' # change model here\n",
    "\n",
    "filename = 'results.pkl'\n",
    "filename_ensemble = 'ensemble_3_results.pkl'\n",
    "\n",
    "original = f'experiments\\\\{model_name}\\\\{filename}'\n",
    "new = f'experiments\\\\{model_name}_ENSEMBLE\\\\{filename_ensemble}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict_original = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "                'Sensitivity': [], 'Specificity': [], 'AUC': [], \n",
    "                'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "metrics_cls_aux = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "            'Sensitivity': [], 'Specificity': [], 'AUC': []}\n",
    "\n",
    "metrics_calib_aux = {'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "\n",
    "with open(original, 'rb') as f:\n",
    "    results_originais = pd.DataFrame(pickle.load(f))\n",
    "    \n",
    "    # Collect metric values for each fold\n",
    "for fold in results_originais['fold'].unique():\n",
    "    metrics_cls = validation_metrics(results_originais[results_originais['fold']==fold]['preds'], results_originais[results_originais['fold']==fold]['probs'], results_originais[results_originais['fold']==fold]['labels'])\n",
    "    metrics_calib = calibration_metrics(results_originais[results_originais['fold']==fold]['probs'], results_originais[results_originais['fold']==fold]['labels'], n_bins=n_bins, mode=mode_metrics)\n",
    "\n",
    "    for metric in metrics_cls_aux.keys():\n",
    "        metrics_dict_original[metric].append(metrics_cls[metric])\n",
    "\n",
    "    for metric in metrics_calib_aux.keys():\n",
    "        metrics_dict_original[metric].append(metrics_calib[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,625\n",
      "0,625\n",
      "0,875\n",
      "1,0\n",
      "1,0\n",
      "0,82\n",
      "1,0\n",
      "0,695\n",
      "0,375\n",
      "1,0\n",
      "Probability Distribution Comparison (Wilcoxon and K-S Tests)\n",
      "Wilcoxon p-value: 0,134469\n",
      "K-S test p-value: 0,791667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "            'Sensitivity': [], 'Specificity': [], 'AUC': [], \n",
    "            'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "metrics_cls_aux = {'Accuracy': [], 'F1 Score': [], 'Precision': [], \n",
    "        'Sensitivity': [], 'Specificity': [], 'AUC': []}\n",
    "\n",
    "metrics_calib_aux = {'ECE': [], 'MCE': [], 'NLL': [], 'Brier': []}\n",
    "\n",
    "\n",
    "with open(new, 'rb') as f:\n",
    "    results = pd.DataFrame(pickle.load(f))\n",
    "    \n",
    "    # Collect metric values for each fold\n",
    "for fold in results['fold'].unique():\n",
    "    metrics_cls = validation_metrics(results[results['fold']==fold]['preds'], results[results['fold']==fold]['probs'], results[results['fold']==fold]['labels'])\n",
    "    metrics_calib = calibration_metrics(results[results['fold']==fold]['probs'], results[results['fold']==fold]['labels'], n_bins=n_bins, mode=mode_metrics)\n",
    "\n",
    "    for metric in metrics_cls_aux.keys():\n",
    "        metrics_dict[metric].append(metrics_cls[metric])\n",
    "\n",
    "    for metric in metrics_calib_aux.keys():\n",
    "        metrics_dict[metric].append(metrics_calib[metric])\n",
    "    \n",
    "\n",
    "for metric in metrics_dict.keys():\n",
    "    \n",
    "    values_originais = np.asarray(metrics_dict_original[metric])\n",
    "    values = np.asarray(metrics_dict[metric])\n",
    "\n",
    "    try:\n",
    "        stat, p_value = wilcoxon(values_originais, values)\n",
    "        print(str(round(p_value,3)).replace('.',','))\n",
    "    except ValueError:\n",
    "        print(f'####')\n",
    "\n",
    "print(\"Probability Distribution Comparison (Wilcoxon and K-S Tests)\")\n",
    "\n",
    "# Flatten all probabilities from all folds\n",
    "probs_model = results['probs'].values\n",
    "probs_originais = results_originais['probs'].values\n",
    "\n",
    "try:\n",
    "    # Wilcoxon signed-rank test (paired comparison, only if lengths match)\n",
    "    if len(probs_originais) == len(probs_model):\n",
    "        stat_w, p_w = wilcoxon(probs_originais, probs_model)\n",
    "        print(f\"Wilcoxon p-value: {str(round(p_w, 6)).replace('.', ',')}\")\n",
    "    else:\n",
    "        print(\"Wilcoxon skipped: lengths of probability arrays do not match.\")\n",
    "\n",
    "    # Kolmogorov–Smirnov test (doesn't require same length)\n",
    "    \n",
    "    stat_ks, p_ks = ks_2samp(probs_originais, probs_model)\n",
    "    print(f\"K-S test p-value: {str(round(p_ks, 6)).replace('.', ',')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in probability tests: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "print()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
