{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create a .pkl file with the model's preditions and other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import load_config\n",
    "from dataloaders import *  \n",
    "from models import *       \n",
    "from uncertainty.MCDropout import MCDropout\n",
    "from validate import validation_metrics, calibration_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "BATCH_SIZE = 64\n",
    "POSITIVE_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"_LS_01\", \"_LS_03\", \"_LS_05\", \"_LINEAR\", \"_SIGMOID\", \"_STEP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_FOLDER = ''  # change experiment folder here\n",
    "MODEL_NAME ='VGGFace_FINAL'  # change model here\n",
    "BASE_EXPERIMENT_PATH = os.path.join('experiments', EXPERIMENTS_FOLDER, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCDP Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCDP SETTINGS\n",
    "MCDP = False\n",
    "MCDP_FOWARD_PASSES = 50\n",
    "MCDP_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hook(embeddings_list):\n",
    "    \"\"\"Return a hook function that appends flattened outputs to embeddings_list.\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        output_np = output.detach().cpu().numpy()\n",
    "        for x in output_np:\n",
    "            embeddings_list.append(x.flatten())\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(exp, mode, device, positive_threshold, batch_size):\n",
    "    \"\"\"\n",
    "    Process a single experiment directory.\n",
    "    \n",
    "    Parameters:\n",
    "        exp (str): Name of the experiment folder.\n",
    "        mode (str): Either 'train' or 'test'.\n",
    "        device (str): Device to run inference on.\n",
    "        positive_threshold (float): Threshold for positive predictions.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        \n",
    "    Returns:\n",
    "        fold (str): Extracted fold name.\n",
    "        result (dict): Dictionary containing outputs and optionally embeddings.\n",
    "    \"\"\"\n",
    "    exp_path = os.path.join(BASE_EXPERIMENT_PATH, exp)\n",
    "    model_path = os.path.join(exp_path, 'Model', 'best_model.pt')\n",
    "    config_path = os.path.join(exp_path, 'Model', 'config.yaml')\n",
    "    \n",
    "    # Load configuration\n",
    "    config = load_config(config_path)\n",
    "    data_path = config['path_train'] if mode == 'train' else config['path_test']\n",
    "    data_path = data_path.replace('/', '\\\\')  # Ensure platform independence\n",
    "    \n",
    "    # Extract the fold from the data path (platform independent)\n",
    "    fold = os.path.normpath(data_path).split(os.sep)[-2]\n",
    "    print(f\"Processing {mode} data from: {data_path}\")\n",
    "    \n",
    "    # Set up embeddings collection and hook handle (if needed)\n",
    "    embeddings = []\n",
    "    hook_handle = None\n",
    "\n",
    "    # Choose model architecture and dataset based on experiment name\n",
    "    if \"NCNN\" in exp:\n",
    "        model_instance = NCNN()\n",
    "        dataset = BaseDataset(model_name=\"NCNN\", img_dir=data_path)\n",
    "        #hook_handle = model_instance.merge_branch[0].register_forward_hook(create_hook(embeddings))\n",
    "    elif \"VGGFace\" in exp:\n",
    "        model_instance = VGGFace()\n",
    "        dataset = BaseDataset(model_name=\"VGGFace\", img_dir=data_path)\n",
    "        # You may choose which layer to hook:\n",
    "        # hook_handle = model_instance.VGGFace.features.conv5_3.register_forward_hook(create_hook(embeddings))\n",
    "        #hook_handle = model_instance.VGGFace.classifier[3].register_forward_hook(create_hook(embeddings))\n",
    "    elif \"ViT\" in exp:\n",
    "        model_instance = ViT()\n",
    "        dataset = BaseDataset(model_name=\"ViT\", img_dir=data_path)\n",
    "    elif \"PainClassifier\" in exp:\n",
    "        model_instance = PainClassifier()\n",
    "        dataset = BaseDataset(model_name=\"PainClassifier\", img_dir=data_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown experiment type in {exp}\")\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Load model weights and prepare model for inference\n",
    "    model_instance.load_state_dict(torch.load(model_path))\n",
    "    model_instance = model_instance.to(device)\n",
    "    model_instance.eval()\n",
    "\n",
    "    # Accumulate outputs using lists (more efficient than repeated concatenation)\n",
    "    probs_list, preds_list, logits_list, labels_list = [], [], [], []\n",
    "\n",
    "    # If MCDP is activated, accumulate probabilities using a list\n",
    "    if MCDP:\n",
    "        probs_uq_list = []\n",
    "        model_instance = MCDropout(model_instance, p=MCDP_DROPOUT)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {exp}\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # If MCDP calculate probabilities for each forward pass\n",
    "            if MCDP:\n",
    "                probs = model_instance.predict(inputs, reps=MCDP_FOWARD_PASSES)\n",
    "                preds = torch.ge(torch.mean(probs, dim=1), positive_threshold).type(torch.int)\n",
    "                probs_uq_list.append(probs)\n",
    "\n",
    "            else:\n",
    "                logits = model_instance(inputs)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= positive_threshold).int()\n",
    "                logits_list.append(logits)\n",
    "\n",
    "            probs_list.append(probs)\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Concatenate tensors and convert to numpy arrays\n",
    "    probs_all = torch.cat(probs_list).cpu().numpy()\n",
    "    preds_all = torch.cat(preds_list).cpu().numpy()\n",
    "    labels_all = torch.cat(labels_list).cpu().numpy()\n",
    "\n",
    "    if MCDP:\n",
    "    \n",
    "        result = {\n",
    "            'img_names': np.array(dataset.img_paths),\n",
    "            'probs': probs_all.mean(axis=1),\n",
    "            'preds': preds_all,\n",
    "            'labels': labels_all,\n",
    "            'probs_uq': probs_all\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        logits_all = torch.cat(logits_list).cpu().numpy()\n",
    "        \n",
    "        result = {\n",
    "            'img_names': np.array(dataset.img_paths),\n",
    "            'probs': probs_all,\n",
    "            'preds': preds_all,\n",
    "            'logits': logits_all,\n",
    "            'labels': labels_all,\n",
    "            #'embeddings': np.array(embeddings)\n",
    "        }\n",
    "\n",
    "    # Remove hook if it was set\n",
    "    if hook_handle is not None:\n",
    "        hook_handle.remove()\n",
    "\n",
    "    # Cleanup GPU memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return fold, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main loop to process all experiments for each mode and save results.\"\"\"\n",
    "    modes = ['train', 'test']\n",
    "\n",
    "    if MCDP:\n",
    "        save_filenames = [f'train_results_MCDP_{MCDP_FOWARD_PASSES}_{MCDP_DROPOUT}.pkl', f'results_MCDP_{MCDP_FOWARD_PASSES}_{MCDP_DROPOUT}.pkl']\n",
    "    else:\n",
    "        save_filenames = ['train_results.pkl', 'results.pkl']\n",
    "    \n",
    "    for mode, save_filename in zip(modes, save_filenames):\n",
    "        results = {'fold': []}\n",
    "        # List all experiment directories in the base experiments path\n",
    "        for exp in os.listdir(BASE_EXPERIMENT_PATH):\n",
    "            # Filter out non-experiment files\n",
    "            if any(sub in exp for sub in ['.pkl', 'masks', '.png']):\n",
    "                continue\n",
    "            try:\n",
    "                fold, res = process_experiment(exp, mode, DEVICE, POSITIVE_THRESHOLD, BATCH_SIZE)\n",
    "                #results[fold] = res\n",
    "                for key, value in res.items():\n",
    "                    if key not in results.keys():\n",
    "                        results[key] = value.tolist()\n",
    "                    else:\n",
    "                        results[key].extend(value.tolist())\n",
    "                \n",
    "                results['fold'].extend([int(fold)] * len(res['img_names']))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {exp}: {e}\")\n",
    "                results['fold'].extend([fold] * len(res['img_names']))\n",
    "\n",
    "        output_path = os.path.join(BASE_EXPERIMENT_PATH, save_filename)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"Saved {mode} results to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mcdp & original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_calibrators = [\"_LS_01\", \"_LS_03\", \"_LS_05\", \"_LINEAR\", \"_SIGMOID\", \"_STEP\"]\n",
    "\n",
    "model = \"VGGFace\"  # change model here\n",
    "for calibrator in train_calibrators:\n",
    "    print(f\"Train calibrator: {calibrator}\")\n",
    "    MODEL_NAME = f'{model}{calibrator}'  \n",
    "    BASE_EXPERIMENT_PATH = os.path.join('experiments', MODEL_NAME)\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCDP SETTINGS\n",
    "MCDP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS_FOLDER = 'ViT_B_32_ENSEMBLE_FINAL'  # change experiment folder here\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(f\"Train ensemble {i}\")\n",
    "    MODEL_NAME = f'ensemble_{i}'  # change model here\n",
    "    BASE_EXPERIMENT_PATH = os.path.join('experiments', EXPERIMENTS_FOLDER, MODEL_NAME)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Path to the parent ensemble folder\n",
    "PARENT_DIR = \"experiments\\\\ViT_B_32_ENSEMBLE_FINAL\"\n",
    "\n",
    "#for ensembles_n in [3,5,10]:\n",
    "for ensembles_n in [10]:\n",
    "    print(f\"Creating ensemble with {ensembles_n} models\")\n",
    "    save_filenames = [f'train_results.pkl', f'results.pkl']\n",
    "\n",
    "    for save_filename in save_filenames:\n",
    "        # 1. Gather all results.pkl paths\n",
    "        paths = []\n",
    "        for sub in sorted(os.listdir(PARENT_DIR)):\n",
    "            p = os.path.join(PARENT_DIR, sub, save_filename)\n",
    "            if os.path.isfile(p):\n",
    "                paths.append(p)\n",
    "        if not paths:\n",
    "            raise RuntimeError(f\"No results.pkl files found under {PARENT_DIR}\")\n",
    "\n",
    "        # 2. Load them\n",
    "        all_probs = []\n",
    "        for i, p in enumerate(paths[:ensembles_n]):\n",
    "            with open(p, \"rb\") as f:\n",
    "                d = pickle.load(f)\n",
    "            # On first file, save the img_names and labels\n",
    "            if i == 0:\n",
    "                img_names = d[\"img_names\"]\n",
    "                labels    = d.get(\"labels\", None)\n",
    "                folds     = d.get(\"fold\", None)\n",
    "            else:\n",
    "                # sanity check\n",
    "                if d[\"img_names\"] != img_names:\n",
    "                    raise RuntimeError(f\"Image ordering mismatch in {p}\")\n",
    "            all_probs.append(np.array(d[\"probs\"]))\n",
    "\n",
    "        # 3. Stack and average\n",
    "        #    shape = (n_models, n_images)\n",
    "        stacked = np.vstack(all_probs)\n",
    "        mean_probs = stacked.mean(axis=0)\n",
    "\n",
    "        # 4. Threshold to get predictions\n",
    "        preds = (mean_probs >= 0.5).astype(int)\n",
    "\n",
    "        # 5. Build the ensemble dict\n",
    "        ensemble = {\n",
    "            \"fold\":      folds,\n",
    "            \"img_names\": img_names,\n",
    "            \"labels\":    labels,\n",
    "            \"probs\":     mean_probs.tolist(),\n",
    "            # for each image, the list of its prob predictions across the 5 models\n",
    "            \"probs_uq\":  [list(row) for row in stacked.T],\n",
    "            \"preds\":     preds.tolist(),\n",
    "        }\n",
    "\n",
    "        # 6. Save it\n",
    "        save_filename = f'ensemble_{ensembles_n}_{save_filename}'\n",
    "        with open(os.path.join(PARENT_DIR, save_filename), \"wb\") as f:\n",
    "            pickle.dump(ensemble, f)\n",
    "\n",
    "        print(f\"Wrote ensemble results to {save_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-hoc Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import create_folder\n",
    "from calibration.calibrators import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EXPERIMENT_PATH = os.path.join('experiments', \"ViT_B_32_ENSEMBLE\")\n",
    "MCDP = True\n",
    "\n",
    "filename = 'ensemble_10_results.pkl'  # change here\n",
    "filename_calib = 'ensemble_10_train_results.pkl'  # change here\n",
    "\n",
    "\n",
    "positive_threshold = 0.5\n",
    "\n",
    "if \"MCDP\" in filename:\n",
    "    mcdp = \"_MCDP_50_0.5\"\n",
    "    save_filename = f'results{mcdp}.pkl'\n",
    "\n",
    "elif \"ensemble\" in filename:\n",
    "    mcdp = \"ensemble_10_\"\n",
    "    save_filename = f'{mcdp}results.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_post_hoc(results, calib_results, calibrator):\n",
    "\n",
    "    # Get unique folds from the results.\n",
    "    unique_folds = results['fold'].unique()\n",
    "    \n",
    "    for fold in unique_folds:\n",
    "        # Select calibration data for the current fold.\n",
    "        calib_fold = calib_results[calib_results['fold'] == fold]\n",
    "        # Fit the calibrator on this fold's calibration data.\n",
    "        calibrator.fit(calib_fold['probs'].values, calib_fold['labels'].values)\n",
    "        \n",
    "        # Identify the indices for the current fold in the results.\n",
    "        idx = results['fold'] == fold\n",
    "        # Calibrate the probabilities for this fold.\n",
    "        calibrated_probs = calibrator.predict(results.loc[idx, 'probs'].values)\n",
    "        \n",
    "        # Update the DataFrame in place.\n",
    "        results.loc[idx, 'probs'] = calibrated_probs\n",
    "        results.loc[idx, 'preds'] = (calibrated_probs >= positive_threshold).astype('float32')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(BASE_EXPERIMENT_PATH + '_PLATT')\n",
    "\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename), 'rb') as f:\n",
    "    results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename_calib), 'rb') as f:\n",
    "    calib_results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "calibrator = PlattScaling()\n",
    "\n",
    "new_results = calibrate_post_hoc(results=results, calib_results=calib_results, calibrator=calibrator)\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH + '_PLATT', save_filename), 'wb') as f:\n",
    "    pickle.dump(new_results.to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(BASE_EXPERIMENT_PATH + '_TEMP')\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename), 'rb') as f:\n",
    "    results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename_calib), 'rb') as f:\n",
    "    calib_results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "calibrator = TemperatureScaling()\n",
    "\n",
    "new_results = calibrate_post_hoc(results=results, calib_results=calib_results, calibrator=calibrator)\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH + '_TEMP', save_filename), 'wb') as f:\n",
    "    pickle.dump(new_results.to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isotonic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(BASE_EXPERIMENT_PATH + '_ISOTONIC')\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename), 'rb') as f:\n",
    "    results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename_calib), 'rb') as f:\n",
    "    calib_results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "calibrator = IsotonicRegressor()\n",
    "\n",
    "new_results = calibrate_post_hoc(results=results, calib_results=calib_results, calibrator=calibrator)\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH + '_ISOTONIC', save_filename), 'wb') as f:\n",
    "    pickle.dump(new_results.to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder(BASE_EXPERIMENT_PATH + '_HIST')\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename), 'rb') as f:\n",
    "    results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH, filename_calib), 'rb') as f:\n",
    "    calib_results = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "calibrator = HistogramBinning()\n",
    "\n",
    "new_results = calibrate_post_hoc(results=results, calib_results=calib_results, calibrator=calibrator)\n",
    "\n",
    "with open(os.path.join(BASE_EXPERIMENT_PATH + '_HIST', save_filename), 'wb') as f:\n",
    "    pickle.dump(new_results.to_dict('list'), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
