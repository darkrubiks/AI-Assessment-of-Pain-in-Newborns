{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leonardo\\anaconda3\\envs\\doutorado\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.utils import load_config, create_folder, scale_coords\n",
    "from utils.plots import *\n",
    "from dataloaders import *\n",
    "from models import *\n",
    "from validate import *\n",
    "\n",
    "from utils.plots import *\n",
    "from utils.utils import scale_coords, resize_landmarks\n",
    "\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "from tqdm import tqdm\n",
    "\n",
    "from XAI import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_icopevid = 'Datasets\\\\Originais\\\\iCOPE\\\\iCOPEvid'\n",
    "path_icopevid_frames = 'Datasets\\\\Originais\\\\iCOPE\\\\iCOPEvid\\\\all_frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinaface = FaceAnalysis(allowed_modules=['detection','landmark_2d_106'], providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "retinaface.prepare(ctx_id=0, det_size=(640, 640)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in tqdm(os.listdir(path_icopevid)):\n",
    "    if video_path.endswith('mp4'):\n",
    "        video_name = video_path.split('.mp4')[0]\n",
    "\n",
    "        if \"Pain\" in video_name or \"Rest\" in video_name:\n",
    "\n",
    "            create_folder(os.path.join(path_icopevid_frames, video_name))\n",
    "\n",
    "            video = cv2.VideoCapture(os.path.join(path_icopevid, video_path))\n",
    "\n",
    "            idx = 0\n",
    "            while True:\n",
    "                ret, frame = video.read()\n",
    "\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                try:\n",
    "                    faces = retinaface.get(frame_rgb)[0]\n",
    "                    bbox = faces['bbox'].astype('int')\n",
    "                    bbox[bbox < 0] = 0\n",
    "                    keypoints = faces['kps'].astype('int')\n",
    "                    landmarks = faces['landmark_2d_106'].astype('int')\n",
    "\n",
    "                    # Scale the landmarks based on the previous bbox, so it matches the facial image shape\n",
    "                    scaled_landmarks = [scale_coords(x, y, bbox) for x, y in landmarks]\n",
    "\n",
    "                    x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                    cropped_face = frame[y1:y2, x1:x2]\n",
    "                    cropped_face = cv2.resize(cropped_face, (512, 512))\n",
    "\n",
    "                    # Resize the landmarks to match the new image size\n",
    "                    resized_landmarks = resize_landmarks(np.array(scaled_landmarks), cropped_face.shape[:2], (512, 512))\n",
    "\n",
    "                    cv2.imwrite(os.path.join(path_icopevid_frames, \n",
    "                                            video_name, \n",
    "                                            video_name+\"_\"+f'{idx:04d}'+'.jpg'), cropped_face)\n",
    "\n",
    "\n",
    "                    create_folder(os.path.join(path_icopevid_frames, \n",
    "                                            video_name,\n",
    "                                            'landmarks'))\n",
    "\n",
    "                    with open(os.path.join(path_icopevid_frames, \n",
    "                                            video_name, \n",
    "                                            'landmarks',\n",
    "                                            video_name+\"_\"+f'{idx:04d}'+'.pkl'), 'wb') as f:\n",
    "                        pickle.dump(resized_landmarks, f)\n",
    "\n",
    "                except IndexError:\n",
    "                    cv2.imwrite(os.path.join(path_icopevid_frames, \n",
    "                                            video_name, \n",
    "                                            video_name+\"_\"+f'{idx:04d}'+'_blank'+'.jpg'), np.zeros((512,512,1)))\n",
    "                    pass\n",
    "\n",
    "                idx += 1\n",
    "            video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in tqdm(os.listdir(path_icopevid)):\n",
    "    if video_path.endswith('mp4'):\n",
    "        video_name = video_path.split('.mp4')[0]\n",
    "\n",
    "        if \"Pain\" in video_name or \"Rest\" in video_name:\n",
    "\n",
    "            # Load the video using MoviePy\n",
    "            video_clip = VideoFileClip(os.path.join(path_icopevid, video_path), audio_fps=48000)\n",
    "\n",
    "            # Extract audio from the video\n",
    "            audio_signal = video_clip.audio\n",
    "\n",
    "            # Convert audio signal to a numpy array\n",
    "            audio_array = np.array(audio_signal.to_soundarray())\n",
    "\n",
    "            with open(os.path.join(path_icopevid_frames,video_name,f'audio_signal.pkl'), 'wb') as f:\n",
    "                pickle.dump(audio_array, f)\n",
    "\n",
    "            # Close the video clip\n",
    "            video_clip.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the predictions dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with MCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "\n",
    "model_name = \"VGGFace_FINAL\"\n",
    "final_model = '\\\\20250826_1353_VGGFace'\n",
    "\n",
    "path_experiments = 'experiments\\\\' + model_name\n",
    "path_modell = path_experiments + final_model \n",
    "\n",
    "create_folder(os.path.join(path_experiments,\"icopevid\"))\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertainty.MCDropout import MCDropout\n",
    "from dataloaders import iCOPEVidDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "reps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video = dict()\n",
    "for video_path in tqdm(os.listdir(path_icopevid_frames)):\n",
    "    if not video_path.endswith('.pkl'):\n",
    "        results_folds = dict()\n",
    "        # Para cada video iremos rodar os modelos de cada fold e salvar os resultados  \n",
    "        # Carregar modelo e config.yaml\n",
    "        path_model = os.path.join(path_modell, 'Model','best_model.pt')\n",
    "        path_yaml = os.path.join(path_modell, 'Model','config.yaml')\n",
    "\n",
    "        # Carrego o .yaml para pegar o path do test, ou seja, o fold que foi utilizado\n",
    "        config = load_config(path_yaml)\n",
    "        test_path = config['path_test']\n",
    "        fold = test_path.split('\\\\')[-2]\n",
    "        \n",
    "        # Carrego o .yaml para pegar o path do test, ou seja, o fold que foi utilizado\n",
    "        if \"NCNN\" in path_model:\n",
    "            model = NCNN().to(device)\n",
    "            name = \"NCNN\"\n",
    "        elif \"VGGFace\" in path_model:\n",
    "            model = VGGFace().to(device)\n",
    "            name = \"VGGFace\"\n",
    "        elif \"ViT\" in path_model:\n",
    "            model = ViT().to(device)\n",
    "            name = \"ViT\"\n",
    "        \n",
    "        # Carrego o modelo\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(path_model))\n",
    "        model = model.to(device)\n",
    "        model = MCDropout(model, p=p)\n",
    "        \n",
    "        # Iteração sobre os dados salvando os probs, preds e labels\n",
    "        probs_list = torch.empty(0, device=device)\n",
    "\n",
    "        dataset = iCOPEVidDataset(os.path.join(path_icopevid_frames,video_path), name)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                img_batch, blanks, _ = batch\n",
    "\n",
    "                probs = model.predict(img_batch.to(device), reps)\n",
    "\n",
    "                probs[blanks] = np.nan\n",
    "\n",
    "                probs_list = torch.cat([probs_list, probs])\n",
    "                    \n",
    "        results_folds[fold] = probs_list.cpu().numpy()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "                \n",
    "        results_video[video_path] = results_folds\n",
    "\n",
    "with open(os.path.join(path_experiments,\"icopevid\",f'results_icopevid_MCDP_{reps}_{p}.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_video, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "\n",
    "model_name = \"ViT_B_32_ENSEMBLE_FINAL\\\\ensemble_9\"\n",
    "final_model = '\\\\20250826_1529_ViT'\n",
    "\n",
    "path_experiments = 'experiments\\\\' + model_name\n",
    "path_modell = path_experiments + final_model \n",
    "\n",
    "create_folder(os.path.join(path_experiments,\"icopevid\"))\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_video = dict()\n",
    "for video_path in tqdm(os.listdir(path_icopevid_frames)):\n",
    "    if not video_path.endswith('.pkl'):\n",
    "        results_folds = dict()\n",
    "        # Para cada video iremos rodar os modelos de cada fold e salvar os resultados  \n",
    "        # Carregar modelo e config.yaml\n",
    "        path_model = os.path.join(path_modell, 'Model','best_model.pt')\n",
    "        path_yaml = os.path.join(path_modell, 'Model','config.yaml')\n",
    "\n",
    "        # Carrego o .yaml para pegar o path do test, ou seja, o fold que foi utilizado\n",
    "        config = load_config(path_yaml)\n",
    "        test_path = config['path_test']\n",
    "        fold = test_path.split('\\\\')[-2]\n",
    "        \n",
    "        # Carrego o .yaml para pegar o path do test, ou seja, o fold que foi utilizado\n",
    "        if \"NCNN\" in path_model:\n",
    "            model = NCNN().to(device)\n",
    "            name = \"NCNN\"\n",
    "        elif \"VGGFace\" in path_model:\n",
    "            model = VGGFace().to(device)\n",
    "            name = \"VGGFace\"\n",
    "        elif \"ViT\" in path_model:\n",
    "            model = ViT().to(device)\n",
    "            name = \"ViT\"\n",
    "        \n",
    "        # Carrego o modelo\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(path_model))\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Iteração sobre os dados salvando os probs, preds e labels\n",
    "        probs_list = torch.empty(0, device=device)\n",
    "\n",
    "        dataset = iCOPEVidDataset(os.path.join(path_icopevid_frames,video_path), name)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                img_batch, blanks, _ = batch\n",
    "\n",
    "                probs = model.predict(img_batch.to(device))\n",
    "\n",
    "                probs[blanks] = np.nan\n",
    "\n",
    "                probs_list = torch.cat([probs_list, probs])\n",
    "                    \n",
    "        results_folds[fold] = probs_list.cpu().numpy()\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "                \n",
    "        results_video[video_path] = results_folds\n",
    "\n",
    "with open(os.path.join(path_experiments,\"icopevid\",f'results_icopevid.pkl'), 'wb') as f:\n",
    "    pickle.dump(results_video, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_results = {}\n",
    "num_ensembles = 10\n",
    "base_path = 'experiments\\\\ViT_B_32_ENSEMBLE_FINAL'\n",
    "output_file= base_path + \"\\\\icopevid\\\\results_ensemble_10.pkl\"\n",
    "\n",
    "for i in range(num_ensembles):\n",
    "    file_path = os.path.join(base_path, f\"ensemble_{i}\", \"icopevid\", \"results_icopevid.pkl\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for video, inner_dict in results.items():\n",
    "        if \"TrainAll\" not in inner_dict:\n",
    "            raise KeyError(f\"'TrainAll' key not found for video {video} in {file_path}\")\n",
    "\n",
    "        probs = np.array(inner_dict[\"TrainAll\"])\n",
    "        if video not in stacked_results:\n",
    "            stacked_results[video] = {\"TrainAll\": []}\n",
    "        stacked_results[video][\"TrainAll\"].append(probs)\n",
    "\n",
    "# Convert lists to stacked numpy arrays (shape: [num_ensembles, ...])\n",
    "for video in stacked_results:\n",
    "    stacked_results[video][\"TrainAll\"] = np.stack(stacked_results[video][\"TrainAll\"], axis=0)\n",
    "\n",
    "# Save final dictionary\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump(stacked_results, f)\n",
    "\n",
    "    print(f\"✅ Stacked results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XAIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def enable_mc_dropout(model: nn.Module, p: float = 0.1):\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.p = p\n",
    "            m.train()\n",
    "\n",
    "def welford_update(mean, m2, x, k):\n",
    "    if mean is None:\n",
    "        mean = x.astype(np.float32)\n",
    "        m2   = np.zeros_like(mean, dtype=np.float32)\n",
    "    else:\n",
    "        delta = x - mean\n",
    "        mean += delta / k\n",
    "        m2   += delta * (x - mean)\n",
    "    return mean, m2\n",
    "\n",
    "def welford_finalize(mean, m2, n, unbiased=False):\n",
    "    denom = (n - 1) if unbiased and n > 1 else n\n",
    "    std   = np.sqrt(m2 / denom)\n",
    "    return mean, std\n",
    "\n",
    "def to_uint8(m): return (np.clip(m, 0, 1)*255).round().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_experiments = 'experiments\\\\ViT_B_32_ENSEMBLE_FINAL'\n",
    "model_path = '20250826_1353_VGGFace'\n",
    "model_name = \"VGGFace\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_P = 0.5\n",
    "MC_SAMPLES = 50\n",
    "UNBIASED_STD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [10:50:13<00:00, 325.11s/it] \n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo e config.yaml\n",
    "path_model = os.path.join(path_experiments, model_path, 'Model', 'best_model.pt')\n",
    "path_yaml  = os.path.join(path_experiments, model_path, 'Model', 'config.yaml')\n",
    "\n",
    "# Carrego o .yaml para pegar o path do test (fold)\n",
    "config    = load_config(path_yaml)\n",
    "test_path = config['path_test']\n",
    "fold      = test_path.split('\\\\')[-2]\n",
    "\n",
    "# Instancia o modelo e os métodos de atribuição\n",
    "if \"NCNN\" in path_experiments:\n",
    "    model   = NCNN().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.merge_branch[0], device=device)\n",
    "elif \"VGGFace\" in path_experiments:\n",
    "    model   = VGGFace().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.VGGFace.features.conv5_3, device=device)\n",
    "elif \"ViT\" in path_experiments:\n",
    "    model   = ViT().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.ViT.encoder.layers.encoder_layer_11.ln_1,\n",
    "                      device=device, reshape_transform_ViT=True)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported model in path_experiments\")\n",
    "\n",
    "# Carrega pesos e coloca em eval\n",
    "model.load_state_dict(torch.load(path_model))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Habilita MC Dropout\n",
    "enable_mc_dropout(model, DROPOUT_P)\n",
    "\n",
    "BATCH_SIZE = 32  # Ajuste conforme a GPU\n",
    "\n",
    "for video_path in tqdm(os.listdir(path_icopevid_frames)):\n",
    "    if video_path.endswith('.pkl'):\n",
    "        continue\n",
    "\n",
    "    # Pasta de saída para máscaras XAI com MCDP\n",
    "    mcdp_folder = os.path.join(path_experiments, \"icopevid\", \"xai_masks_MCDP\", video_path)\n",
    "    create_folder(mcdp_folder)\n",
    "\n",
    "    dataset = iCOPEVidDataset(os.path.join(path_icopevid_frames, video_path), model_name)\n",
    "\n",
    "    batch_imgs  = []\n",
    "    batch_files = []\n",
    "\n",
    "    for img, blank, file_name in dataset:\n",
    "        if blank:\n",
    "            continue\n",
    "\n",
    "        batch_imgs.append(img)\n",
    "        batch_files.append(file_name)\n",
    "\n",
    "        if len(batch_imgs) == BATCH_SIZE:\n",
    "            batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
    "\n",
    "            ig_mean = ig_m2 = gc_mean = gc_m2 = None\n",
    "            for k in range(1, MC_SAMPLES + 1):\n",
    "                masks_ig_k = ig.attribution_mask(batch_tensor)\n",
    "                masks_gc_k = gradcam.attribution_mask(batch_tensor)\n",
    "                ig_mean, ig_m2 = welford_update(ig_mean, ig_m2, masks_ig_k, k)\n",
    "                gc_mean, gc_m2 = welford_update(gc_mean, gc_m2, masks_gc_k, k)\n",
    "\n",
    "            masks_ig_mean, masks_ig_std = welford_finalize(ig_mean, ig_m2, MC_SAMPLES, UNBIASED_STD)\n",
    "            masks_gc_mean, masks_gc_std = welford_finalize(gc_mean, gc_m2, MC_SAMPLES, UNBIASED_STD)\n",
    "\n",
    "            for i, f_name in enumerate(batch_files):\n",
    "                base = f_name.replace(\".jpg\", \"\")\n",
    "                out = to_uint8(masks_ig_mean[i])\n",
    "                np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_IG.npz\"), mask=out)\n",
    "\n",
    "                out = to_uint8(masks_ig_std[i])\n",
    "                np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_IG_std.npz\"), mask=out)\n",
    "\n",
    "                out = to_uint8(masks_gc_mean[i])\n",
    "                np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_GC.npz\"), mask=out)\n",
    "\n",
    "                out = to_uint8(masks_gc_std[i])\n",
    "                np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_GC_std.npz\"), mask=out)\n",
    "\n",
    "\n",
    "            batch_imgs  = []\n",
    "            batch_files = []\n",
    "\n",
    "    # Processa lote final incompleto\n",
    "    if len(batch_imgs) > 0:\n",
    "        batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
    "\n",
    "        ig_mean = ig_m2 = gc_mean = gc_m2 = None\n",
    "        for k in range(1, MC_SAMPLES + 1):\n",
    "            masks_ig_k = ig.attribution_mask(batch_tensor)\n",
    "            masks_gc_k = gradcam.attribution_mask(batch_tensor)\n",
    "            ig_mean, ig_m2 = welford_update(ig_mean, ig_m2, masks_ig_k, k)\n",
    "            gc_mean, gc_m2 = welford_update(gc_mean, gc_m2, masks_gc_k, k)\n",
    "\n",
    "        masks_ig_mean, masks_ig_std = welford_finalize(ig_mean, ig_m2, MC_SAMPLES, UNBIASED_STD)\n",
    "        masks_gc_mean, masks_gc_std = welford_finalize(gc_mean, gc_m2, MC_SAMPLES, UNBIASED_STD)\n",
    "\n",
    "        for i, f_name in enumerate(batch_files):\n",
    "            base = f_name.replace(\".jpg\", \"\")\n",
    "            out = to_uint8(masks_ig_mean[i])\n",
    "            np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_IG.npz\"), mask=out)\n",
    "\n",
    "            out = to_uint8(masks_ig_std[i])\n",
    "            np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_IG_std.npz\"), mask=out)\n",
    "\n",
    "            out = to_uint8(masks_gc_mean[i])\n",
    "            np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_GC.npz\"), mask=out)\n",
    "\n",
    "            out = to_uint8(masks_gc_std[i])\n",
    "            np.savez_compressed(os.path.join(mcdp_folder, f\"{base}_GC_std.npz\"), mask=out)\n",
    "\n",
    "    # Limpeza por vídeo\n",
    "    del dataset, batch_imgs, batch_files\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_experiments = 'experiments\\\\ViT_B_32_ENSEMBLE_FINAL'\n",
    "model_name = \"ViT\"\n",
    "device = \"cuda\"\n",
    "UNBIASED_STD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [09:12<00:00, 61.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import os, re, gc, pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Find ensemble folders (accepts \"ensemble_*\" and common misspelling \"ensenble_*\")\n",
    "ensemble_dirs = sorted(\n",
    "    d for d in os.listdir(path_experiments)\n",
    "    if os.path.isdir(os.path.join(path_experiments, d)) and re.match(r\"ense[nm]ble_\\d+$\", d)\n",
    ")\n",
    "if not ensemble_dirs:\n",
    "    raise RuntimeError(f\"No ensemble_* folders found under {path_experiments}\")\n",
    "\n",
    "def find_model_dir(ens_root: str) -> str:\n",
    "    # Prefer exactly one extra folder level: <ensemble>/<something>/Model\n",
    "    try:\n",
    "        for d in os.listdir(ens_root):\n",
    "            candidate = os.path.join(ens_root, d, 'Model')\n",
    "            if os.path.isdir(candidate):\n",
    "                return candidate\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    # Fallback to old layout: <ensemble>/Model\n",
    "    direct = os.path.join(ens_root, 'Model')\n",
    "    if os.path.isdir(direct):\n",
    "        return direct\n",
    "\n",
    "    # Last resort: search recursively\n",
    "    for root, dirs, files in os.walk(ens_root):\n",
    "        if os.path.basename(root) == 'Model':\n",
    "            return root\n",
    "\n",
    "    raise RuntimeError(f\"Could not find a 'Model' directory under {ens_root}\")\n",
    "\n",
    "# Precompute Model dirs for all ensembles\n",
    "ensemble_model_dirs = [\n",
    "    find_model_dir(os.path.join(path_experiments, ens_name)) for ens_name in ensemble_dirs\n",
    "]\n",
    "\n",
    "# Load config from the first ensemble if you need the fold info\n",
    "path_yaml = os.path.join(ensemble_model_dirs[0], 'config.yaml')\n",
    "config    = load_config(path_yaml)\n",
    "test_path = config['path_test']\n",
    "fold      = test_path.split('\\\\')[-2]\n",
    "\n",
    "# Instantiate model + attribution methods once; we will swap weights per ensemble\n",
    "if \"NCNN\" in path_experiments:\n",
    "    model   = NCNN().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.merge_branch[0], device=device)\n",
    "elif \"VGGFace\" in path_experiments:\n",
    "    model   = VGGFace().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.VGGFace.features.conv5_3, device=device)\n",
    "elif \"ViT\" in path_experiments:\n",
    "    model   = ViT().to(device)\n",
    "    ig      = IntegratedGradients(model, device=device)\n",
    "    gradcam = GradCAM(model, model.ViT.encoder.layers.encoder_layer_11.ln_1,\n",
    "                      device=device, reshape_transform_ViT=True)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported model in path_experiments\")\n",
    "\n",
    "def process_batch(batch_imgs, batch_files, out_folder):\n",
    "    if not batch_imgs:\n",
    "        return\n",
    "\n",
    "    batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
    "\n",
    "    # Aggregate across ensembles (single deterministic pass per ensemble; no MC Dropout)\n",
    "    ens_mean_ig = ens_m2_ig = ens_mean_gc = ens_m2_gc = None\n",
    "\n",
    "    for e_idx, model_dir in enumerate(ensemble_model_dirs, 1):\n",
    "        path_model_e = os.path.join(model_dir, 'best_model.pt')\n",
    "\n",
    "        # Load this ensemble member\n",
    "        state = torch.load(path_model_e, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.eval().to(device)  # dropout/bn in eval; no MCDP\n",
    "\n",
    "        # Compute attributions once per ensemble\n",
    "        masks_ig = ig.attribution_mask(batch_tensor)\n",
    "        masks_gc = gradcam.attribution_mask(batch_tensor)\n",
    "\n",
    "        # Update ensemble-level Welford accumulators\n",
    "        ens_mean_ig, ens_m2_ig = welford_update(ens_mean_ig, ens_m2_ig, masks_ig, e_idx)\n",
    "        ens_mean_gc, ens_m2_gc = welford_update(ens_mean_gc, ens_m2_gc, masks_gc, e_idx)\n",
    "\n",
    "    # Final mean + std across ensembles\n",
    "    final_ig_mean, final_ig_std = welford_finalize(ens_mean_ig, ens_m2_ig, len(ensemble_dirs), UNBIASED_STD)\n",
    "    final_gc_mean, final_gc_std = welford_finalize(ens_mean_gc, ens_m2_gc, len(ensemble_dirs), UNBIASED_STD)\n",
    "\n",
    "    # Save only final results\n",
    "    for i, f_name in enumerate(batch_files):\n",
    "        base = f_name.replace(\".jpg\", \"\")\n",
    "\n",
    "        out = to_uint8(final_ig_mean[i])\n",
    "        np.savez_compressed(os.path.join(out_folder, f\"{base}_IG.npz\"), mask=out)\n",
    "\n",
    "        out = to_uint8(final_ig_std[i])\n",
    "        np.savez_compressed(os.path.join(out_folder, f\"{base}_IG_std.npz\"), mask=out)\n",
    "\n",
    "        out = to_uint8(final_gc_mean[i])\n",
    "        np.savez_compressed(os.path.join(out_folder, f\"{base}_GC.npz\"), mask=out)\n",
    "\n",
    "        out = to_uint8(final_gc_std[i])\n",
    "        np.savez_compressed(os.path.join(out_folder, f\"{base}_GC_std.npz\"), mask=out)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "BATCH_SIZE = 32  # Ajuste conforme a GPU\n",
    "\n",
    "for video_path in tqdm(os.listdir(path_icopevid_frames)[111:]):\n",
    "    if video_path.endswith('.pkl'):\n",
    "        continue\n",
    "\n",
    "    # Output folder for final ensemble XAI (mean + std)\n",
    "    out_folder = os.path.join(path_experiments, \"icopevid\", \"xai_masks_ENSEMBLE\", video_path)\n",
    "    create_folder(out_folder)\n",
    "\n",
    "    dataset = iCOPEVidDataset(os.path.join(path_icopevid_frames, video_path), model_name)\n",
    "\n",
    "    batch_imgs  = []\n",
    "    batch_files = []\n",
    "\n",
    "    for img, blank, file_name in dataset:\n",
    "        if blank:\n",
    "            continue\n",
    "\n",
    "        batch_imgs.append(img)\n",
    "        batch_files.append(file_name)\n",
    "\n",
    "        if len(batch_imgs) == BATCH_SIZE:\n",
    "            process_batch(batch_imgs, batch_files, out_folder)\n",
    "            batch_imgs  = []\n",
    "            batch_files = []\n",
    "\n",
    "    # Tail: process last partial batch\n",
    "    process_batch(batch_imgs, batch_files, out_folder)\n",
    "\n",
    "    # Cleanup per video\n",
    "    del dataset, batch_imgs, batch_files\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os, gc, pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "\n",
    "# Single colormap matching your green→yellow→red\n",
    "_CMAP = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"green\", \"yellow\", \"red\"])\n",
    "\n",
    "def _colorize(mask_norm, cmap=_CMAP):\n",
    "    # mask_norm: float [0,1], shape (H, W)\n",
    "    mask_norm = np.asarray(mask_norm, dtype=np.float32)\n",
    "    mask_norm = np.clip(mask_norm, 0.0, 1.0)\n",
    "    rgba = cmap(mask_norm)                  # (H, W, 4), float [0,1]\n",
    "    return (rgba[..., :3] * 255).astype(np.uint8)  # (H, W, 3), uint8\n",
    "\n",
    "def _maybe_resize_2d(arr, size):\n",
    "    # size is (W, H). If arr is scalar, return as-is. If 2D, resize to (W,H) if needed.\n",
    "    if np.isscalar(arr):\n",
    "        return arr\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 2 and (arr.shape[1] != size[0] or arr.shape[0] != size[1]):\n",
    "        arr = cv2.resize(arr, size, interpolation=cv2.INTER_LINEAR)\n",
    "    return arr\n",
    "\n",
    "def _blend_rgb(base_rgb, heat_rgb, alpha):\n",
    "    # base_rgb/heat_rgb: uint8 (H, W, 3). alpha: scalar or (H, W)\n",
    "    if np.isscalar(alpha):\n",
    "        a = float(alpha)\n",
    "        return cv2.addWeighted(heat_rgb, a, base_rgb, 1.0 - a, 0.0)\n",
    "    a = np.clip(alpha, 0.0, 1.0).astype(np.float32)\n",
    "    a = a[..., None]  # (H, W, 1)\n",
    "    out = base_rgb.astype(np.float32) * (1.0 - a) + heat_rgb.astype(np.float32) * a\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "def create_xais(images_dir, xai_dir, model, num_workers=None):\n",
    "    # Resolve output size based on model family\n",
    "    size = (224, 224) if (\"VGGFace\" in model or \"ViT\" in model) else (120, 120)\n",
    "\n",
    "    os.makedirs(xai_dir, exist_ok=True)\n",
    "\n",
    "    # Collect frame files to process\n",
    "    frame_files = [\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if (\"blank\" not in f) and f.lower().endswith((\".jpg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    def _paths(file_name):\n",
    "        # Matches your naming (if .png, it keeps .png in stem, as in your code)\n",
    "        return (\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '_GC.npz')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '_IG.npz')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '_GC_std.npz')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '_IG_std.npz')),\n",
    "        )\n",
    "\n",
    "    def _out_paths(file_name):\n",
    "        return (\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '._GC.jpg')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '._IG.jpg')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '._GC_std.jpg')),\n",
    "            os.path.join(xai_dir, file_name.replace('.jpg', '._IG_std.jpg')),\n",
    "        )\n",
    "\n",
    "    def _process_one(file_name):\n",
    "        try:\n",
    "            # Read frame\n",
    "            img_bgr = cv2.imread(os.path.join(images_dir, file_name))\n",
    "            if img_bgr is None:\n",
    "                return False\n",
    "            img_rgb = cv2.cvtColor(cv2.resize(img_bgr, size), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Load masks\n",
    "            gc_pkl, ig_pkl, gc_std_pkl, ig_std_pkl = _paths(file_name)\n",
    "            mask_gc = np.load(gc_pkl)['mask'].astype(np.float32) / 255.0\n",
    "            mask_ig = np.load(ig_pkl)['mask'].astype(np.float32) / 255.0\n",
    "            mask_gc_std = np.load(gc_std_pkl)['mask'].astype(np.float32) / 255.0\n",
    "            mask_ig_std = np.load(ig_std_pkl)['mask'].astype(np.float32) / 255.0\n",
    "\n",
    "            # Normalize and alphas using your existing helper\n",
    "            mask_gc_norm, alpha_gc       = attribution_mask_processing(mask_gc)\n",
    "            mask_ig_norm, alpha_ig       = attribution_mask_processing(mask_ig)\n",
    "            mask_gc_std_norm, alpha_gc_s = attribution_mask_processing(mask_gc_std)\n",
    "            mask_ig_std_norm, alpha_ig_s = attribution_mask_processing(mask_ig_std)\n",
    "\n",
    "            # Ensure masks/alphas match the target size\n",
    "            mask_gc_norm   = _maybe_resize_2d(mask_gc_norm, size)\n",
    "            mask_ig_norm   = _maybe_resize_2d(mask_ig_norm, size)\n",
    "            mask_gc_std_n  = _maybe_resize_2d(mask_gc_std_norm, size)\n",
    "            mask_ig_std_n  = _maybe_resize_2d(mask_ig_std_norm, size)\n",
    "            alpha_gc       = _maybe_resize_2d(alpha_gc, size)\n",
    "            alpha_ig       = _maybe_resize_2d(alpha_ig, size)\n",
    "            alpha_gc_s     = _maybe_resize_2d(alpha_gc_s, size)\n",
    "            alpha_ig_s     = _maybe_resize_2d(alpha_ig_s, size)\n",
    "\n",
    "            # Colorize and blend\n",
    "            heat_gc     = _colorize(mask_gc_norm)\n",
    "            heat_ig     = _colorize(mask_ig_norm)\n",
    "            heat_gc_std = _colorize(mask_gc_std_n)\n",
    "            heat_ig_std = _colorize(mask_ig_std_n)\n",
    "\n",
    "            out_gc     = _blend_rgb(img_rgb, heat_gc, alpha_gc)\n",
    "            out_ig     = _blend_rgb(img_rgb, heat_ig, alpha_ig)\n",
    "            out_gc_std = _blend_rgb(img_rgb, heat_gc_std, alpha_gc_s)\n",
    "            out_ig_std = _blend_rgb(img_rgb, heat_ig_std, alpha_ig_s)\n",
    "\n",
    "            # Save\n",
    "            out_gc_path, out_ig_path, out_gc_s_path, out_ig_s_path = _out_paths(file_name)\n",
    "            cv2.imwrite(out_gc_path,     cv2.cvtColor(out_gc, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(out_ig_path,     cv2.cvtColor(out_ig, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(out_gc_s_path,   cv2.cvtColor(out_gc_std, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(out_ig_s_path,   cv2.cvtColor(out_ig_std, cv2.COLOR_RGB2BGR))\n",
    "            return True\n",
    "        except Exception:\n",
    "            # Optional: log per-file errors for debugging\n",
    "            # print(f\"[warn] {file_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "    max_workers = num_workers or (os.cpu_count() or 4)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        list(ex.map(_process_one, frame_files))  # simple parallel map\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [2:15:19<00:00, 67.67s/it] \n"
     ]
    }
   ],
   "source": [
    "for video_path in tqdm(os.listdir(path_experiments + \"\\\\icopevid\\\\xai_masks_ENSEMBLE\")):\n",
    "    if not video_path.endswith(\".pkl\"):\n",
    "        create_xais(\n",
    "            os.path.join(path_icopevid_frames, video_path),\n",
    "            os.path.join(path_experiments, \"icopevid\", \"xai_masks_ENSEMBLE\", video_path),\n",
    "            model_name,\n",
    "            num_workers=8  # tune 4–12 based on your disk/CPU\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain Signal Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validate import validation_metrics\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import glob\n",
    "from scipy.stats import entropy\n",
    "from scipy import integrate\n",
    "from scipy.signal import butter, lfilter, freqz, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def butter_lowpass_filter(data, cutoff=2, fs=30, order=2):\n",
    "    normal_cutoff = cutoff / (0.5 * fs)\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "def get_hist(signal):\n",
    "    return np.histogram(signal, bins=np.linspace(0.0, 1.0, 11))\n",
    "\n",
    "def get_probs(signal):\n",
    "    hist, _ = get_hist(signal)\n",
    "    return hist / len(signal)\n",
    "\n",
    "def get_entropy(signal):\n",
    "    pk = get_probs(signal)\n",
    "    return entropy(pk, base=2)\n",
    "\n",
    "def get_entropy_curve(signal):\n",
    "    hist, bin_edges = get_hist(signal)\n",
    "    pk = hist / len(signal)\n",
    "    entropies = []\n",
    "    for x in signal:\n",
    "        idx = np.digitize(x, bin_edges, right=True)\n",
    "        pkx = pk[idx-1]\n",
    "        entropies.append(-((pkx * np.log(pkx)) / np.log(2)))\n",
    "    return entropies\n",
    "\n",
    "def get_auc(signal):\n",
    "    return integrate.trapezoid(signal) / len(signal)\n",
    "\n",
    "def get_auc_curve(signal):\n",
    "    window_size = 10\n",
    "    step = 1\n",
    "    auc_curve = []\n",
    "    for x in range(0, len(signal)-window_size, step):\n",
    "        window = signal[x:x+window_size]\n",
    "        auc_curve.append(get_auc(window))\n",
    "    return auc_curve\n",
    "\n",
    "def interp_curve(signal):\n",
    "    time = np.linspace(0,len(signal),len(signal))\n",
    "    # Find indices of missing values\n",
    "    missing_indices = np.isnan(signal)\n",
    "    # Find indices of non-missing values\n",
    "    valid_indices = ~missing_indices\n",
    "    # Perform linear interpolation\n",
    "    interpolated_values = np.interp(time[missing_indices], time[valid_indices], signal[valid_indices])\n",
    "    # Replace missing values with interpolated values\n",
    "    signal[missing_indices] = interpolated_values\n",
    "\n",
    "    return signal\n",
    "\n",
    "def fill_nan(signal):\n",
    "    # Find indices of missing values\n",
    "    missing_indices = np.isnan(signal)\n",
    "    # Find indices of non-missing values\n",
    "    valid_indices = ~missing_indices\n",
    "    # Perform linear interpolation\n",
    "    mean = np.mean(signal[valid_indices])\n",
    "    # Replace missing values with interpolated values\n",
    "    signal[missing_indices] = mean\n",
    "\n",
    "    return signal\n",
    "\n",
    "\n",
    "def peak_to_peak(signal):\n",
    "    return (np.abs(np.max(signal)) - np.abs(np.min(signal)))\n",
    "\n",
    "def derivative(signal, power=False):\n",
    "    if power:\n",
    "        return np.power(np.diff(signal),2)\n",
    "    else:\n",
    "        return np.diff(signal)\n",
    "    \n",
    "def RMS(signal):\n",
    "    return np.sqrt(np.mean(np.square(signal)))\n",
    "\n",
    "def thresh_crossing(signal, thresh=0.5):\n",
    "    return len(np.where(np.diff(np.sign(signal-thresh)))[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"VGGFace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_experiments,'icopevid',f'results_icopevid_MCDP_50_0.5.pkl'), 'rb') as f:\n",
    "    results_video = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdp = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCDP predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "probs = []\n",
    "\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"green\",\"yellow\",\"red\"])\n",
    "plt.style.use('utils\\plotstyle.mplstyle')\n",
    "\n",
    "if \"NCNN\" in model:\n",
    "    theta_1 = 0.542\n",
    "    theta_3 = 0.047\n",
    "    lower_tresh = 0.1\n",
    "    upper_tresh = 0.8\n",
    "elif \"VGGNB\" in model:\n",
    "    theta_1 = 0.494\n",
    "    theta_3 = 0.0447\n",
    "    lower_tresh = 0.2\n",
    "    upper_tresh = 0.8\n",
    "else:\n",
    "    theta_1 = 0.431\n",
    "\n",
    "\n",
    "for video in tqdm(results_video.keys()):\n",
    "\n",
    "    full_frames_path = os.path.join(path_icopevid_frames,video)\n",
    "    img_files = os.listdir(full_frames_path)\n",
    "    frame_seq = []\n",
    "\n",
    "    gc_frame_seq = []\n",
    "    gc_alpha_seq = []\n",
    "\n",
    "    ig_frame_seq = []\n",
    "    ig_alpha_seq= []\n",
    "\n",
    "    ar_frame_seq = []\n",
    "    ar_alpha_seq= []\n",
    "\n",
    "    for i in range(0, len(img_files), 30):\n",
    "        if img_files[i].endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(full_frames_path, img_files[i]))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (256,256))\n",
    "            frame_seq.append(img/255)\n",
    "\n",
    "            try:\n",
    "                mask_gc = cv2.imread(os.path.join(full_frames_path,f\"GC_{model}\", img_files[i]))\n",
    "                mask_gc = cv2.cvtColor(mask_gc, cv2.COLOR_BGR2RGB)\n",
    "                mask_gc = cv2.resize(mask_gc, (256,256))\n",
    "            except:\n",
    "                mask_gc = np.zeros((256,256,3))\n",
    "\n",
    "            try:    \n",
    "                mask_ig = cv2.imread(os.path.join(full_frames_path,f\"IG_{model}\", img_files[i]))\n",
    "                mask_ig = cv2.cvtColor(mask_ig, cv2.COLOR_BGR2RGB)\n",
    "                mask_ig = cv2.resize(mask_ig, (256,256))\n",
    "            except:\n",
    "                mask_ig = np.zeros((256,256,3))\n",
    "\n",
    "            gc_frame_seq.append(mask_gc/255)\n",
    "            ig_frame_seq.append(mask_ig/255)\n",
    "\n",
    "    frame_seq = np.hstack(frame_seq)\n",
    "\n",
    "\n",
    "    gc_seq = np.hstack(gc_frame_seq)\n",
    "    ig_seq = np.hstack(ig_frame_seq)\n",
    "\n",
    "    frame_seq = np.vstack((frame_seq, gc_seq, ig_seq))\n",
    "   \n",
    "    if \"Pain\" in video:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "    if mcdp:\n",
    "        mean_probs = []\n",
    "        std_probs = []\n",
    "        for fold in list(results_video[video].keys()):\n",
    "            for array in results_video[video][fold]:\n",
    "                array = np.asarray(array)\n",
    "                if not np.isnan(array).any():\n",
    "                    mean_probs.append(array.mean())\n",
    "                    std_probs.append(array.std())\n",
    "                else:\n",
    "                    mean_probs.append(np.nan)\n",
    "                    std_probs.append(np.nan)\n",
    "\n",
    "        mean_probs = interp_curve(np.asarray(mean_probs))\n",
    "        std_probs = interp_curve(np.asarray(std_probs))\n",
    "        \n",
    "        ma_window = 30\n",
    "        ma_probs = moving_average(mean_probs, ma_window)\n",
    "        std_ma_probs = moving_average(std_probs, ma_window)\n",
    "    \n",
    "    else:\n",
    "        stacked_probs = []\n",
    "        for fold in list(results_video[video].keys()):\n",
    "            stacked_probs.append(interp_curve(results_video[video][fold]))\n",
    "\n",
    "        stacked_probs = np.array(stacked_probs)\n",
    "        mean_probs = stacked_probs.mean(axis=0)\n",
    "        \n",
    "        ma_window = 30\n",
    "        ma_probs = moving_average(mean_probs, ma_window)\n",
    "        \n",
    "    preds.append((ma_probs.mean() >= theta_1).astype('int'))\n",
    "    probs.append(ma_probs.mean())\n",
    "\n",
    "    time = np.linspace(0,20,len(ma_probs))\n",
    "\n",
    "    #idx_precise = (ma_probs <= lower_tresh) & (ma_probs >= upper_tresh)\n",
    "    #idx_certain = std_ma_probs <= theta_3\n",
    "\n",
    "    #ma_probs_precision = ma_probs[idx_precision]\n",
    "    #std_ma_precision = std_ma_probs[idx_precision]\n",
    "    #time_precision = time[idx_precision]\n",
    "\n",
    "    #ma_probs_imprecision = ma_probs[~idx_precision]\n",
    "    #std_ma_imprecision = std_ma_probs[~idx_precision]\n",
    "    #time_imprecision = time[~idx_precision]\n",
    "\n",
    "    #ma_probs_precision = np.ma.masked_where(std_ma_probs > theta_3, ma_probs) # é ao contratrio essa função\n",
    "    #ma_probs_imprecision = np.ma.masked_where(std_ma_probs <= theta_3, ma_probs)\n",
    "\n",
    "    #std_ma_precision = np.ma.masked_where(std_ma_probs > theta_3, std_ma_probs) # é ao contratrio essa função\n",
    "    #std_ma_imprecision = np.ma.masked_where(std_ma_probs <= theta_3, std_ma_probs)\n",
    "\n",
    "    idx = np.where(np.diff(np.sign(ma_probs-theta_1)))[0]\n",
    "   \n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "\n",
    "    plt.plot(time, ma_probs, 'k')\n",
    "    #plt.plot(time, ma_probs_precision, 'k', time, ma_probs_imprecision, 'r')\n",
    "    #plt.plot(time_audio, (audio_signal*0.8)+0.5, 'b', alpha=0.2)\n",
    "    #plt.fill_between(x=time, y1=ma_probs+std_ma_probs, y2=ma_probs-std_ma_probs, alpha=0.2, color=\"#000000\", edgecolor=\"none\")\n",
    "    #plt.fill_between(x=time, y1=ma_probs_precision+std_ma_precision, y2=ma_probs_precision-std_ma_precision, alpha=0.2, color=\"#000000\", edgecolor=\"none\")\n",
    "    #plt.fill_between(x=time, y1=ma_probs_imprecision+std_ma_imprecision, y2=ma_probs_imprecision-std_ma_imprecision, alpha=0.2, color=\"red\", edgecolor=\"none\")\n",
    "\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.xlim([-0.05,20])\n",
    "\n",
    "    label = \"Pain\" if \"Pain\" in video else \"No Pain\"\n",
    "\n",
    "    temp_1 = \"Pain\" if ma_probs.mean() >= theta_1 else \"No Pain\"\n",
    "\n",
    "    #temp_3 = \"Certo\" if std_ma_probs.mean() <= theta_3 else \"Incerto\"\n",
    "\n",
    "    #plt.title(f\"Classe real = {label} / $\\\\hat{{p}}$ = {ma_probs.mean():.4f} $\\\\rightarrow$ {temp_1} / $\\\\hat{{\\\\sigma}}$ = {std_ma_probs.mean():.4f} $\\\\rightarrow$ {temp_3} / $H$ = {get_entropy(ma_probs):.4f} / Pontos de Inflexão =  {thresh_crossing(ma_probs, theta_1)}\", fontsize=20)\n",
    "    plt.title(f\"Model = {model} / Label = {label} / $\\\\hat{{p}}$ = {ma_probs.mean():.2f} $\\\\rightarrow$ {temp_1} / Entropy = {get_entropy(ma_probs):.2f} / Crossings = {thresh_crossing(ma_probs, theta_1)}\") #, fontsize=20\n",
    "    plt.ylabel(\"Pain Probability\")\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "\n",
    "    plt.plot(time[idx],[theta_1]*len(idx), 'ro')\n",
    "\n",
    "    plt.hlines(y=theta_1, xmin=0,xmax=20, linestyle=\":\", color='#9e9e9e')\n",
    "    #plt.text(20.1, theta_1, \"$\\\\theta_{{1}}$\", fontsize=20)\n",
    "\n",
    "    #plt.hlines(y=lower_tresh, xmin=0,xmax=20, linestyle=\":\", color='#9e9e9e')\n",
    "    #plt.text(20.1, lower_tresh, \"$\\\\theta_{{2}}$\", fontsize=20)\n",
    "\n",
    "    #plt.hlines(y=upper_tresh, xmin=0,xmax=20, linestyle=\":\", color='#9e9e9e')\n",
    "    #plt.text(20.1, upper_tresh, \"$\\\\theta_{{2}}$\", fontsize=20)\n",
    "\n",
    "    #plt.text(0.5, 0.8, f\"$\\\\hat{{p}}$ = {ma_probs.mean():.4f}\\n\"+\n",
    "                       #f\"$\\\\hat{{\\\\sigma}}$ = {std_ma_probs.mean():.4f}\\n\"+\n",
    "                       #f\"$H$ = {get_entropy(ma_probs):.4f}\\n\"+\n",
    "                       #f\"Pontos de Inflexão =  {thresh_crossing(ma_probs, theta_1)}\")\n",
    "\n",
    "    #plt.legend(loc='upper right')\n",
    "\n",
    "    #plt.subplot(5,1,2)\n",
    "    #entropies = get_entropy_curve(ma_probs)\n",
    "    #plt.plot(entropies, label=f'Entropy: {get_entropy(ma_probs):.4f}', color=\"#000000\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.xlim([-0.05,600])\n",
    "    #plt.ylim([-0.05,1.05])\n",
    "\n",
    "    #plt.subplot(6,1,3)\n",
    "    #auc_curve = get_auc_curve(ma_probs)\n",
    "    #plt.plot(auc_curve, label=f'AUC: {get_auc(ma_probs):.4f}', color=\"#000000\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    #plt.xlim([-0.05,600])\n",
    "    #plt.ylim([-0.05,1.05])\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(frame_seq)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.text(-50, 125, 'Frames', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 375, 'GC', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 625, 'IG', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "\n",
    "    #plt.subplot(3,1,3)\n",
    "    #plt.specgram(audio_signal, Fs=48000, NFFT=512, cmap='jet')\n",
    "\n",
    "    #plt.subplot(4,1,3)\n",
    "    #plt.imshow(frame_seq)\n",
    "    #plt.imshow(gc_seq, cmap=cmap, alpha=gc_alpha_seq)\n",
    "    #plt.axis('off')\n",
    "\n",
    "    #plt.subplot(4,1,4)\n",
    "    #plt.imshow(frame_seq)\n",
    "    #plt.imshow(ig_seq, cmap=cmap, alpha=ig_alpha_seq)\n",
    "    #plt.axis('off')\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(os.path.join(\"C:\\\\Users\\\\leonardo\\\\Desktop\\\\icopevid_results\", f'{video}_{model}.pdf'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(validation_metrics(np.array(preds), np.array(probs), np.array(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ALL PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"green\",\"yellow\",\"red\"])\n",
    "plt.style.use('utils\\plotstyle.mplstyle')\n",
    "\n",
    "def stack_XAI(img_files, model):\n",
    "    gc_frame_seq = []\n",
    "    ig_frame_seq = []\n",
    "\n",
    "    for i in range(0, len(img_files), 30):\n",
    "        if img_files[i].endswith('.jpg'):\n",
    "\n",
    "            try:\n",
    "                mask_gc = cv2.imread(os.path.join(full_frames_path,f\"GC_{model}\", img_files[i]))\n",
    "                mask_gc = cv2.cvtColor(mask_gc, cv2.COLOR_BGR2RGB)\n",
    "                mask_gc = cv2.resize(mask_gc, (256,256))\n",
    "            except:\n",
    "                mask_gc = np.zeros((256,256,3))\n",
    "\n",
    "            try:    \n",
    "                mask_ig = cv2.imread(os.path.join(full_frames_path,f\"IG_{model}\", img_files[i]))\n",
    "                mask_ig = cv2.cvtColor(mask_ig, cv2.COLOR_BGR2RGB)\n",
    "                mask_ig = cv2.resize(mask_ig, (256,256))\n",
    "            except:\n",
    "                mask_ig = np.zeros((256,256,3))\n",
    "\n",
    "            gc_frame_seq.append(mask_gc/255)\n",
    "            ig_frame_seq.append(mask_ig/255)\n",
    "\n",
    "    gc_seq = np.hstack(gc_frame_seq)\n",
    "    ig_seq = np.hstack(ig_frame_seq)\n",
    "\n",
    "    return np.vstack((gc_seq, ig_seq))\n",
    "\n",
    "with open(os.path.join(path_icopevid_frames,f'results_NCNN_test.pkl'), 'rb') as f:\n",
    "    results_video_NCNN = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(path_icopevid_frames,f'results_VGGNB_test.pkl'), 'rb') as f:\n",
    "    results_video_VGGNB = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(path_icopevid_frames,f'results_ViTNB_test.pkl'), 'rb') as f:\n",
    "    results_video_ViTNB = pickle.load(f)\n",
    "\n",
    "for video in results_video_NCNN.keys():\n",
    "\n",
    "    full_frames_path = os.path.join(path_icopevid_frames,video)\n",
    "    img_files = os.listdir(full_frames_path)[5:] # MELHORAR ISSO, USEI PARA FICAR IGUAL AO PAPER\n",
    "    frame_seq = []\n",
    "\n",
    "    for i in range(0, len(img_files), 30):\n",
    "        if img_files[i].endswith('.jpg'):\n",
    "            img = cv2.imread(os.path.join(full_frames_path, img_files[i]))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (256,256))\n",
    "            frame_seq.append(img/255)\n",
    "\n",
    "    frame_seq = np.hstack(frame_seq)\n",
    "   \n",
    "    VGGNB_stack_xai = stack_XAI(img_files, 'VGGNB_test')\n",
    "    NCNN_stack_xai = stack_XAI(img_files,'NCNN_test')\n",
    "    ViTNB_stack_xai = stack_XAI(img_files,'ViTNB_test')\n",
    "\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.subplot(5,1,1)\n",
    "\n",
    "    for results_video ,name in zip([results_video_VGGNB, results_video_NCNN, results_video_ViTNB],['VGG-Face', 'N-CNN', \"ViT-B/16\"]):\n",
    "\n",
    "        stacked_probs = []\n",
    "        for fold in list(results_video[video].keys()):\n",
    "            if \"N-CNN\" in name and fold == \"9\":\n",
    "                stacked_probs.append(interp_curve(results_video[video][fold]))\n",
    "            elif \"VGG\" in name and fold == \"7\":\n",
    "                stacked_probs.append(interp_curve(results_video[video][fold]))\n",
    "            elif \"ViT\" in name and fold == \"8\":\n",
    "                stacked_probs.append(interp_curve(results_video[video][fold]))\n",
    "\n",
    "        stacked_probs = np.array(stacked_probs)\n",
    "        mean_probs = stacked_probs.mean(axis=0)\n",
    "        \n",
    "        ma_window = 30\n",
    "        ma_probs = moving_average(mean_probs, ma_window)\n",
    "\n",
    "        time = np.linspace(0,20,len(ma_probs))\n",
    "\n",
    "        \n",
    "\n",
    "        if \"N-CNN\" in name:\n",
    "            theta_1 = 0.542\n",
    "            color = '#FF9500'\n",
    "            marker = 'o'\n",
    "            temp_1 = \"Pain\" if ma_probs.mean() >= theta_1 else \"No Pain\"\n",
    "            legenda = f\"{name} / $\\\\hat{{p}}$ = {ma_probs.mean():.2f} $\\\\rightarrow$ {temp_1} / Entropy = {get_entropy(ma_probs):.2f} / Crossings = {thresh_crossing(ma_probs, theta_1)}\"\n",
    "        elif \"VGG\" in name:\n",
    "            theta_1 = 0.494\n",
    "            color = '#0C5DA5'\n",
    "            marker = 's'\n",
    "            temp_1 = \"Pain\" if ma_probs.mean() >= theta_1 else \"No Pain\"\n",
    "            legenda = f\"{name} / $\\\\hat{{p}}$ = {ma_probs.mean():.2f} $\\\\rightarrow$ {temp_1} / Entropy = {get_entropy(ma_probs):.2f} / Crossings = {thresh_crossing(ma_probs, theta_1)}\"\n",
    "        else:\n",
    "            theta_1 = 0.431\n",
    "            color = '#00B945'\n",
    "            marker = 'P'\n",
    "            temp_1 = \"Pain\" if ma_probs.mean() >= theta_1 else \"No Pain\"\n",
    "            legenda = f\"{name} $\\\\text{{}}$ / $\\\\hat{{p}}$ = {ma_probs.mean():.2f} $\\\\rightarrow$ {temp_1} / Entropy = {get_entropy(ma_probs):.2f} / Crossings = {thresh_crossing(ma_probs, theta_1)}\"\n",
    "        \n",
    "        plt.plot(time, ma_probs, label=legenda, linewidth=2)\n",
    "\n",
    "        idx = np.where(np.diff(np.sign(ma_probs-theta_1)))[0]\n",
    "        plt.scatter(time[idx],[theta_1]*len(idx), color=color, marker=marker, edgecolor='k', s=50, zorder=10, clip_on=False)\n",
    "\n",
    "    plt.ylim([-0.05,1.05])\n",
    "    plt.xlim([-0.05,20])\n",
    "\n",
    "    label = \"Pain\" if \"Pain\" in video else \"No Pain\"\n",
    "\n",
    "    plt.ylabel(\"Pain Probability\")\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "\n",
    "\n",
    "    #plt.hlines(y=0.5, xmin=0,xmax=20, linestyle=\":\", color='#9e9e9e')\n",
    "\n",
    "    plt.legend(title=f\"Ground-Truth Label = {label}\", loc='upper left', bbox_to_anchor=(-0.05, 1.4), frameon=False, ncol=3)\n",
    "    \n",
    "    plt.subplot(5,1,2)\n",
    "    plt.imshow(frame_seq)\n",
    "    plt.axis('off')\n",
    "    plt.text(-50, 125, 'Frames', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "    plt.subplot(5,1,3)\n",
    "    plt.imshow(VGGNB_stack_xai)\n",
    "    plt.axis('off')\n",
    "    plt.text(-150, 250, 'VGG-Face', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 125, 'GC', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 375, 'IG', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "    plt.subplot(5,1,4)\n",
    "    plt.imshow(NCNN_stack_xai)\n",
    "    plt.axis('off')\n",
    "    plt.text(-150, 250, 'N-CNN', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 125, 'GC', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 375, 'IG', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "    plt.subplot(5,1,5)\n",
    "    plt.imshow(ViTNB_stack_xai)\n",
    "    plt.axis('off')\n",
    "    plt.text(-150, 250, 'ViT-B/16', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 125, 'GC', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "    plt.text(-50, 375, 'IG', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.savefig(os.path.join(\"C:\\\\Users\\\\leonardo\\\\Desktop\\\\icopevid_results\", f'{video}_ALL.pdf'), dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"VGGNB_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_icopevid_frames,f'results_{model}_MCDP.pkl'), 'rb') as f:\n",
    "    results_video = pickle.load(f)\n",
    "\n",
    "if \"NCNN\" in model:\n",
    "    theta_1 = 0.542\n",
    "    theta_3 = 0.047\n",
    "    lower_tresh = 0.1\n",
    "    upper_tresh = 0.8\n",
    "elif \"VGGNB\" in model:\n",
    "    theta_1 = 0.494\n",
    "    theta_3 = 0.0447\n",
    "    lower_tresh = 0.2\n",
    "    upper_tresh = 0.8\n",
    "else:\n",
    "    theta_1 = 0.431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probs = []\n",
    "all_entropy = []\n",
    "all_crossings = []\n",
    "all_preds = []\n",
    "all_curves = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "best_fold = '9'\n",
    "\n",
    "for video in results_video.keys():\n",
    "    # se MCDP\n",
    "    mean_probs = []\n",
    "    for array in results_video[video][best_fold]:\n",
    "        array = np.asarray(array)\n",
    "        if not np.isnan(array).any():\n",
    "            mean_probs.append(array.mean())\n",
    "        else:\n",
    "            mean_probs.append(np.nan)\n",
    "\n",
    "    mean_probs = np.asarray(mean_probs)\n",
    "    \n",
    "    interpoleted_curve = interp_curve(mean_probs)\n",
    "    filtered_curve = moving_average(interpoleted_curve, 30)\n",
    "    \n",
    "    # se normal\n",
    "    #plt.figure(figsize=(20,5))\n",
    "    #interpoleted_curve = interp_curve(results_video[video][best_fold])\n",
    "    #filtered_curve = moving_average(interpoleted_curve, 30)\n",
    "    #plt.plot(filtered_curve)\n",
    "    #plt.ylim([0,1.01])\n",
    "    #plt.show()\n",
    "\n",
    "    all_curves.append(filtered_curve)\n",
    "    all_probs.append(filtered_curve.mean())\n",
    "    all_preds.append(filtered_curve.mean() >= theta_1)\n",
    "    all_entropy.append(get_entropy(filtered_curve))\n",
    "    all_crossings.append(thresh_crossing(filtered_curve, theta_1))\n",
    "    \n",
    "    if \"Pain\" in video:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "all_entropy = np.array(all_entropy)\n",
    "all_crossings = np.array(all_crossings)\n",
    "all_preds = np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = validation_metrics(all_preds, all_probs, np.array(labels))\n",
    "accuracy = metrics['Accuracy']\n",
    "f1 = metrics['F1 Score']\n",
    "precision = metrics['Precision']\n",
    "sensitivity = metrics['Sensitivity']\n",
    "specificity = metrics['Specificity']\n",
    "auc = metrics['AUC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({'video':results_video.keys(),'probs':all_probs, 'entropy':all_entropy, 'crossings':all_crossings, 'labels':labels, 'preds': all_preds, 'curves':all_curves})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento dos sinais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe[['entropy','crossings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia =[] \n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\").fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,10),inertia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\")\n",
    "kmeans.fit(X)\n",
    "\n",
    "dataframe['kmeans'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for videoname, kmeans_label in zip(dataframe['video_name'], dataframe['kmeans']):\n",
    "    for filename in os.listdir('C:\\\\Users\\\\leonardo\\\\Desktop\\\\icopevid results\\\\NCNN\\\\'):\n",
    "        if videoname in filename:\n",
    "            os.rename(os.path.join('C:\\\\Users\\\\leonardo\\\\Desktop\\\\icopevid results\\\\NCNN\\\\',filename),os.path.join(f'C:\\\\Users\\\\leonardo\\\\Desktop\\\\icopevid results\\\\NCNN\\\\{kmeans_label}',filename))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable = dataframe[dataframe['kmeans']==0]\n",
    "irregular = dataframe[dataframe['kmeans']==2]\n",
    "unstable = dataframe[dataframe['kmeans']==1]\n",
    "\n",
    "for y in [stable, irregular, unstable]:\n",
    "    print(f\"{y['probs'].mean():.4f};{y['probs'].std():.4f}\\n{y['entropy'].mean():.4f};{y['entropy'].std():.4f}\\n{np.median(y['crossings']):.4f};{y['crossings'].std():.4f}\\n\".replace('.',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0,20,571)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_t(signal, theta_1):\n",
    "\n",
    "    time = np.linspace(0,20,len(signal))\n",
    "\n",
    "    crossings = np.insert(np.where(np.diff(np.sign(signal-theta_1)))[0], 0, 0)\n",
    "    crossings = np.insert(crossings, len(crossings), len(signal)-1)\n",
    "\n",
    "    delta_t_pain = []\n",
    "    delta_t_nopain = []\n",
    "\n",
    "    for i in range(len(crossings)-1):\n",
    "        delta_t = time[crossings[i+1]] - time[crossings[i]]\n",
    "        avg_prob_delta_t = signal[crossings[i]:crossings[i+1]].mean()\n",
    "\n",
    "        if avg_prob_delta_t >= theta_1:\n",
    "            delta_t_pain.append(delta_t)\n",
    "        else:\n",
    "            delta_t_nopain.append(delta_t)\n",
    "\n",
    "    return delta_t_pain, delta_t_nopain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain Signal 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable['preds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = validation_metrics(stable['preds'], stable['probs'], stable['labels'])\n",
    "\n",
    "for values in metrics.values():\n",
    "    print(f\"{values:.4f}\".replace('.',\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "k_pain = np.array(stable[stable['labels']==1]['curves'])\n",
    "k_nopain = np.array(stable[stable['labels']==0]['curves'])\n",
    "\n",
    "mean_pain = np.mean(k_pain,axis=0)\n",
    "mean_nopain = np.mean(k_nopain,axis=0)\n",
    "\n",
    "for k in k_pain:\n",
    "    plt.plot(time, k, 'r', alpha=0.1)\n",
    "\n",
    "for k in k_nopain:\n",
    "    plt.plot(time, k, 'b', alpha=0.1)\n",
    "\n",
    "plt.plot(time, mean_pain, 'r', label='Pain')\n",
    "plt.plot(time, mean_nopain, 'b', label='Non-pain')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Stable pain signals')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Predicted Pain Probability')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"stable_signal_avg_{model}.pdf\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction, k_signal in zip([\"PAIN\", \"NO-PAIN\"],[k_pain, k_nopain]):\n",
    "\n",
    "    delta_t_pain_k = []\n",
    "    delta_t_nopain_k = []\n",
    "\n",
    "    total_pain_events = []\n",
    "    total_nopain_events = []\n",
    "\n",
    "    for signal in k_signal:\n",
    "        delta_t_pain, delta_t_nopain = get_delta_t(signal, theta_1)\n",
    "        \n",
    "        delta_t_pain_k.extend(delta_t_pain)\n",
    "        delta_t_nopain_k.extend(delta_t_nopain)\n",
    "\n",
    "        total_pain_events.append(len(delta_t_pain))\n",
    "        total_nopain_events.append(len(delta_t_nopain))\n",
    "\n",
    "\n",
    "    delta_t_pain_k = np.array(delta_t_pain_k)\n",
    "    delta_t_nopain_k = np.array(delta_t_nopain_k)\n",
    "\n",
    "    total_pain_events = np.array(total_pain_events)\n",
    "    total_nopain_events = np.array(total_nopain_events)\n",
    "\n",
    "\n",
    "    print(f\"Label {prediction}: Painful Event duration {delta_t_pain_k.mean():.4f} ± {delta_t_pain_k.std():.2f}, # Occurances: {total_pain_events.mean():.4f} ± {total_pain_events.std():.4f}\")\n",
    "    print(f\"Label {prediction}: Non-Painful Event duration {delta_t_nopain_k.mean():.4f} ± {delta_t_nopain_k.std():.2f} # Occurances: {total_nopain_events.mean():.4f} ± {total_nopain_events.std():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain Signal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irregular['preds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = (validation_metrics(irregular['preds'], irregular['probs'], irregular['labels']))\n",
    "\n",
    "for values in metrics.values():\n",
    "    print(f\"{values:.4f}\".replace('.',\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "k_pain = np.array(irregular[irregular['labels']==1]['curves'])\n",
    "k_nopain = np.array(irregular[irregular['labels']==0]['curves'])\n",
    "\n",
    "mean_pain = np.mean(k_pain,axis=0)\n",
    "mean_nopain = np.mean(k_nopain,axis=0)\n",
    "\n",
    "for k in k_pain:\n",
    "    plt.plot(time, k, 'r', alpha=0.1)\n",
    "\n",
    "for k in k_nopain:\n",
    "    plt.plot(time, k, 'b', alpha=0.1)\n",
    "\n",
    "plt.plot(time, mean_pain, 'r', label='Pain')\n",
    "plt.plot(time, mean_nopain, 'b', label='Non-pain')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Irregular pain signals')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Predicted Pain Probability')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"irregular_signal_avg_{model}.pdf\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction, k_signal in zip([\"PAIN\", \"NO-PAIN\"],[k_pain, k_nopain]):\n",
    "\n",
    "    delta_t_pain_k = []\n",
    "    delta_t_nopain_k = []\n",
    "\n",
    "    total_pain_events = []\n",
    "    total_nopain_events = []\n",
    "\n",
    "    for signal in k_signal:\n",
    "        delta_t_pain, delta_t_nopain = get_delta_t(signal, theta_1)\n",
    "        \n",
    "        delta_t_pain_k.extend(delta_t_pain)\n",
    "        delta_t_nopain_k.extend(delta_t_nopain)\n",
    "\n",
    "        total_pain_events.append(len(delta_t_pain))\n",
    "        total_nopain_events.append(len(delta_t_nopain))\n",
    "\n",
    "\n",
    "    delta_t_pain_k = np.array(delta_t_pain_k)\n",
    "    delta_t_nopain_k = np.array(delta_t_nopain_k)\n",
    "\n",
    "    total_pain_events = np.array(total_pain_events)\n",
    "    total_nopain_events = np.array(total_nopain_events)\n",
    "\n",
    "\n",
    "    print(f\"Label {prediction}: Painful Event duration {delta_t_pain_k.mean():.4f} ± {delta_t_pain_k.std():.2f}, # Occurances: {total_pain_events.mean():.4f} ± {total_pain_events.std():.4f}\")\n",
    "    print(f\"Label {prediction}: Non-Painful Event duration {delta_t_nopain_k.mean():.4f} ± {delta_t_nopain_k.std():.2f} # Occurances: {total_nopain_events.mean():.4f} ± {total_nopain_events.std():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain Signal 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable['preds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = (validation_metrics(unstable['preds'], unstable['probs'], unstable['labels']))\n",
    "\n",
    "for values in metrics.values():\n",
    "    print(f\"{values:.4f}\".replace('.',\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "k_pain = np.array(unstable[unstable['labels']==1]['curves'])\n",
    "k_nopain = np.array(unstable[unstable['labels']==0]['curves'])\n",
    "\n",
    "mean_pain = np.mean(k_pain,axis=0)\n",
    "mean_nopain = np.mean(k_nopain,axis=0)\n",
    "\n",
    "for k in k_pain:\n",
    "    plt.plot(time, k, 'r', alpha=0.1)\n",
    "\n",
    "for k in k_nopain:\n",
    "    plt.plot(time, k, 'b', alpha=0.1)\n",
    "\n",
    "plt.plot(time, mean_pain, 'r', label='Pain')\n",
    "plt.plot(time, mean_nopain, 'b', label='Non-pain')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Unstable pain signals')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Predicted Pain Probability')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"unstable_signal_avg_{model}.pdf\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction, k_signal in zip([\"PAIN\", \"NO-PAIN\"],[k_pain, k_nopain]):\n",
    "\n",
    "    delta_t_pain_k = []\n",
    "    delta_t_nopain_k = []\n",
    "\n",
    "    total_pain_events = []\n",
    "    total_nopain_events = []\n",
    "\n",
    "    for signal in k_signal:\n",
    "        delta_t_pain, delta_t_nopain = get_delta_t(signal, theta_1)\n",
    "        \n",
    "        delta_t_pain_k.extend(delta_t_pain)\n",
    "        delta_t_nopain_k.extend(delta_t_nopain)\n",
    "\n",
    "        total_pain_events.append(len(delta_t_pain))\n",
    "        total_nopain_events.append(len(delta_t_nopain))\n",
    "\n",
    "\n",
    "    delta_t_pain_k = np.array(delta_t_pain_k)\n",
    "    delta_t_nopain_k = np.array(delta_t_nopain_k)\n",
    "\n",
    "    total_pain_events = np.array(total_pain_events)\n",
    "    total_nopain_events = np.array(total_nopain_events)\n",
    "\n",
    "\n",
    "    print(f\"Label {prediction}: Painful Event duration {delta_t_pain_k.mean():.4f} ± {delta_t_pain_k.std():.2f}, # Occurances: {total_pain_events.mean():.4f} ± {total_pain_events.std():.4f}\")\n",
    "    print(f\"Label {prediction}: Non-Painful Event duration {delta_t_nopain_k.mean():.4f} ± {delta_t_nopain_k.std():.2f} # Occurances: {total_nopain_events.mean():.4f} ± {total_nopain_events.std():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLuster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(stable['entropy'], stable['crossings'], marker='s', edgecolors='k', s=65, label='Stable')\n",
    "plt.scatter(irregular['entropy'], irregular['crossings'], marker='o', edgecolors='k', s=65, label='Irregular')\n",
    "plt.scatter(unstable['entropy'], unstable['crossings'], marker='P', edgecolors='k', s=65, label='Unstable')\n",
    "\n",
    "plt.xlim([-0.1,3.5])\n",
    "plt.ylim([-0.9,10.5])\n",
    "plt.xlabel('Entropy')\n",
    "plt.ylabel('Crossings')\n",
    "\n",
    "#x1 = stable['entropy'].mean()+2*stable['entropy'].std()\n",
    "#x2 = irregular['entropy'].mean()-2*irregular['entropy'].std()\n",
    "#x = np.abs(x1-x2)/2\n",
    "\n",
    "#y1 = irregular['crossings'].mean()+2*irregular['crossings'].std()\n",
    "#y2 = unstable['crossings'].mean()-2*unstable['crossings'].std()\n",
    "#y = np.abs(y1-y2)/2\n",
    "\n",
    "vline = 1.25 if \"VGG\" in model else 1.75\n",
    "hline = irregular['crossings'].max()+1\n",
    "\n",
    "#plt.vlines(vline , ymax=10, ymin=-0.5, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "#plt.hlines(hline, xmax=3.4, xmin=0, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "model_name = \"VGG-Face\" if \"VGG\" in model else \"N-CNN\"\n",
    "plt.title(f'Pain Signal Types for {model_name}')\n",
    "\n",
    "plt.savefig(f\"plot_entropy_crossings_{model}.pdf\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "plt.vlines(0.5 , ymax=1, ymin=0, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "plt.hlines(0.5, xmax=1, xmin=0, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "\n",
    "plt.text(0.75, 1, 'High Entropy', horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "plt.text(0.25, 1, 'Low Entropy', horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.text(0, 0.75, 'High Crossings', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "plt.text(0, 0.25, 'Low Crossings', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "plt.text(0.75, 0.75, f\"Unstable\\n\\nAction Needed\\n\\n Entropy$\\ge${vline:.2f}\\nCrossings$\\ge${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.75, 0.25, f\"Irregular\\n\\nAction Needed\\n\\n Entropy$\\ge${vline:.2f}\\nCrossings$<${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.25, 0.75, f\"Unsure\\n\\nAction Needed\\n\\n Entropy$<${vline:.2f}\\nCrossings$\\ge${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.25, 0.25, f\"Stable\\n\\nAction Needed\\n\\n Entropy$<${vline:.2f}\\nCrossings$<${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.fill_between([0.04, 0.46], 0.04, 0.46, color='#f25757', alpha=0.3)\n",
    "plt.fill_between([0.04, 0.46], 0.54, 0.96 , color='#f25757', alpha=0.3)\n",
    "plt.fill_between([0.54, 0.96], 0.04, 0.46 , color='#f25757', alpha=0.3)\n",
    "plt.fill_between([0.54, 0.96], 0.54, 0.96 , color='#f25757', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.title(f'For \"Pain\" predictions of {model_name}')\n",
    "\n",
    "plt.savefig(f\"pain_quadrant_{model}.png\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "\n",
    "plt.vlines(0.5 , ymax=1, ymin=0, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "plt.hlines(0.5, xmax=1, xmin=0, color='#8c8c8c', linestyles='--', linewidth=1.2)\n",
    "\n",
    "plt.text(0.75, 1, 'High Entropy', horizontalalignment='center', verticalalignment='center', weight='bold')\n",
    "plt.text(0.25, 1, 'Low Entropy', horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.text(0, 0.75, 'High Crossings', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "plt.text(0, 0.25, 'Low Crossings', horizontalalignment='center', verticalalignment='center', rotation=90)\n",
    "\n",
    "plt.text(0.75, 0.75, f\"Unstable\\n\\nAction Needed\\n\\n Entropy$\\ge${vline:.2f}\\nCrossings$\\ge${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.75, 0.25, f\"Irregular\\n\\nAction Not Needed\\n\\n Entropy$\\ge${vline:.2f}\\nCrossings$<${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.25, 0.75, f\"Unsure\\n\\nAction Needed\\n\\n Entropy$<${vline:.2f}\\nCrossings$\\ge${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "plt.text(0.25, 0.25, f\"Stable\\n\\nAction Not Needed\\n\\n Entropy$<${vline:.2f}\\nCrossings$<${hline:.0f}\", horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "plt.fill_between([0.04, 0.46], 0.04, 0.46, color='#57F257', alpha=0.3)\n",
    "plt.fill_between([0.04, 0.46], 0.54, 0.96 , color='#f25757', alpha=0.3)\n",
    "plt.fill_between([0.54, 0.96], 0.04, 0.46 , color='#57F257', alpha=0.3)\n",
    "plt.fill_between([0.54, 0.96], 0.54, 0.96 , color='#f25757', alpha=0.3)\n",
    "\n",
    "plt.title(f'For \"No-Pain\" predictions of {model_name}')\n",
    "\n",
    "plt.savefig(f\"nopain_quadrant_{model}.png\", dpi=150, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
