{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123fe012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import AffineTransform, warp\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from skimage.transform import AffineTransform, warp\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592c16e",
   "metadata": {},
   "source": [
    "# Comparative and Functional Evaluation of XAI Methods for Neonatal Pain Recognition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8394094",
   "metadata": {},
   "source": [
    "# 1. Extract XAI Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b55e30",
   "metadata": {},
   "source": [
    "XAI Explainers:\n",
    "\n",
    "- IntegratedGradients\n",
    "- Saliency\n",
    "- DeepLift\n",
    "- Occlusion\n",
    "- LayerGradCam\n",
    "- GuidedGradCam\n",
    "- Deconvolution\n",
    "- GradientShap\n",
    "- DeepLiftShap\n",
    "- Lime\n",
    "\n",
    "Baselines to Explainers:\n",
    "\n",
    "- Canny Edge Detector\n",
    "- Same XAI explainer, but with random weights initialization (for pre-trained models keep pre-trained weights?)\n",
    "\n",
    "Models:\n",
    "\n",
    "- NCNN\n",
    "- VGGFace\n",
    "- ViT_B_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf91a52",
   "metadata": {},
   "source": [
    "### XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1c513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"RGU\\\\XAI_EXPLAINERS\\\\RANDOM\"\n",
    "RANDOM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f25aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Processing model: VGGFace---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 4/50 [02:08<24:41, 32.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 269\u001b[0m\n\u001b[0;32m    266\u001b[0m spec_kwargs \u001b[38;5;241m=\u001b[39m spec\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m ctx: {})(method_ctx)\n\u001b[0;32m    267\u001b[0m attr_kwargs \u001b[38;5;241m=\u001b[39m spec_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 269\u001b[0m attributions \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mattribute(method_ctx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr_kwargs)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m spec:\n\u001b[0;32m    272\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m spec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m](attributions, method_ctx)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\captum\\log\\dummy_log.py:39\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\captum\\attr\\_core\\occlusion.py:259\u001b[0m, in \u001b[0;36mOcclusion.attribute\u001b[1;34m(self, inputs, sliding_window_shapes, strides, baselines, target, additional_forward_args, perturbations_per_eval, show_progress)\u001b[0m\n\u001b[0;32m    252\u001b[0m     shift_counts\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    254\u001b[0m             np\u001b[38;5;241m.\u001b[39madd(np\u001b[38;5;241m.\u001b[39mceil(np\u001b[38;5;241m.\u001b[39mdivide(current_shape, strides[i]))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m     )\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Use ablation attribute method\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mperturbations_per_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbations_per_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshift_counts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\captum\\attr\\_core\\feature_ablation.py:347\u001b[0m, in \u001b[0;36mFeatureAblation.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, feature_mask, perturbations_per_eval, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m    328\u001b[0m     current_inputs,\n\u001b[0;32m    329\u001b[0m     current_add_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m#   non-agg mode:\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m#     (feature_perturbed * batch_size, *initial_eval.shape[1:])\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     modified_eval: Union[Tensor, Future[Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_add_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[0;32m    355\u001b[0m         attr_progress\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\captum\\_utils\\common.py:588\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    585\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    586\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 588\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mthen(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x\u001b[38;5;241m.\u001b[39mvalue(), target))\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\### DOUTORADO\\Mestrado\\models\\VGGFace.py:52\u001b[0m, in \u001b[0;36mVGGFace.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVGGFace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leona\\anaconda3\\envs\\doutorado\\lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    Saliency,\n",
    "    DeepLift,\n",
    "    Occlusion,\n",
    "    LayerGradCam,\n",
    "    GuidedGradCam,\n",
    "    Deconvolution,\n",
    "    GradientShap,\n",
    "    DeepLiftShap,\n",
    "    Lime,\n",
    "    LayerAttribution,\n",
    ")\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "from utils.utils import create_folder, load_config\n",
    "from dataloaders.presets import PresetTransform\n",
    "from models import *\n",
    "\n",
    "def perturb_fn(inputs):\n",
    "    noise = torch.tensor(np.random.normal(0, 0.1, inputs.shape)).float().to(inputs.device)\n",
    "    return noise, torch.clip(inputs - noise, 0, 1)\n",
    "\n",
    "# --- Create superpixel feature mask for Captum ---\n",
    "def make_feature_mask(img_tensor, n_segments=100):\n",
    "    x = img_tensor.detach().cpu().squeeze(0)  # 3 x H x W\n",
    "    x_np = x.numpy()\n",
    "    x_np = (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-8)\n",
    "    x_np = np.transpose(x_np, (1, 2, 0))  # H, W, 3\n",
    "\n",
    "    seg = slic(x_np, n_segments=n_segments, compactness=10.0, sigma=0.0,\n",
    "               start_label=0, channel_axis=2)\n",
    "\n",
    "    seg_t = torch.from_numpy(seg).long().unsqueeze(0).unsqueeze(0)\n",
    "    return seg_t, seg\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def resolve_experiment(exp_name: str, device: torch.device):\n",
    "    if \"NCNN\" in exp_name:\n",
    "        model = NCNN().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 120,\n",
    "            \"transform\": PresetTransform(\"NCNN\").transforms,\n",
    "            \"layer\": model.merge_branch[0],\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    if \"VGGFace\" in exp_name:\n",
    "        model = VGGFace().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 224,\n",
    "            \"transform\": PresetTransform(\"VGGFace\").transforms,\n",
    "            \"layer\": model.VGGFace.features.conv5_3,\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    if \"ViT\" in exp_name:\n",
    "        model = ViT().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 224,\n",
    "            \"transform\": PresetTransform(\"ViT\").transforms,\n",
    "            \"layer\": model.ViT.conv_proj,\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_feature_mask(ctx: dict, n_segments: int = 100):\n",
    "    if \"feature_mask\" not in ctx:\n",
    "        mask, _ = make_feature_mask(ctx[\"input\"], n_segments=n_segments)\n",
    "        ctx[\"feature_mask\"] = mask.to(ctx[\"device\"]).contiguous()\n",
    "    return ctx[\"feature_mask\"]\n",
    "\n",
    "\n",
    "# explainer catalogue ----------------------------------------------------------\n",
    "\n",
    "EXPLAINER_SPECS = [\n",
    "    (\n",
    "        \"IntegratedGradients\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: IntegratedGradients(model),\n",
    "            \"prepare\": lambda ctx: {\"attribute\": {\"internal_batch_size\": 10}},\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Saliency\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Saliency(model),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"DeepLift\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: DeepLift(model),\n",
    "            \"prepare\": lambda ctx: {\"attribute\": {\"baselines\": ctx[\"blurred\"]}},\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Occlusion\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Occlusion(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"sliding_window_shapes\": (3, 20, 20),\n",
    "                    \"strides\": (3, 5, 5),\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GradCAM\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: LayerGradCam(model, layer),\n",
    "            \"postprocess\": lambda attr, ctx: LayerAttribution.interpolate(\n",
    "                attr, ctx[\"target_shape\"], interpolate_mode=\"bilinear\"\n",
    "            ).repeat(1, 3, 1, 1),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GuidedGradCAM\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: GuidedGradCam(model, layer),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Deconvolution\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Deconvolution(model),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GradientShap\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: GradientShap(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"baselines\": torch.zeros_like(ctx[\"input\"]),\n",
    "                    \"n_samples\": 10,\n",
    "                    \"stdevs\": 0.0,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"DeepLiftShap\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: DeepLiftShap(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\"baselines\": ctx[\"blurred\"].repeat(10, 1, 1, 1)}\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Lime\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Lime(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"baselines\": torch.zeros_like(ctx[\"input\"]),\n",
    "                    \"feature_mask\": ensure_feature_mask(ctx),\n",
    "                    \"n_samples\": 500,\n",
    "                    \"perturbations_per_eval\": 64,\n",
    "                    \"show_progress\": False,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# main pipeline ----------------------------------------------------------------\n",
    "\n",
    "for model_name in [\"VGGFace\", \"ViT_B_32\"]:\n",
    "    print(f\"---------------Processing model: {model_name}---------------\")\n",
    "\n",
    "    path_experiments = os.path.join('experiments', model_name)\n",
    "\n",
    "    all_data = defaultdict(list)\n",
    "\n",
    "    for exp in os.listdir(path_experiments):\n",
    "        if any(ext in exp for ext in (\".pkl\", \"masks\", \".png\", \".pdf\")):\n",
    "            continue\n",
    "\n",
    "        experiment_cfg = resolve_experiment(exp, device)\n",
    "        if experiment_cfg is None:\n",
    "            continue\n",
    "\n",
    "        model = experiment_cfg[\"model\"]\n",
    "        img_size = experiment_cfg[\"img_size\"]\n",
    "        transform = experiment_cfg[\"transform\"]\n",
    "        layer = experiment_cfg[\"layer\"]\n",
    "\n",
    "        path_model = os.path.join(path_experiments, exp, \"Model\", \"best_model.pt\")\n",
    "        path_yaml = os.path.join(path_experiments, exp, \"Model\", \"config.yaml\")\n",
    "        config = load_config(path_yaml)\n",
    "        test_path = config[\"path_test\"].replace(\"\\\\\", \"/\")\n",
    "\n",
    "        state_dict = torch.load(path_model, map_location=device)\n",
    "        if not RANDOM:\n",
    "            model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        explainers = {name: spec[\"factory\"](model, layer) for name, spec in EXPLAINER_SPECS}\n",
    "\n",
    "        image_files = [f for f in os.listdir(test_path) if f.lower().endswith(\".jpg\")]\n",
    "        for image_file in tqdm(image_files):\n",
    "            full_img_path = os.path.join(test_path, image_file)\n",
    "\n",
    "            img_rgb = Image.open(full_img_path).convert(\"RGB\")\n",
    "            img_rgb = img_rgb.resize((img_size, img_size))\n",
    "            img_name = os.path.splitext(image_file)[0]\n",
    "            label = 1 if img_name.split(\"_\")[3] == \"pain\" else 0\n",
    "\n",
    "            img_input = img_rgb\n",
    "\n",
    "            blurred_image = img_input.filter(ImageFilter.GaussianBlur(radius=5))\n",
    "\n",
    "            transformed = transform(img_input)\n",
    "            transformed_blurred = transform(blurred_image)\n",
    "\n",
    "            base_input = transformed.unsqueeze(0).to(device)\n",
    "            base_blurred = transformed_blurred.unsqueeze(0).to(device)\n",
    "\n",
    "            ctx_base = {\n",
    "                \"device\": device,\n",
    "                \"target_shape\": (img_size, img_size),\n",
    "                \"input_base\": base_input,\n",
    "                \"blurred\": base_blurred,\n",
    "            }\n",
    "\n",
    "            for XAI_name, spec in EXPLAINER_SPECS:\n",
    "                explainer = explainers[XAI_name]\n",
    "\n",
    "                method_ctx = dict(ctx_base)\n",
    "                method_ctx[\"input\"] = (\n",
    "                    ctx_base[\"input_base\"].clone().detach().requires_grad_(True)\n",
    "                )\n",
    "\n",
    "                if XAI_name == \"Lime\" or XAI_name == \"DeepLift\":\n",
    "                    method_ctx[\"input\"] = (\n",
    "                        ctx_base[\"input_base\"].clone().detach().requires_grad_(True).contiguous()\n",
    "                )\n",
    "\n",
    "                spec_kwargs = spec.get(\"prepare\", lambda ctx: {})(method_ctx)\n",
    "                attr_kwargs = spec_kwargs.get(\"attribute\", {})\n",
    "\n",
    "                attributions = explainer.attribute(method_ctx[\"input\"], **attr_kwargs)\n",
    "\n",
    "                if \"postprocess\" in spec:\n",
    "                    attributions = spec[\"postprocess\"](attributions, method_ctx)\n",
    "\n",
    "                attributions_np = (\n",
    "                    attributions.squeeze(0)\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .transpose(1, 2, 0)\n",
    "                )\n",
    "\n",
    "                output_dir = os.path.join(OUT_DIR, model_name, XAI_name)\n",
    "                create_folder(output_dir)\n",
    "                output_path = os.path.join(output_dir, f\"{img_name}.npz\")\n",
    "                np.savez_compressed(output_path, mask_raw=attributions_np)\n",
    "\n",
    "                all_data[\"img_path\"].append(full_img_path)\n",
    "                all_data[\"fold\"].append(os.path.basename(os.path.dirname(test_path)))\n",
    "                all_data[\"label\"].append(label)\n",
    "                all_data[\"mask_path\"].append(output_path)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    probs = model.predict(ctx_base[\"input_base\"])\n",
    "                pred = (probs >= 0.5).int()\n",
    "                all_data[\"probability\"].append(float(probs))\n",
    "                all_data[\"prediction\"].append(int(pred))\n",
    "                all_data[\"XAI_name\"].append(XAI_name)\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    dataframe = pd.DataFrame(all_data)\n",
    "    create_folder(os.path.join(OUT_DIR, model_name))\n",
    "    dataframe.to_csv(os.path.join(OUT_DIR, model_name, \"explainers.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0380699",
   "metadata": {},
   "source": [
    "### Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root = Path(r\"Datasets\\Folds\")\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "processed = 0\n",
    "skipped = []\n",
    "\n",
    "for fold_dir in sorted(input_root.iterdir()):\n",
    "    if not fold_dir.is_dir():\n",
    "        continue\n",
    "    test_dir = fold_dir / \"Test\"\n",
    "    if not test_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for img_path in test_dir.rglob(\"*\"):\n",
    "        if not img_path.is_file() or img_path.suffix.lower() not in image_exts:\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            skipped.append(str(img_path))\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 100)\n",
    "        edges = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "        edges_NCNN = cv2.resize(edges, (120,120), interpolation=cv2.INTER_NEAREST)\n",
    "        edges = cv2.resize(edges, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        out_name = f\"{img_path.stem}.npz\"\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\NCNN\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges_NCNN)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\VGGFace\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\ViT_B_32\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "print(f\"Processed {processed} images.\")\n",
    "if skipped:\n",
    "    print(\"Skipped files (failed to load):\")\n",
    "    for item in skipped:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27928b67",
   "metadata": {},
   "source": [
    "# 2. Align\n",
    "\n",
    "All masks (heatmaps) are aligned to the same common reference based on 5 face landmarks (eye right, eye left, mouth right corner, mouth left corner, and nose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce118071",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_DIR = Path(r\"Datasets\\DatasetFaces\\Landmarks\")\n",
    "ALIGN_INDICES = np.array([86, 52, 61, 88, 38])  # order matters\n",
    "ALIGN_SUBDIR = \"aligned\"\n",
    "\n",
    "# Load all landmark meshes and compute the mean reference subset.\n",
    "faces = []\n",
    "for fname in os.listdir(LANDMARK_DIR):\n",
    "    with open(LANDMARK_DIR / fname, \"rb\") as f:\n",
    "        faces.append(np.array(pickle.load(f)))\n",
    "\n",
    "faces_np = np.array(faces)\n",
    "face_mesh_mean = faces_np.mean(axis=0)\n",
    "AVG_SUBSET = face_mesh_mean[ALIGN_INDICES]\n",
    "\n",
    "def load_heatmap(path: Path) -> tuple[np.ndarray, tuple[int, int], np.dtype]:\n",
    "    mask = np.load(path)[\"mask_raw\"]\n",
    "    #mask = rgb_to_gray_and_scale(mask)\n",
    "    return cv2.resize(mask, (512, 512)), mask.shape, mask.dtype\n",
    "\n",
    "\n",
    "def load_landmarks(path: Path) -> np.ndarray:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return np.array(pickle.load(f))\n",
    "\n",
    "\n",
    "def align_heatmaps(\n",
    "    heatmap_dir: Path = Path(\"RGU\\\\TRAINED\\\\NCNN\"),\n",
    "    landmark_dir: Path = LANDMARK_DIR,\n",
    "    output_shape: tuple[int, int] | None = None,\n",
    ") -> None:\n",
    "\n",
    "    for explainer_path in tqdm(sorted(heatmap_dir.iterdir()), desc=\"Explainers\"):\n",
    "        if not explainer_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        #if not explainer_path.name == \"EdgeDetector\":\n",
    "            #continue\n",
    "\n",
    "        aligned_dir = explainer_path / ALIGN_SUBDIR\n",
    "        aligned_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for heatmap_path in sorted(explainer_path.iterdir()):\n",
    "            if heatmap_path.suffix.lower() != \".npz\":\n",
    "                continue\n",
    "\n",
    "            lm_path = landmark_dir / f\"{heatmap_path.stem}.pkl\"\n",
    "            if not lm_path.exists():\n",
    "                print(f\"Skip {heatmap_path.name} (missing landmarks at {lm_path})\")\n",
    "                continue\n",
    "\n",
    "            heatmap, original_shape, original_dtype = load_heatmap(heatmap_path)\n",
    "            sample_landmarks = load_landmarks(lm_path)\n",
    "            sample_subset = sample_landmarks[ALIGN_INDICES]\n",
    "\n",
    "            transform = AffineTransform()\n",
    "            if not transform.estimate(sample_subset, AVG_SUBSET):\n",
    "                print(f\"Failed to estimate affine transform for {heatmap_path.name}\")\n",
    "                continue\n",
    "\n",
    "            target_shape = (\n",
    "                output_shape if output_shape is not None else heatmap.shape[:2]\n",
    "            )\n",
    "            warped = warp(\n",
    "                heatmap,\n",
    "                inverse_map=transform.inverse,\n",
    "                output_shape=target_shape,\n",
    "                preserve_range=True,\n",
    "            ).astype(original_dtype, copy=False)\n",
    "\n",
    "            restored = cv2.resize(\n",
    "                warped,\n",
    "                (original_shape[1], original_shape[0]),\n",
    "            ).astype(original_dtype, copy=False)\n",
    "\n",
    "            aligned_path = aligned_dir / f\"{heatmap_path.stem}.npz\"\n",
    "            np.savez_compressed(aligned_path, mask_raw=restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tipos in [\"TRAINED\", \"RANDOM\", \"EDGEDETECTOR\"]:\n",
    "    for model in [\"NCNN\", \"VGGFace\", \"ViT_B_32\"]:\n",
    "        heatmap_dir = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\{tipos}\\\\{model}\")\n",
    "        print(f\"Aligning heatmaps in {heatmap_dir}...\")\n",
    "        align_heatmaps(heatmap_dir=heatmap_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba2b44",
   "metadata": {},
   "source": [
    "# 3. Load and Standardize\n",
    "\n",
    "for the next steps, all analysis are done with a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, method='absolute'):\n",
    "    x = np.asarray(x)\n",
    "    # Shape must be in (H, W, C)\n",
    "    if x.ndim != 2:\n",
    "        x_combined = np.sum(x, axis=2)\n",
    "    else:\n",
    "        x_combined = x\n",
    "\n",
    "    if method == 'positive':\n",
    "        x_combined = (x_combined > 0) * x_combined # positive values only\n",
    "    elif method == 'negative':\n",
    "        x_combined = (x_combined < 0) * x_combined # negative values only\n",
    "    elif method == 'absolute':\n",
    "        x_combined = np.abs(x_combined) # absolute values\n",
    "    elif method == 'all':\n",
    "        pass # use all values as they are\n",
    "\n",
    "    attr_norm = (x_combined - np.min(x_combined)) / (np.max(x_combined) - np.min(x_combined) + 1e-20)\n",
    "\n",
    "    return attr_norm\n",
    "\n",
    "\n",
    "def load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=True):\n",
    "    \"\"\"Return {sample_id: {method_name: bool_mask}} for every .npz in the folders.\"\"\"\n",
    "    samples = {}\n",
    "    for xai_dir in map(Path, xai_dirs):\n",
    "        method = xai_dir.name\n",
    "        for npz_file in tqdm(sorted(xai_dir.glob(\"aligned/*.npz\") if aligned else xai_dir.glob(\"*.npz\"))):\n",
    "            with np.load(npz_file) as archive:\n",
    "                if mask_key not in archive:\n",
    "                    raise KeyError(f\"{npz_file} missing '{mask_key}' array.\")\n",
    "                mask = archive[mask_key]\n",
    "                mask = normalize(mask, method='absolute')                \n",
    "            samples.setdefault(npz_file.stem, {})[method] = mask\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_aux = \"NCNN\"\n",
    "\n",
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\TRAINED\\\\{model_name_aux}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_trained = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=True)\n",
    "\n",
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\RANDOM\\\\{model_name_aux}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_random = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=True)\n",
    "\n",
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\{model_name_aux}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_edgedetector = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_explanations(*dicts):\n",
    "    merged = {}\n",
    "    for data in dicts:\n",
    "        for sample_id, expl_dict in data.items():\n",
    "            inner = merged.setdefault(sample_id, {})\n",
    "            for explainer, value in expl_dict.items():\n",
    "                name = explainer\n",
    "                while name in inner:\n",
    "                    name = f\"{explainer}_RANDOM\"\n",
    "                inner[name] = value\n",
    "    return merged\n",
    "\n",
    "samples = merge_explanations(samples_trained, samples_random, samples_edgedetector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b0c0a",
   "metadata": {},
   "source": [
    "# 4. Show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c980515",
   "metadata": {},
   "source": [
    "## Sample Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xai_overlays(\n",
    "    explanations: Mapping[str, Mapping[str, np.ndarray]],\n",
    "    image_root: Path | str = Path(r\"D:\\### DOUTORADO\\Mestrado\\Datasets\\DatasetFaces\\Images\"),\n",
    "    figsize_per_panel: tuple[float, float] = (4.0, 4.0),\n",
    "    cmap: str = \"jet\",\n",
    "    alpha: float = 0.45,\n",
    ") -> None:\n",
    "    \"\"\"Plot each XAI explainer overlayed on its source image.\"\"\"\n",
    "    image_root = Path(image_root)\n",
    "\n",
    "    def _load_image(sample_id: str) -> np.ndarray:\n",
    "        candidates: Sequence[Path] = [\n",
    "            image_root / f\"{sample_id}{ext}\" for ext in (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "        ]\n",
    "        for candidate in candidates:\n",
    "            if candidate.exists():\n",
    "                return np.array(Image.open(candidate).convert(\"RGB\"))\n",
    "        raise FileNotFoundError(\n",
    "            f\"Image for '{sample_id}' not found in {image_root} with extensions \"\n",
    "            f\"{tuple(path.suffix for path in candidates)}\"\n",
    "        )\n",
    "\n",
    "    def _resize_image(image: np.ndarray, target_shape: tuple[int, int]) -> np.ndarray:\n",
    "        pil_img = Image.fromarray(image)\n",
    "        pil_img = pil_img.resize(target_shape[::-1], resample=Image.BILINEAR)\n",
    "        return np.array(pil_img)\n",
    "\n",
    "    for sample_id, expl_dict in explanations.items():\n",
    "        if not expl_dict:\n",
    "            continue\n",
    "\n",
    "        image = _load_image(sample_id)\n",
    "        methods = [m for m, h in expl_dict.items() if h is not None and np.size(h) > 0]\n",
    "        if not methods:\n",
    "            continue\n",
    "\n",
    "        cols = len(methods)\n",
    "        fig, axes = plt.subplots(\n",
    "            1, cols,\n",
    "            figsize=(figsize_per_panel[0] * cols, figsize_per_panel[1]),\n",
    "            squeeze=False,\n",
    "        )\n",
    "        fig.suptitle(sample_id)\n",
    "\n",
    "        for ax, method in zip(axes.flat, methods):\n",
    "            heatmap = expl_dict[method]\n",
    "            resized_img = _resize_image(image, heatmap.shape)\n",
    "            ax.imshow(resized_img)\n",
    "            ax.imshow(heatmap, cmap=cmap, alpha=alpha)\n",
    "            ax.set_title(method)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = [\"ID105_iCOPE_S10_pain\", \"ID6_iCOPE_S00_nopain\"]\n",
    "\n",
    "subset = {\n",
    "    sample_id: samples[sample_id]\n",
    "    for sample_id in target_ids\n",
    "}\n",
    "\n",
    "plot_xai_overlays(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5b10e",
   "metadata": {},
   "source": [
    "## Average Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_average_heatmaps_from_dict(\n",
    "    explanations: dict[str, dict[str, np.ndarray]],\n",
    "    labels: tuple[str, ...] = (\"nopain\", \"pain\"),\n",
    "    label_getter=None,\n",
    ") -> tuple[dict[str, dict[str, np.ndarray]], dict[str, dict[str, int]]]:\n",
    "    \"\"\"Compute per-label mean heatmaps for each explainer from an explanations dict.\"\"\"\n",
    "    labels = tuple(labels)\n",
    "    if label_getter is None:\n",
    "        suffix_map = {label: f\"_{label}\" for label in labels}\n",
    "        def label_getter(sample_id: str) -> str:\n",
    "            sample_lower = sample_id.lower()\n",
    "            for label, suffix in suffix_map.items():\n",
    "                if sample_lower.endswith(suffix):\n",
    "                    return label\n",
    "            raise ValueError(f\"Could not infer label for sample '{sample_id}'\")\n",
    "    avg_maps = {label: {} for label in labels}\n",
    "    file_counts = {label: {} for label in labels}\n",
    "\n",
    "    for sample_id, expl_dict in explanations.items():\n",
    "        label = label_getter(sample_id)\n",
    "        for explainer, heatmap in expl_dict.items():\n",
    "            if heatmap is None:\n",
    "                continue\n",
    "            data = np.asarray(heatmap, dtype=np.float32)\n",
    "\n",
    "            valid = np.isfinite(data)\n",
    "\n",
    "            total = avg_maps[label].setdefault(explainer, np.zeros_like(data, dtype=np.float64))\n",
    "            count = file_counts[label].setdefault(explainer, np.zeros_like(data, dtype=np.float64))\n",
    "            total[valid] += data[valid]\n",
    "            count[valid] += 1\n",
    "\n",
    "    for label in labels:\n",
    "        for explainer in list(avg_maps[label].keys()):\n",
    "            total = avg_maps[label][explainer]\n",
    "            count = file_counts[label][explainer]\n",
    "            valid = count > 0\n",
    "            if not np.any(valid):\n",
    "                del avg_maps[label][explainer]\n",
    "                del file_counts[label][explainer]\n",
    "                continue\n",
    "            avg = np.zeros_like(total, dtype=np.float32)\n",
    "            avg[valid] = (total[valid] / count[valid]).astype(np.float32)\n",
    "            avg_maps[label][explainer] = avg\n",
    "            file_counts[label][explainer] = int(np.max(count))  # number of contributing heatmaps\n",
    "\n",
    "    return avg_maps, file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda98d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\"nopain\", \"pain\")\n",
    "average_heatmaps, file_counts = build_average_heatmaps_from_dict(samples, labels=labels)\n",
    "\n",
    "explainer_names = sorted({name for maps in average_heatmaps.values() for name in maps})\n",
    "rows = len(explainer_names)\n",
    "cols = len(labels)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows), squeeze=False)\n",
    "\n",
    "for row, explainer in enumerate(explainer_names):\n",
    "    for col, label in enumerate(labels):\n",
    "        ax = axes[row, col]\n",
    "        heatmap = average_heatmaps[label].get(explainer)\n",
    "        if heatmap is None:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "        im = ax.imshow(heatmap, cmap=\"coolwarm\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"{explainer}: {label.capitalize()} (n={file_counts[label][explainer]})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037fef6",
   "metadata": {},
   "source": [
    "# 5. Correlations with Random and Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e630f53",
   "metadata": {},
   "source": [
    "SSIM and Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def spearman_and_ssim(img_a: np.ndarray, img_b: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Return Spearman rank correlation and SSIM between two images.\"\"\"\n",
    "    if img_a.shape != img_b.shape:\n",
    "        raise ValueError(\"Images must share shape.\")\n",
    "\n",
    "    flat_a = img_a.flatten()\n",
    "    flat_b = img_b.flatten()\n",
    "    rho, _ = spearmanr(flat_a, flat_b)\n",
    "\n",
    "    if img_a.ndim == 3 and img_a.shape[-1] == 3:\n",
    "        img_a_gray = rgb2gray(img_a)\n",
    "        img_b_gray = rgb2gray(img_b)\n",
    "    else:\n",
    "        img_a_gray = img_a\n",
    "        img_b_gray = img_b\n",
    "\n",
    "    ssim_val = ssim(img_a_gray, img_b_gray, data_range=img_b_gray.max() - img_b_gray.min())\n",
    "    return rho, ssim_val\n",
    "\n",
    "# Demo with synthetic data\n",
    "img1 = samples[\"ID105_iCOPE_S10_pain\"][\"IntegratedGradients\"]\n",
    "img2 = samples[\"ID105_iCOPE_S10_pain\"][\"EdgeDetector\"]\n",
    "\n",
    "rho, ssim_value = spearman_and_ssim(img1, img2)\n",
    "print(f\"Spearman: {rho:.3f} | SSIM: {ssim_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f90190",
   "metadata": {},
   "source": [
    "# 6. Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a0383",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from XAI.metrics import create_face_regions_masks, calculate_xai_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to classify pixels by importance\n",
    "def get_top_k_pixels(importance_map, k_percent=10):\n",
    "\n",
    "    # Flatten the map and sort the pixels by importance\n",
    "    flat_map = importance_map.flatten()\n",
    "    threshold_value = np.percentile(flat_map, 100 - k_percent)\n",
    "    top_k_mask = importance_map >= threshold_value\n",
    "\n",
    "    return top_k_mask.astype('int')\n",
    "\n",
    "\n",
    "def feature_agreement(mask_1, mask_2):\n",
    "    \n",
    "    mask_1 = get_top_k_pixels(mask_1)\n",
    "    mask_2 = get_top_k_pixels(mask_2)\n",
    "\n",
    "    # Compute the Intersection\n",
    "    intersection = np.logical_and(mask_1, mask_2)\n",
    "    union = np.logical_or(mask_1, mask_2)\n",
    "\n",
    "    # Calculate Feature Agreement\n",
    "    feature_agreement = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return feature_agreement\n",
    "\n",
    "\n",
    "def merge_symmetric_masks(face_masks):\n",
    "    # Define symmetric region mappings: (left_key, right_key) â†’ new_key\n",
    "    merge_map = {\n",
    "        ('left_eye', 'right_eye'): 'eyes',\n",
    "        ('left_cheek', 'right_cheek'): 'cheeks',\n",
    "        ('left_eyebrown', 'right_eyebrown'): 'eyebrowns',\n",
    "        ('left_nasolabial_fold', 'right_nasolabial_fold'): 'nasolabial_folds',\n",
    "    }\n",
    "\n",
    "    new_masks = {}\n",
    "    used_keys = set()\n",
    "\n",
    "    # Merge symmetric pairs\n",
    "    for (left, right), new_key in merge_map.items():\n",
    "        if left in face_masks and right in face_masks:\n",
    "            new_masks[new_key] = np.logical_or(face_masks[left], face_masks[right]).astype(np.uint8)\n",
    "            used_keys.update([left, right])\n",
    "\n",
    "    # Keep all other regions that are not merged\n",
    "    for key, mask in face_masks.items():\n",
    "        if key not in used_keys:\n",
    "            new_masks[key] = mask\n",
    "\n",
    "    return new_masks\n",
    "\n",
    "\n",
    "def calculate_region_agreement(importance_dict1, importance_dict2, k, return_agreed_set=False):\n",
    "    # Sort each dictionary by importance and get the top k regions\n",
    "    top_k_regions_1 = set(sorted(importance_dict1, key=importance_dict1.get, reverse=True)[:k])\n",
    "    top_k_regions_2 = set(sorted(importance_dict2, key=importance_dict2.get, reverse=True)[:k])\n",
    "\n",
    "    # Calculate the intersection of the top k regions\n",
    "    agreed_regions = top_k_regions_1.intersection(top_k_regions_2)\n",
    "\n",
    "    # Calculate the agreement score\n",
    "    agreement_score = len(agreed_regions) / k\n",
    "\n",
    "    if return_agreed_set:\n",
    "        return agreement_score, agreed_regions\n",
    "    else:\n",
    "        return agreement_score\n",
    "\n",
    "\n",
    "def pairwise_agreement(samples):\n",
    "\n",
    "    path_mesh = 'Datasets\\\\DatasetFaces\\\\Landmarks'\n",
    "\n",
    "    rows = []\n",
    "    for sample_id, masks in tqdm(samples.items()):\n",
    "        methods = sorted(masks)\n",
    "        for left, right in combinations(methods, 2):\n",
    "            mask_l = get_top_k_pixels(cv2.resize(masks[left], (512,512)), 10)\n",
    "            mask_r = get_top_k_pixels(cv2.resize(masks[right], (512,512)), 10)\n",
    "            \n",
    "            if mask_l.shape != mask_r.shape:\n",
    "                raise ValueError(f\"Shape mismatch on '{sample_id}' between {left} and {right}.\")\n",
    "            \n",
    "            agreement_score_pixel = feature_agreement(mask_l, mask_r)\n",
    "            \n",
    "            with open(os.path.join(path_mesh, sample_id+\".pkl\"), 'rb') as f:\n",
    "                    mesh = np.array(pickle.load(f))\n",
    "            \n",
    "            regions = create_face_regions_masks(mesh)\n",
    "            regions = merge_symmetric_masks(regions)\n",
    "\n",
    "            xai_score_l = calculate_xai_score(mask_l, regions, sort=True)\n",
    "            xai_score_r = calculate_xai_score(mask_r, regions, sort=True)\n",
    "\n",
    "            agreement_score_region, agreed_regions = calculate_region_agreement(\n",
    "                xai_score_l, xai_score_r, 3, return_agreed_set=True\n",
    "            )\n",
    "\n",
    "            rows.append(\n",
    "                dict(\n",
    "                    sample=sample_id,\n",
    "                    method_a=left,\n",
    "                    method_b=right,\n",
    "                    pixel=agreement_score_pixel,\n",
    "                    region=agreement_score_region,\n",
    "                    agreed_regions=sorted(agreed_regions)   # or ';'.join(sorted(agreed_regions))\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_top_regions(\n",
    "    samples,\n",
    "    top_k=3,\n",
    "    mask_size=(512, 512),\n",
    "    landmark_dir=r\"Datasets\\DatasetFaces\\Landmarks\",\n",
    "):\n",
    "    rows = []\n",
    "    for sample_id, method_masks in tqdm(samples.items(), desc=\"Samples\"):\n",
    "        mesh_path = Path(landmark_dir) / f\"{sample_id}.pkl\"\n",
    "        if not mesh_path.exists():\n",
    "            print(f\"Skip {sample_id} (missing landmarks at {mesh_path})\")\n",
    "            continue\n",
    "\n",
    "        with open(mesh_path, \"rb\") as f:\n",
    "            mesh = np.array(pickle.load(f))\n",
    "\n",
    "        regions = merge_symmetric_masks(create_face_regions_masks(mesh))\n",
    "\n",
    "        for method, mask in method_masks.items():\n",
    "            mask = np.nan_to_num(mask, copy=False)\n",
    "            mask_resized = cv2.resize(mask, mask_size)\n",
    "            mask_resized = get_top_k_pixels(mask_resized, 10)\n",
    "            region_scores = calculate_xai_score(mask_resized, regions, sort=True)\n",
    "            sorted_regions = sorted(region_scores, key=region_scores.get, reverse=True)[:top_k]\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"sample\": sample_id,\n",
    "                    \"explainer\": method,\n",
    "                    \"regions\": sorted_regions,  # store list directly\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e7c8",
   "metadata": {},
   "source": [
    "# 7. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36a1ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64247c2",
   "metadata": {},
   "source": [
    "# 8. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cdfd4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
