{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123fe012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Callable, Iterable, Mapping, Sequence\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from XAI.metrics import create_face_regions_masks, calculate_xai_score\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592c16e",
   "metadata": {},
   "source": [
    "# Comparative and Functional Evaluation of XAI Methods for Neonatal Pain Recognition Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8394094",
   "metadata": {},
   "source": [
    "# 1. Extract XAI Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b55e30",
   "metadata": {},
   "source": [
    "XAI Explainers:\n",
    "\n",
    "- IntegratedGradients\n",
    "- Saliency\n",
    "- DeepLift\n",
    "- Occlusion\n",
    "- LayerGradCam\n",
    "- GuidedGradCam\n",
    "- Deconvolution\n",
    "- GradientShap\n",
    "- DeepLiftShap\n",
    "- Lime\n",
    "\n",
    "Baselines to Explainers:\n",
    "\n",
    "- Canny Edge Detector\n",
    "- Same XAI explainer, but with random weights initialization (for pre-trained models keep pre-trained weights?)\n",
    "\n",
    "Models:\n",
    "\n",
    "- NCNN\n",
    "- VGGFace\n",
    "- ViT_B_32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf91a52",
   "metadata": {},
   "source": [
    "### XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"RGU\\\\XAI_EXPLAINERS\\\\RANDOM\"\n",
    "RANDOM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f25aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    Saliency,\n",
    "    DeepLift,\n",
    "    Occlusion,\n",
    "    LayerGradCam,\n",
    "    GuidedGradCam,\n",
    "    Deconvolution,\n",
    "    GradientShap,\n",
    "    DeepLiftShap,\n",
    "    Lime,\n",
    "    LayerAttribution,\n",
    ")\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "\n",
    "from utils.utils import create_folder, load_config\n",
    "from dataloaders.presets import PresetTransform\n",
    "from models import *\n",
    "\n",
    "def perturb_fn(inputs):\n",
    "    noise = torch.tensor(np.random.normal(0, 0.1, inputs.shape)).float().to(inputs.device)\n",
    "    return noise, torch.clip(inputs - noise, 0, 1)\n",
    "\n",
    "# --- Create superpixel feature mask for Captum ---\n",
    "def make_feature_mask(img_tensor, n_segments=100):\n",
    "    x = img_tensor.detach().cpu().squeeze(0)  # 3 x H x W\n",
    "    x_np = x.numpy()\n",
    "    x_np = (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-8)\n",
    "    x_np = np.transpose(x_np, (1, 2, 0))  # H, W, 3\n",
    "\n",
    "    seg = slic(x_np, n_segments=n_segments, compactness=10.0, sigma=0.0,\n",
    "               start_label=0, channel_axis=2)\n",
    "\n",
    "    seg_t = torch.from_numpy(seg).long().unsqueeze(0).unsqueeze(0)\n",
    "    return seg_t, seg\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def resolve_experiment(exp_name: str, device: torch.device):\n",
    "    if \"NCNN\" in exp_name:\n",
    "        model = NCNN().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 120,\n",
    "            \"transform\": PresetTransform(\"NCNN\").transforms,\n",
    "            \"layer\": model.merge_branch[0],\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    if \"VGGFace\" in exp_name:\n",
    "        model = VGGFace().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 224,\n",
    "            \"transform\": PresetTransform(\"VGGFace\").transforms,\n",
    "            \"layer\": model.VGGFace.features.conv5_3,\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    if \"ViT\" in exp_name:\n",
    "        model = ViT().to(device)\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"img_size\": 224,\n",
    "            \"transform\": PresetTransform(\"ViT\").transforms,\n",
    "            \"layer\": model.ViT.conv_proj,\n",
    "            \"model_name\": exp_name,\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "def ensure_feature_mask(ctx: dict, n_segments: int = 100):\n",
    "    if \"feature_mask\" not in ctx:\n",
    "        mask, _ = make_feature_mask(ctx[\"input\"], n_segments=n_segments)\n",
    "        ctx[\"feature_mask\"] = mask.to(ctx[\"device\"]).contiguous()\n",
    "    return ctx[\"feature_mask\"]\n",
    "\n",
    "\n",
    "# explainer catalogue ----------------------------------------------------------\n",
    "\n",
    "EXPLAINER_SPECS = [\n",
    "    (\n",
    "        \"IntegratedGradients\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: IntegratedGradients(model),\n",
    "            \"prepare\": lambda ctx: {\"attribute\": {\"internal_batch_size\": 10}},\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Saliency\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Saliency(model),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"DeepLift\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: DeepLift(model),\n",
    "            \"prepare\": lambda ctx: {\"attribute\": {\"baselines\": ctx[\"blurred\"]}},\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Occlusion\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Occlusion(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"sliding_window_shapes\": (3, 20, 20),\n",
    "                    \"strides\": (3, 5, 5),\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GradCAM\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: LayerGradCam(model, layer),\n",
    "            \"postprocess\": lambda attr, ctx: LayerAttribution.interpolate(\n",
    "                attr, ctx[\"target_shape\"], interpolate_mode=\"bilinear\"\n",
    "            ).repeat(1, 3, 1, 1),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GuidedGradCAM\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: GuidedGradCam(model, layer),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Deconvolution\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Deconvolution(model),\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"GradientShap\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: GradientShap(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"baselines\": torch.zeros_like(ctx[\"input\"]),\n",
    "                    \"n_samples\": 10,\n",
    "                    \"stdevs\": 0.0,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"DeepLiftShap\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: DeepLiftShap(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\"baselines\": ctx[\"blurred\"].repeat(10, 1, 1, 1)}\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Lime\",\n",
    "        {\n",
    "            \"factory\": lambda model, layer: Lime(model),\n",
    "            \"prepare\": lambda ctx: {\n",
    "                \"attribute\": {\n",
    "                    \"baselines\": torch.zeros_like(ctx[\"input\"]),\n",
    "                    \"feature_mask\": ensure_feature_mask(ctx),\n",
    "                    \"n_samples\": 500,\n",
    "                    \"perturbations_per_eval\": 64,\n",
    "                    \"show_progress\": False,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# main pipeline ----------------------------------------------------------------\n",
    "\n",
    "for model_name in [\"NCNN\", \"VGGFace\", \"ViT_B_32\"]:\n",
    "    print(f\"---------------Processing model: {model_name}---------------\")\n",
    "\n",
    "    path_experiments = os.path.join('experiments', model_name)\n",
    "\n",
    "    all_data = defaultdict(list)\n",
    "\n",
    "    for exp in os.listdir(path_experiments):\n",
    "        if any(ext in exp for ext in (\".pkl\", \"masks\", \".png\", \".pdf\")):\n",
    "            continue\n",
    "\n",
    "        experiment_cfg = resolve_experiment(exp, device)\n",
    "        if experiment_cfg is None:\n",
    "            continue\n",
    "\n",
    "        model = experiment_cfg[\"model\"]\n",
    "        img_size = experiment_cfg[\"img_size\"]\n",
    "        transform = experiment_cfg[\"transform\"]\n",
    "        layer = experiment_cfg[\"layer\"]\n",
    "\n",
    "        path_model = os.path.join(path_experiments, exp, \"Model\", \"best_model.pt\")\n",
    "        path_yaml = os.path.join(path_experiments, exp, \"Model\", \"config.yaml\")\n",
    "        config = load_config(path_yaml)\n",
    "        test_path = config[\"path_test\"].replace(\"\\\\\", \"/\")\n",
    "\n",
    "        state_dict = torch.load(path_model, map_location=device)\n",
    "        if not RANDOM:\n",
    "            model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "\n",
    "        explainers = {name: spec[\"factory\"](model, layer) for name, spec in EXPLAINER_SPECS}\n",
    "\n",
    "        image_files = [f for f in os.listdir(test_path) if f.lower().endswith(\".jpg\")]\n",
    "        for image_file in tqdm(image_files):\n",
    "            full_img_path = os.path.join(test_path, image_file)\n",
    "\n",
    "            img_rgb = Image.open(full_img_path).convert(\"RGB\")\n",
    "            img_rgb = img_rgb.resize((img_size, img_size))\n",
    "            img_name = os.path.splitext(image_file)[0]\n",
    "            label = 1 if img_name.split(\"_\")[3] == \"pain\" else 0\n",
    "\n",
    "            img_input = img_rgb\n",
    "\n",
    "            blurred_image = img_input.filter(ImageFilter.GaussianBlur(radius=5))\n",
    "\n",
    "            transformed = transform(img_input)\n",
    "            transformed_blurred = transform(blurred_image)\n",
    "\n",
    "            base_input = transformed.unsqueeze(0).to(device)\n",
    "            base_blurred = transformed_blurred.unsqueeze(0).to(device)\n",
    "\n",
    "            ctx_base = {\n",
    "                \"device\": device,\n",
    "                \"target_shape\": (img_size, img_size),\n",
    "                \"input_base\": base_input,\n",
    "                \"blurred\": base_blurred,\n",
    "            }\n",
    "\n",
    "            for XAI_name, spec in EXPLAINER_SPECS:\n",
    "                explainer = explainers[XAI_name]\n",
    "\n",
    "                method_ctx = dict(ctx_base)\n",
    "                method_ctx[\"input\"] = (\n",
    "                    ctx_base[\"input_base\"].clone().detach().requires_grad_(True)\n",
    "                )\n",
    "\n",
    "                if XAI_name == \"Lime\" or XAI_name == \"DeepLift\":\n",
    "                    method_ctx[\"input\"] = (\n",
    "                        ctx_base[\"input_base\"].clone().detach().requires_grad_(True).contiguous()\n",
    "                )\n",
    "\n",
    "                spec_kwargs = spec.get(\"prepare\", lambda ctx: {})(method_ctx)\n",
    "                attr_kwargs = spec_kwargs.get(\"attribute\", {})\n",
    "\n",
    "                attributions = explainer.attribute(method_ctx[\"input\"], **attr_kwargs)\n",
    "\n",
    "                if \"postprocess\" in spec:\n",
    "                    attributions = spec[\"postprocess\"](attributions, method_ctx)\n",
    "\n",
    "                attributions_np = (\n",
    "                    attributions.squeeze(0)\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .transpose(1, 2, 0)\n",
    "                )\n",
    "\n",
    "                output_dir = os.path.join(OUT_DIR, model_name, XAI_name)\n",
    "                create_folder(output_dir)\n",
    "                output_path = os.path.join(output_dir, f\"{img_name}.npz\")\n",
    "                np.savez_compressed(output_path, mask_raw=attributions_np)\n",
    "\n",
    "                all_data[\"img_path\"].append(full_img_path)\n",
    "                all_data[\"fold\"].append(os.path.basename(os.path.dirname(test_path)))\n",
    "                all_data[\"label\"].append(label)\n",
    "                all_data[\"mask_path\"].append(output_path)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    probs = model.predict(ctx_base[\"input_base\"])\n",
    "                pred = (probs >= 0.5).int()\n",
    "                all_data[\"probability\"].append(float(probs))\n",
    "                all_data[\"prediction\"].append(int(pred))\n",
    "                all_data[\"XAI_name\"].append(XAI_name)\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    dataframe = pd.DataFrame(all_data)\n",
    "    create_folder(os.path.join(OUT_DIR, model_name))\n",
    "    dataframe.to_csv(os.path.join(OUT_DIR, model_name, \"explainers.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0380699",
   "metadata": {},
   "source": [
    "### Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa3022",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root = Path(r\"Datasets\\Folds\")\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "processed = 0\n",
    "skipped = []\n",
    "\n",
    "for fold_dir in sorted(input_root.iterdir()):\n",
    "    if not fold_dir.is_dir():\n",
    "        continue\n",
    "    test_dir = fold_dir / \"Test\"\n",
    "    if not test_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for img_path in test_dir.rglob(\"*\"):\n",
    "        if not img_path.is_file() or img_path.suffix.lower() not in image_exts:\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            skipped.append(str(img_path))\n",
    "            continue\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(gray, 50, 100)\n",
    "        edges = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "        edges_NCNN = cv2.resize(edges, (120,120), interpolation=cv2.INTER_NEAREST)\n",
    "        edges = cv2.resize(edges, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        out_name = f\"{img_path.stem}.npz\"\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\NCNN\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges_NCNN)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\VGGFace\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\ViT_B_32\\\\EdgeDetector\")\n",
    "        out_path = output_root / out_name\n",
    "        np.savez_compressed(str(out_path), mask_raw=edges)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "print(f\"Processed {processed} images.\")\n",
    "if skipped:\n",
    "    print(\"Skipped files (failed to load):\")\n",
    "    for item in skipped:\n",
    "        print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26bed5",
   "metadata": {},
   "source": [
    "### Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root = Path(r\"Datasets\\Folds\")\n",
    "image_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "processed = 0\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for fold_dir in sorted(input_root.iterdir()):\n",
    "    if not fold_dir.is_dir():\n",
    "        continue\n",
    "    test_dir = fold_dir / \"Test\"\n",
    "    if not test_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for img_path in test_dir.rglob(\"*\"):\n",
    "        if not img_path.is_file() or img_path.suffix.lower() not in image_exts:\n",
    "            continue\n",
    "\n",
    "        mask_ncnn = rng.normal(loc=0.5, scale=0.15, size=(120, 120))\n",
    "        mask_ncnn = np.clip(mask_ncnn, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "        mask_vgg_vit = rng.normal(loc=0.5, scale=0.15, size=(224, 224))\n",
    "        mask_vgg_vit = np.clip(mask_vgg_vit, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "        out_name = f\"{img_path.stem}.npz\"\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\NOISE\\\\NCNN\\\\Noise\")\n",
    "        np.savez_compressed(str(output_root / out_name), mask_raw=mask_ncnn)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\NOISE\\\\VGGFace\\\\Noise\")\n",
    "        np.savez_compressed(str(output_root / out_name), mask_raw=mask_vgg_vit)\n",
    "\n",
    "        output_root = Path(\"RGU\\\\XAI_EXPLAINERS\\\\NOISE\\\\ViT_B_32\\\\Noise\")\n",
    "        np.savez_compressed(str(output_root / out_name), mask_raw=mask_vgg_vit)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "print(f\"Processed {processed} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27928b67",
   "metadata": {},
   "source": [
    "# 2. Align\n",
    "\n",
    "All masks (heatmaps) are aligned to the same common reference based on 5 face landmarks (eye right, eye left, mouth right corner, mouth left corner, and nose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDMARK_DIR = Path(r\"Datasets\\DatasetFaces\\Landmarks\")\n",
    "ALIGN_INDICES = np.array([86, 52, 61, 88, 38])  # order matters\n",
    "ALIGN_SUBDIR = \"aligned\"\n",
    "\n",
    "# Load all landmark meshes and compute the mean reference subset.\n",
    "faces = []\n",
    "for fname in os.listdir(LANDMARK_DIR):\n",
    "    with open(LANDMARK_DIR / fname, \"rb\") as f:\n",
    "        faces.append(np.array(pickle.load(f)))\n",
    "\n",
    "faces_np = np.array(faces)\n",
    "face_mesh_mean = faces_np.mean(axis=0)\n",
    "AVG_SUBSET = face_mesh_mean[ALIGN_INDICES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce118071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heatmap(path: Path) -> tuple[np.ndarray, tuple[int, int], np.dtype]:\n",
    "    mask = np.load(path)[\"mask_raw\"]\n",
    "    return cv2.resize(mask, (512, 512)), mask.shape, mask.dtype\n",
    "\n",
    "def load_landmarks(path: Path) -> np.ndarray:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return np.array(pickle.load(f))\n",
    "\n",
    "def align_heatmaps(\n",
    "    heatmap_dir: Path = Path(\"RGU\\\\TRAINED\\\\NCNN\"),\n",
    "    landmark_dir: Path = LANDMARK_DIR,\n",
    "    output_shape: tuple[int, int] | None = None,\n",
    ") -> None:\n",
    "\n",
    "    for explainer_path in tqdm(sorted(heatmap_dir.iterdir()), desc=\"Explainers\"):\n",
    "        if not explainer_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        aligned_dir = explainer_path / ALIGN_SUBDIR\n",
    "        aligned_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for heatmap_path in sorted(explainer_path.iterdir()):\n",
    "            if heatmap_path.suffix.lower() != \".npz\":\n",
    "                continue\n",
    "\n",
    "            lm_path = landmark_dir / f\"{heatmap_path.stem}.pkl\"\n",
    "            if not lm_path.exists():\n",
    "                print(f\"Skip {heatmap_path.name} (missing landmarks at {lm_path})\")\n",
    "                continue\n",
    "\n",
    "            heatmap, original_shape, original_dtype = load_heatmap(heatmap_path)\n",
    "            sample_landmarks = load_landmarks(lm_path)\n",
    "            sample_subset = sample_landmarks[ALIGN_INDICES]\n",
    "\n",
    "            transform = AffineTransform()\n",
    "            if not transform.estimate(sample_subset, AVG_SUBSET):\n",
    "                print(f\"Failed to estimate affine transform for {heatmap_path.name}\")\n",
    "                continue\n",
    "\n",
    "            target_shape = (\n",
    "                output_shape if output_shape is not None else heatmap.shape[:2]\n",
    "            )\n",
    "            warped = warp(\n",
    "                heatmap,\n",
    "                inverse_map=transform.inverse,\n",
    "                output_shape=target_shape,\n",
    "                preserve_range=True,\n",
    "            ).astype(original_dtype, copy=False)\n",
    "\n",
    "            restored = cv2.resize(\n",
    "                warped,\n",
    "                (original_shape[1], original_shape[0]),\n",
    "            ).astype(original_dtype, copy=False)\n",
    "\n",
    "            aligned_path = aligned_dir / f\"{heatmap_path.stem}.npz\"\n",
    "            np.savez_compressed(aligned_path, mask_raw=restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tipos in [\"TRAINED\", \"RANDOM\", \"EDGEDETECTOR\", \"NOISE\"]:\n",
    "    for model in [\"NCNN\", \"VGGFace\", \"ViT_B_32\"]:\n",
    "        heatmap_dir = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\{tipos}\\\\{model}\")\n",
    "        print(f\"Aligning heatmaps in {heatmap_dir}...\")\n",
    "        align_heatmaps(heatmap_dir=heatmap_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba2b44",
   "metadata": {},
   "source": [
    "# 3. Load and Standardize\n",
    "\n",
    "for the next steps, all analysis are done with a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6294f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_AUX = \"VGGFace\"  # \"NCNN\", \"VGGFace\", \"ViT_B_32\"\n",
    "METHOD_VIZU = 'absolute'  # 'absolute', 'positive', 'negative'\n",
    "ALIGN = False  # True or False}\n",
    "\n",
    "def vizu_normalize(x, method=METHOD_VIZU):\n",
    "    x = np.asarray(x)\n",
    "    # Shape must be in (H, W, C)\n",
    "    if x.ndim != 2:\n",
    "        x_combined = np.sum(x, axis=2)\n",
    "    else:\n",
    "        x_combined = x\n",
    "\n",
    "    if method == 'positive':\n",
    "        x_combined = (x_combined > 0) * x_combined # positive values only\n",
    "    elif method == 'negative':\n",
    "        x_combined = (x_combined < 0) * x_combined # negative values only\n",
    "    elif method == 'absolute':\n",
    "        x_combined = np.abs(x_combined) # absolute values\n",
    "    elif method == 'all':\n",
    "        pass # use all values as they are\n",
    "\n",
    "    sorted_vals = np.sort(np.abs(x_combined).flatten())\n",
    "    cum_sums = np.cumsum(sorted_vals)\n",
    "    threshold_id: int = np.where(cum_sums >= cum_sums[-1] * 0.01 * 98)[0][0]\n",
    "    threshold = sorted_vals[threshold_id]\n",
    "\n",
    "    attr_norm = x_combined / threshold\n",
    "\n",
    "    return np.clip(attr_norm, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40398884",
   "metadata": {},
   "source": [
    "### Generate First time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=True):\n",
    "    \"\"\"Return {sample_id: {method_name: bool_mask}} for every .npz in the folders.\"\"\"\n",
    "    samples = {}\n",
    "    for xai_dir in map(Path, xai_dirs):\n",
    "        method = xai_dir.name\n",
    "        for npz_file in tqdm(sorted(xai_dir.glob(\"aligned/*.npz\") if aligned else xai_dir.glob(\"*.npz\"))):\n",
    "            with np.load(npz_file) as archive:\n",
    "                if mask_key not in archive:\n",
    "                    raise KeyError(f\"{npz_file} missing '{mask_key}' array.\")\n",
    "                mask = archive[mask_key]\n",
    "                #mask = vizu_normalize(mask, method=method)\n",
    "            samples.setdefault(npz_file.stem, {})[method] = mask\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\TRAINED\\\\{MODEL_NAME_AUX}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_trained = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=ALIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\RANDOM\\\\{MODEL_NAME_AUX}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_random = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=ALIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\EDGEDETECTOR\\\\{MODEL_NAME_AUX}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_edgedetector = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=ALIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d17b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEATMAP_DIR = Path(f\"RGU\\\\XAI_EXPLAINERS\\\\NOISE\\\\{MODEL_NAME_AUX}\")\n",
    "xai_dirs = glob.glob(f\"{HEATMAP_DIR}\\*\")\n",
    "samples_noise = load_masks(xai_dirs, mask_key=\"mask_raw\", aligned=ALIGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062d46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_explanations(*dicts):\n",
    "    merged = {}\n",
    "    for data in dicts:\n",
    "        for sample_id, expl_dict in data.items():\n",
    "            inner = merged.setdefault(sample_id, {})\n",
    "            for explainer, value in expl_dict.items():\n",
    "                name = explainer\n",
    "                while name in inner:\n",
    "                    name = f\"RANDOM_{explainer}\"\n",
    "                inner[name] = value\n",
    "    return merged\n",
    "\n",
    "\n",
    "samples = merge_explanations(samples_trained, samples_random, samples_edgedetector, samples_noise)\n",
    "\n",
    "with open(Path(f\"RGU\\\\XAI_EXPLAINERS\\\\samples_{MODEL_NAME_AUX}_align_{ALIGN}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f881cc7",
   "metadata": {},
   "source": [
    "### Load if already saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ce071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(f\"RGU\\\\XAI_EXPLAINERS\\\\samples_{MODEL_NAME_AUX}_align_{ALIGN}.pkl\"), \"rb\") as f:\n",
    "    samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b0c0a",
   "metadata": {},
   "source": [
    "# 4. Show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c980515",
   "metadata": {},
   "source": [
    "## Sample Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xai_overlays(\n",
    "    explanations: Mapping[str, Mapping[str, np.ndarray]],\n",
    "    image_root: Path | str = Path(r\"D:\\### DOUTORADO\\Mestrado\\Datasets\\DatasetFaces\\Images\"),\n",
    "    figsize_per_panel: tuple[float, float] = (4.0, 4.0),\n",
    "    cmap: str = \"inferno\",\n",
    "    alpha: float = 0.55,\n",
    "    save_root: Path | str = Path(r\"D:\\### DOUTORADO\\Mestrado\\RGU\\XAI_EXPLAINERS\\imgs\"),\n",
    "    save_dpi: int = 300\n",
    ") -> None:\n",
    "    image_root = Path(image_root)\n",
    "\n",
    "    if save_root is not None:\n",
    "        save_root = Path(save_root)\n",
    "        save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    def _load_image(sample_id: str) -> np.ndarray:\n",
    "        candidates: Sequence[Path] = [\n",
    "            image_root / f\"{sample_id}{ext}\" for ext in (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "        ]\n",
    "        for candidate in candidates:\n",
    "            if candidate.exists():\n",
    "                return np.array(Image.open(candidate).convert(\"RGB\"))\n",
    "        raise FileNotFoundError(\n",
    "            f\"Image for '{sample_id}' not found in {image_root} with extensions \"\n",
    "            f\"{tuple(path.suffix for path in candidates)}\"\n",
    "        )\n",
    "\n",
    "    def _resize_image(image: np.ndarray, target_shape: tuple[int, int]) -> np.ndarray:\n",
    "        pil_img = Image.fromarray(image)\n",
    "        pil_img = pil_img.resize(target_shape[::-1], resample=Image.BILINEAR)\n",
    "        pil_img = pil_img.convert(\"L\")\n",
    "        return np.array(pil_img)\n",
    "\n",
    "    for sample_id, expl_dict in explanations.items():\n",
    "        if not expl_dict:\n",
    "            continue\n",
    "\n",
    "        image = _load_image(sample_id)\n",
    "        methods = [m for m, h in expl_dict.items() if h is not None and np.size(h) > 0]\n",
    "        if not methods:\n",
    "            continue\n",
    "\n",
    "        cols = len(methods)\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            cols,\n",
    "            figsize=(figsize_per_panel[0] * cols, figsize_per_panel[1]),\n",
    "            squeeze=False,\n",
    "        )\n",
    "        fig.suptitle(sample_id)\n",
    "\n",
    "        for ax, method in zip(axes.flat, methods):\n",
    "            heatmap = expl_dict[method]\n",
    "            heatmap = vizu_normalize(heatmap, method=METHOD_VIZU)\n",
    "            resized_img = _resize_image(image, heatmap.shape)\n",
    "            ax.imshow(resized_img, cmap=\"gray\")\n",
    "            ax.imshow(heatmap, cmap=cmap, alpha=alpha, vmin=0, vmax=1)\n",
    "            ax.set_title(method)\n",
    "            ax.axis(\"off\")\n",
    "            file_name = f\"{sample_id}_all_{MODEL_NAME_AUX}\".replace(\" \", \"_\") + \".png\"\n",
    "            fig.savefig(save_root / file_name, dpi=save_dpi, bbox_inches=\"tight\", pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            if save_root is not None:\n",
    "                file_name = f\"{sample_id}_{method}_{MODEL_NAME_AUX}\".replace(\" \", \"_\") + \".png\"\n",
    "                fig_save, ax_save = plt.subplots(figsize=figsize_per_panel)\n",
    "                ax_save.imshow(resized_img, cmap=\"gray\")\n",
    "                \n",
    "                ax_save.imshow(heatmap, cmap=cmap, alpha=alpha, vmin=0, vmax=1)\n",
    "                ax_save.axis(\"off\")\n",
    "\n",
    "                fig_save.savefig(save_root / file_name, dpi=save_dpi, bbox_inches=\"tight\", pad_inches=0)\n",
    "                plt.close(fig_save)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = [\"ID105_iCOPE_S10_pain\", \"ID6_iCOPE_S00_nopain\"]\n",
    "\n",
    "subset = {\n",
    "    sample_id: samples[sample_id]\n",
    "    for sample_id in target_ids\n",
    "}\n",
    "\n",
    "plot_xai_overlays(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5b10e",
   "metadata": {},
   "source": [
    "## Average Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c041d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_average_heatmaps_from_dict(\n",
    "    explanations: dict[str, dict[str, np.ndarray]],\n",
    "    labels: tuple[str, ...] = (\"nopain\", \"pain\"),\n",
    "    label_getter=None,\n",
    ") -> tuple[dict[str, dict[str, np.ndarray]], dict[str, dict[str, int]]]:\n",
    "    \"\"\"Compute per-label mean heatmaps for each explainer from an explanations dict.\"\"\"\n",
    "    labels = tuple(labels)\n",
    "    if label_getter is None:\n",
    "        suffix_map = {label: f\"_{label}\" for label in labels}\n",
    "        def label_getter(sample_id: str) -> str:\n",
    "            sample_lower = sample_id.lower()\n",
    "            for label, suffix in suffix_map.items():\n",
    "                if sample_lower.endswith(suffix):\n",
    "                    return label\n",
    "            raise ValueError(f\"Could not infer label for sample '{sample_id}'\")\n",
    "    avg_maps = {label: {} for label in labels}\n",
    "    file_counts = {label: {} for label in labels}\n",
    "\n",
    "    for sample_id, expl_dict in explanations.items():\n",
    "        label = label_getter(sample_id)\n",
    "        for explainer, heatmap in expl_dict.items():\n",
    "            if heatmap is None:\n",
    "                continue\n",
    "            data = np.asarray(heatmap, dtype=np.float32)\n",
    "\n",
    "            valid = np.isfinite(data)\n",
    "\n",
    "            total = avg_maps[label].setdefault(explainer, np.zeros_like(data, dtype=np.float64))\n",
    "            count = file_counts[label].setdefault(explainer, np.zeros_like(data, dtype=np.float64))\n",
    "            total[valid] += data[valid]\n",
    "            count[valid] += 1\n",
    "\n",
    "    for label in labels:\n",
    "        for explainer in list(avg_maps[label].keys()):\n",
    "            total = avg_maps[label][explainer]\n",
    "            count = file_counts[label][explainer]\n",
    "            valid = count > 0\n",
    "            if not np.any(valid):\n",
    "                del avg_maps[label][explainer]\n",
    "                del file_counts[label][explainer]\n",
    "                continue\n",
    "            avg = np.zeros_like(total, dtype=np.float32)\n",
    "            avg[valid] = (total[valid] / count[valid]).astype(np.float32)\n",
    "            avg_maps[label][explainer] = avg\n",
    "            file_counts[label][explainer] = int(np.max(count))  # number of contributing heatmaps\n",
    "\n",
    "    return avg_maps, file_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda98d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\"nopain\", \"pain\")\n",
    "average_heatmaps, file_counts = build_average_heatmaps_from_dict(samples, labels=labels)\n",
    "\n",
    "explainer_names = sorted({name for maps in average_heatmaps.values() for name in maps})\n",
    "rows = len(labels)\n",
    "cols = len(explainer_names)\n",
    "\n",
    "save_root: Path | str | None = Path(r\"D:\\### DOUTORADO\\Mestrado\\RGU\\XAI_EXPLAINERS\\imgs\")  # or None to skip saving\n",
    "save_dpi = 300\n",
    "if save_root is not None:\n",
    "    save_root = Path(save_root)\n",
    "    save_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows), squeeze=False)\n",
    "\n",
    "for row, label in enumerate(labels):\n",
    "    for col, explainer in enumerate(explainer_names):\n",
    "        ax = axes[row, col]\n",
    "        heatmap = average_heatmaps[label].get(explainer)\n",
    "        heatmap = vizu_normalize(heatmap, method=METHOD_VIZU)\n",
    "\n",
    "        if label == \"nopain\":\n",
    "            ax.imshow(cv2.resize(cv2.imread(save_root / \"avg_aligned_nopain.png\", 0), (heatmap.shape)), cmap=\"gray\")\n",
    "        else:\n",
    "            ax.imshow(cv2.resize(cv2.imread(save_root / \"avg_aligned_pain.png\", 0), (heatmap.shape)), cmap=\"gray\")\n",
    "\n",
    "        im = ax.imshow(heatmap, cmap=\"inferno\", alpha=0.55, vmin=0, vmax=1)\n",
    "        ax.axis(\"off\")\n",
    "        title_label = \"Pain\" if label == \"pain\" else \"No Pain\"\n",
    "        ax.set_title(f\"{explainer}: {title_label}\")\n",
    "\n",
    "\n",
    "        if save_root is not None:\n",
    "            file_name = f\"{label}_{explainer}_{MODEL_NAME_AUX}.png\".replace(\" \", \"_\")\n",
    "            fig_save, ax_save = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "            if label == \"nopain\":\n",
    "                ax_save.imshow(cv2.resize(cv2.imread(save_root / \"avg_aligned_nopain.png\", 0), (heatmap.shape)), cmap=\"gray\")\n",
    "            else:\n",
    "                ax_save.imshow(cv2.resize(cv2.imread(save_root / \"avg_aligned_pain.png\", 0), (heatmap.shape)), cmap=\"gray\")\n",
    "\n",
    "            ax_save.imshow(heatmap, cmap=\"inferno\", alpha=0.55, vmin=0, vmax=1)\n",
    "            ax_save.axis(\"off\")\n",
    "            fig_save.savefig(save_root / file_name, dpi=save_dpi, bbox_inches=\"tight\", pad_inches=0)\n",
    "            plt.close(fig_save)\n",
    "\n",
    "file_name = f\"all_{MODEL_NAME_AUX}.png\".replace(\" \", \"_\")\n",
    "fig.savefig(save_root / file_name, dpi=save_dpi, bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.close(fig)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037fef6",
   "metadata": {},
   "source": [
    "# 5. Correlations with Random, Canny, and Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e630f53",
   "metadata": {},
   "source": [
    "SSIM, Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing correlation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/521 [00:00<?, ?it/s]C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_23948\\2751999651.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  attr_norm = x_combined / threshold\n",
      "  5%|▍         | 26/521 [01:40<31:43,  3.85s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def spearman_and_ssim(img_a: np.ndarray, img_b: np.ndarray) -> tuple[float, float]:\n",
    "    \"\"\"Return Spearman rank correlation and SSIM between two images.\"\"\"\n",
    "    if img_a.shape != img_b.shape:\n",
    "        raise ValueError(\"Images must share shape.\")\n",
    "\n",
    "    flat_a = img_a.flatten()\n",
    "    flat_b = img_b.flatten()\n",
    "    rho, _ = spearmanr(flat_a, flat_b)\n",
    "\n",
    "    if np.isnan(rho):\n",
    "        rho = 0.0\n",
    "\n",
    "    if img_a.ndim == 3 and img_a.shape[-1] == 3:\n",
    "        img_a_gray = rgb2gray(img_a)\n",
    "        img_b_gray = rgb2gray(img_b)\n",
    "    else:\n",
    "        img_a_gray = img_a\n",
    "        img_b_gray = img_b\n",
    "\n",
    "    data_range = img_b_gray.max() - img_b_gray.min()\n",
    "    if data_range == 0:\n",
    "        data_range = 1.0\n",
    "\n",
    "    ssim_val = ssim(img_a_gray, img_b_gray, data_range=data_range)\n",
    "\n",
    "    if np.isnan(ssim_val):\n",
    "        ssim_val = 0.0\n",
    "        \n",
    "    return rho, ssim_val\n",
    "\n",
    "\n",
    "if Path(f\"RGU\\\\XAI_EXPLAINERS\\\\explainers_correlation_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}.csv\").exists():\n",
    "    print(\"Correlation metrics file already exists. Skipping computation.\")\n",
    "    metrics_df = pd.read_csv(Path(f\"RGU\\\\XAI_EXPLAINERS\\\\explainers_correlation_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}.csv\"))\n",
    "else:\n",
    "    print(\"Computing correlation metrics...\")\n",
    "    records = []\n",
    "    for sample_id, explainers in tqdm(samples.items()):\n",
    "        names = list(explainers.keys())\n",
    "        for i, name_a in enumerate(names):\n",
    "            for name_b in names[i + 1:]:\n",
    "                img_a = np.asarray(explainers[name_a])\n",
    "                img_b = np.asarray(explainers[name_b])\n",
    "                img_a = vizu_normalize(img_a, method=METHOD_VIZU)\n",
    "                img_b = vizu_normalize(img_b, method=METHOD_VIZU)\n",
    "                rho, ssim_val = spearman_and_ssim(img_a, img_b)\n",
    "                records.append({\n",
    "                    \"sample_id\": sample_id,\n",
    "                    \"explainer_a\": name_a,\n",
    "                    \"explainer_b\": name_b,\n",
    "                    \"spearman\": rho,\n",
    "                    \"ssim\": ssim_val,\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(records)\n",
    "    metrics_df.to_csv(Path(f\"RGU\\\\XAI_EXPLAINERS\\\\explainers_correlation_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e041e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pairwise_matrix(df, metric, explainer_order=None):\n",
    "    explainers = explainer_order or sorted(set(df[\"explainer_a\"]).union(df[\"explainer_b\"]))\n",
    "    matrix = pd.DataFrame(np.nan, index=explainers, columns=explainers, dtype=float)\n",
    "\n",
    "    pair_means = (\n",
    "        df.assign(pair=df.apply(lambda r: tuple(sorted((r[\"explainer_a\"], r[\"explainer_b\"]))), axis=1))\n",
    "          .groupby(\"pair\")[metric]\n",
    "          .mean()\n",
    "    )\n",
    "    for (a, b), value in pair_means.items():\n",
    "        matrix.at[a, b] = value\n",
    "        matrix.at[b, a] = value\n",
    "\n",
    "    np.fill_diagonal(matrix.values, 1.0)\n",
    "    return matrix.loc[explainers, explainers]  # keeps the requested order\n",
    "\n",
    "\n",
    "from matplotlib.patches import Rectangle  # add once at the top\n",
    "\n",
    "def plot_average_heatmap(df, metric, ax=None, cmap=\"viridis\", explainer_order=None, highlight_groups=None):\n",
    "    matrix = average_pairwise_matrix(df, metric, explainer_order=explainer_order)\n",
    "    vmin, vmax = -1, 1\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        ax=ax,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=cmap,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "        cbar=False,\n",
    "        linewidths=0.5,\n",
    "        square=True,\n",
    "    )\n",
    "\n",
    "    if highlight_groups:\n",
    "        for labels, color in highlight_groups:\n",
    "            idx = [matrix.index.get_loc(lbl) for lbl in labels if lbl in matrix.index]\n",
    "            if not idx:\n",
    "                continue\n",
    "            start = min(idx)\n",
    "            height = max(idx) - min(idx) + 1\n",
    "            rect = Rectangle(\n",
    "                (0, start),\n",
    "                width=matrix.shape[1],\n",
    "                height=height,\n",
    "                linewidth=5,\n",
    "                edgecolor=color,\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    ax.set_title(f\"{metric.upper()} — average across samples\")\n",
    "    ax.set_xlabel(\"Explainer\")\n",
    "    ax.set_ylabel(\"Explainer\")\n",
    "    return ax\n",
    "\n",
    "highlight_groups=[\n",
    "    ([\"Saliency\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"DeepLift\",\n",
    "    \"DeepLiftShap\",\n",
    "    \"GradientShap\",\n",
    "    \"GradCAM\",\n",
    "    \"GuidedGradCAM\",\n",
    "    \"Deconvolution\",\n",
    "    \"Occlusion\",\n",
    "    \"Lime\"], \"green\"),\n",
    "\n",
    "    ([\"EdgeDetector\", \"Noise\"], \"blue\"),\n",
    "    \n",
    "    ([\"RANDOM_Saliency\",\n",
    "    \"RANDOM_IntegratedGradients\",\n",
    "    \"RANDOM_DeepLift\",\n",
    "    \"RANDOM_DeepLiftShap\",\n",
    "    \"RANDOM_GradientShap\",\n",
    "    \"RANDOM_GradCAM\",\n",
    "    \"RANDOM_GuidedGradCAM\",\n",
    "    \"RANDOM_Deconvolution\",\n",
    "    \"RANDOM_Occlusion\",\n",
    "    \"RANDOM_Lime\",], \"purple\"),\n",
    "    ]\n",
    "\n",
    "def plot_and_save_average_heatmap(df, metric, cmap, outfile, explainer_order):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15), constrained_layout=True)\n",
    "    plot_average_heatmap(df, metric, ax=ax, cmap=cmap, explainer_order=explainer_order)\n",
    "    plt.show()  # display in the notebook\n",
    "    fig.savefig(outfile, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "order = [\n",
    "    \"Saliency\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"DeepLift\",\n",
    "    \"DeepLiftShap\",\n",
    "    \"GradientShap\",\n",
    "    \"GradCAM\",\n",
    "    \"GuidedGradCAM\",\n",
    "    \"Deconvolution\",\n",
    "    \"Occlusion\",\n",
    "    \"Lime\",\n",
    "    \"EdgeDetector\",\n",
    "    \"Noise\",\n",
    "    \"RANDOM_Saliency\",\n",
    "    \"RANDOM_IntegratedGradients\",\n",
    "    \"RANDOM_DeepLift\",\n",
    "    \"RANDOM_DeepLiftShap\",\n",
    "    \"RANDOM_GradientShap\",\n",
    "    \"RANDOM_GradCAM\",\n",
    "    \"RANDOM_GuidedGradCAM\",\n",
    "    \"RANDOM_Deconvolution\",\n",
    "    \"RANDOM_Occlusion\",\n",
    "    \"RANDOM_Lime\",\n",
    "]\n",
    "\n",
    "plot_and_save_average_heatmap(metrics_df, \n",
    "                              \"spearman\", \n",
    "                              cmap=\"RdBu_r\", \n",
    "                              outfile=f\"RGU\\\\XAI_EXPLAINERS\\\\imgs\\\\avg_spearman_heatmap_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}.pdf\", \n",
    "                              explainer_order=order)\n",
    "\n",
    "plot_and_save_average_heatmap(metrics_df, \n",
    "                              \"ssim\", \n",
    "                              cmap=\"RdBu_r\", \n",
    "                              outfile=f\"RGU\\\\XAI_EXPLAINERS\\\\imgs\\\\avg_ssim_heatmap_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}.pdf\", \n",
    "                              explainer_order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f90190",
   "metadata": {},
   "source": [
    "# 6. Agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a0383",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_pixels(importance_map, k_percent=10):\n",
    "    # Flatten the map and sort the pixels by importance\n",
    "    flat_map = importance_map.flatten()\n",
    "    threshold_value = np.percentile(flat_map, 100 - k_percent)\n",
    "    top_k_mask = importance_map >= threshold_value\n",
    "\n",
    "    return top_k_mask.astype('int')\n",
    "\n",
    "\n",
    "def feature_agreement(mask_1, mask_2):\n",
    "    # Compute the Intersection\n",
    "    intersection = np.logical_and(mask_1, mask_2)\n",
    "    union = np.logical_or(mask_1, mask_2)\n",
    "\n",
    "    # Calculate Feature Agreement\n",
    "    feature_agreement = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return feature_agreement\n",
    "\n",
    "\n",
    "def merge_symmetric_masks(face_masks):\n",
    "    # Define symmetric region mappings: (left_key, right_key) → new_key\n",
    "    merge_map = {\n",
    "        ('left_eye', 'right_eye'): 'eyes',\n",
    "        ('left_cheek', 'right_cheek'): 'cheeks',\n",
    "        ('left_eyebrown', 'right_eyebrown'): 'eyebrowns',\n",
    "        ('left_nasolabial_fold', 'right_nasolabial_fold'): 'nasolabial_folds',\n",
    "    }\n",
    "\n",
    "    new_masks = {}\n",
    "    used_keys = set()\n",
    "\n",
    "    # Merge symmetric pairs\n",
    "    for (left, right), new_key in merge_map.items():\n",
    "        if left in face_masks and right in face_masks:\n",
    "            new_masks[new_key] = np.logical_or(face_masks[left], face_masks[right]).astype(np.uint8)\n",
    "            used_keys.update([left, right])\n",
    "\n",
    "    # Keep all other regions that are not merged\n",
    "    for key, mask in face_masks.items():\n",
    "        if key not in used_keys:\n",
    "            new_masks[key] = mask\n",
    "\n",
    "    return new_masks\n",
    "\n",
    "\n",
    "def calculate_region_agreement(importance_dict1, importance_dict2, k, return_agreed_set=False):\n",
    "    selected1 = [\n",
    "        region\n",
    "        for region, score in sorted(importance_dict1.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        if score > 0\n",
    "    ][:k]\n",
    "    selected2 = [\n",
    "        region\n",
    "        for region, score in sorted(importance_dict2.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        if score > 0\n",
    "    ][:k]\n",
    "\n",
    "    top_k_regions_1 = set(selected1)\n",
    "    top_k_regions_2 = set(selected2)\n",
    "\n",
    "    agreed_regions = top_k_regions_1.intersection(top_k_regions_2)\n",
    "    agreement_score = len(agreed_regions) / k\n",
    "\n",
    "    if return_agreed_set:\n",
    "        return agreement_score, agreed_regions\n",
    "    return agreement_score\n",
    "\n",
    "\n",
    "def pairwise_agreement(samples, k_pixel=10, k_region=3):\n",
    "    path_mesh = 'Datasets\\\\DatasetFaces\\\\Landmarks'\n",
    "\n",
    "    rows = []\n",
    "    for sample_id, masks in tqdm(samples.items()):\n",
    "        methods = sorted(masks)\n",
    "        for left, right in combinations(methods, 2):\n",
    "            mask_l = vizu_normalize(masks[left], method=METHOD_VIZU)\n",
    "            mask_r = vizu_normalize(masks[right], method=METHOD_VIZU)\n",
    "\n",
    "            mask_l = cv2.resize(mask_l, (512,512))\n",
    "            mask_r = cv2.resize(mask_r, (512,512))\n",
    "            \n",
    "            if mask_l.shape != mask_r.shape:\n",
    "                raise ValueError(f\"Shape mismatch on '{sample_id}' between {left} and {right}.\")\n",
    "            \n",
    "            agreement_score_pixel = feature_agreement(get_top_k_pixels(mask_l, k_pixel),\n",
    "                                                      get_top_k_pixels(mask_r, k_pixel))\n",
    "            \n",
    "            with open(os.path.join(path_mesh, sample_id+\".pkl\"), 'rb') as f:\n",
    "                    mesh = np.array(pickle.load(f))\n",
    "            \n",
    "            regions = create_face_regions_masks(mesh)\n",
    "            regions = merge_symmetric_masks(regions)\n",
    "\n",
    "            xai_score_l = calculate_xai_score(mask_l, regions, sort=True)\n",
    "            xai_score_r = calculate_xai_score(mask_r, regions, sort=True)\n",
    "\n",
    "            agreement_score_region, agreed_regions = calculate_region_agreement(\n",
    "                xai_score_l, xai_score_r, k_region, return_agreed_set=True\n",
    "            )\n",
    "\n",
    "            rows.append(\n",
    "                dict(\n",
    "                    sample=sample_id,\n",
    "                    method_a=left,\n",
    "                    method_b=right,\n",
    "                    pixel=agreement_score_pixel,\n",
    "                    region=agreement_score_region,\n",
    "                    agreed_regions=sorted(agreed_regions)   # or ';'.join(sorted(agreed_regions))\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def extract_top_regions(\n",
    "    samples,\n",
    "    top_k=3,\n",
    "    mask_size=(512, 512),\n",
    "    landmark_dir=r\"Datasets\\DatasetFaces\\Landmarks\",\n",
    "):\n",
    "    rows = []\n",
    "    for sample_id, method_masks in tqdm(samples.items(), desc=\"Samples\"):\n",
    "        mesh_path = Path(landmark_dir) / f\"{sample_id}.pkl\"\n",
    "        if not mesh_path.exists():\n",
    "            print(f\"Skip {sample_id} (missing landmarks at {mesh_path})\")\n",
    "            continue\n",
    "\n",
    "        with open(mesh_path, \"rb\") as f:\n",
    "            mesh = np.array(pickle.load(f))\n",
    "\n",
    "        regions = merge_symmetric_masks(create_face_regions_masks(mesh))\n",
    "\n",
    "        for method, mask in method_masks.items():\n",
    "\n",
    "            mask = np.nan_to_num(mask, copy=False)\n",
    "            mask = vizu_normalize(mask, method=METHOD_VIZU)\n",
    "            mask_resized = cv2.resize(mask, mask_size)\n",
    "            region_scores = calculate_xai_score(mask_resized, regions, sort=True)\n",
    "            top_regions = [\n",
    "                region\n",
    "                for region, score in sorted(region_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "                if score > 0\n",
    "            ][:top_k]\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"sample\": sample_id,\n",
    "                    \"explainer\": method,\n",
    "                    \"regions\": top_regions,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def summarize_pairwise_pixel(agreements: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg = (\n",
    "        agreements\n",
    "        .groupby([\"method_a\", \"method_b\"])[\"pixel\"]\n",
    "        .agg(mean_pixel=\"mean\", std_pixel=\"std\", n=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"mean_pixel\", ascending=False)\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "\n",
    "def summarize_pairwise_region(agreements: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg = (\n",
    "        agreements\n",
    "        .groupby([\"method_a\", \"method_b\"])[\"region\"]\n",
    "        .agg(mean_region=\"mean\", std_region=\"std\", n=\"count\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"mean_region\", ascending=False)\n",
    "    )\n",
    "    return agg\n",
    "\n",
    "def aggregate_top_masks_per_sample(samples, k_percent=10):\n",
    "    \"\"\"Return per-sample heatmaps counting how many explainers flagged each pixel.\"\"\"\n",
    "    aggregated = {}\n",
    "    for sample_id, method_masks in samples.items():\n",
    "        stack = []\n",
    "        for mask in method_masks.values():\n",
    "            mask = np.nan_to_num(mask, copy=False)\n",
    "            mask = vizu_normalize(mask, method=METHOD_VIZU)\n",
    "            stack.append(get_top_k_pixels(mask, k_percent))\n",
    "        if stack:\n",
    "            aggregated[sample_id] = np.stack(stack).mean(axis=0)  # (H, W) agreement count\n",
    "    return aggregated\n",
    "\n",
    "def aggregate_region_scores(samples, mask_size=(512, 512),\n",
    "                            landmark_dir=Path(r\"Datasets\\DatasetFaces\\Landmarks\"), sort=True):\n",
    "    \"\"\"Return per-sample dictionaries giving average importance per face region across all explainers.\"\"\"\n",
    "    output = {}\n",
    "    for sample_id, method_masks in samples.items():\n",
    "        mesh_path = landmark_dir / f\"{sample_id}.pkl\"\n",
    "        if not mesh_path.exists():\n",
    "            continue\n",
    "\n",
    "        with mesh_path.open(\"rb\") as fh:\n",
    "            mesh = np.array(pickle.load(fh))\n",
    "\n",
    "        regions = merge_symmetric_masks(create_face_regions_masks(mesh))\n",
    "        region_accumulator = {name: 0.0 for name in regions}\n",
    "        region_counts = {name: 0 for name in regions}\n",
    "\n",
    "        for mask in method_masks.values():\n",
    "            mask = np.nan_to_num(mask, copy=False)\n",
    "            mask = vizu_normalize(mask, method=METHOD_VIZU)\n",
    "            mask_resized = cv2.resize(mask, mask_size)\n",
    "            region_scores = calculate_xai_score(mask_resized, regions, sort=False)\n",
    "            for region_name, score in region_scores.items():\n",
    "                region_accumulator[region_name] += score\n",
    "                region_counts[region_name] += 1\n",
    "\n",
    "        averaged_scores = {\n",
    "            region: region_accumulator[region] / region_counts[region]\n",
    "            for region in regions if region_counts[region] > 0\n",
    "        }\n",
    "\n",
    "        if sort:\n",
    "            averaged_scores = dict(sorted(averaged_scores.items(),\n",
    "                                          key=lambda kv: kv[1], reverse=True))\n",
    "        output[sample_id] = averaged_scores\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622be76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_pixel = 10\n",
    "k_region = 3\n",
    "\n",
    "path = f\"RGU\\\\XAI_EXPLAINERS\\\\agreement_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}_kpixel_{k_pixel}_kregion_{k_region}.csv\"\n",
    "\n",
    "if Path(path).exists():\n",
    "    agreements = pd.read_csv(path)\n",
    "else:\n",
    "    agreements = pairwise_agreement(samples, k_pixel=k_pixel, k_region=k_region)\n",
    "    agreements.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fc38d",
   "metadata": {},
   "source": [
    "## Top regions by explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_regions_df = extract_top_regions(samples, top_k=k_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_order = [\n",
    "    \"Saliency\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"DeepLift\",\n",
    "    \"DeepLiftShap\",\n",
    "    \"GradientShap\",\n",
    "    \"GradCAM\",\n",
    "    \"GuidedGradCAM\",\n",
    "    \"Deconvolution\",\n",
    "    \"Occlusion\",\n",
    "    \"Lime\",\n",
    "    \"EdgeDetector\",\n",
    "    \"Noise\",\n",
    "    \"RANDOM_Saliency\",\n",
    "    \"RANDOM_IntegratedGradients\",\n",
    "    \"RANDOM_DeepLift\",\n",
    "    \"RANDOM_DeepLiftShap\",\n",
    "    \"RANDOM_GradientShap\",\n",
    "    \"RANDOM_GradCAM\",\n",
    "    \"RANDOM_GuidedGradCAM\",\n",
    "    \"RANDOM_Deconvolution\",\n",
    "    \"RANDOM_Occlusion\",\n",
    "    \"RANDOM_Lime\",\n",
    "]\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for explainer, group in top_regions_df.groupby(\"explainer\"):\n",
    "    counter = Counter()\n",
    "    for regions in group[\"regions\"]:\n",
    "        counter.update(regions)\n",
    "    counts[explainer] = counter\n",
    "\n",
    "if not counts:\n",
    "    raise RuntimeError(\"No regions found to plot.\")\n",
    "\n",
    "all_regions = sorted({region for counter in counts.values() for region in counter})\n",
    "\n",
    "heatmap_data = pd.DataFrame(\n",
    "    {explainer: [counts[explainer].get(region, 0) for region in all_regions]\n",
    "     for explainer in explainer_order},\n",
    "    index=all_regions,\n",
    ")\n",
    "\n",
    "percent_data = heatmap_data.div(heatmap_data.sum(axis=0), axis=1) * 100\n",
    "percent_data = percent_data.fillna(0.0)\n",
    "percent_data = percent_data.reindex(columns=explainer_order)\n",
    "\n",
    "plt.figure(figsize=(0.6 * len(percent_data.columns) + 4,\n",
    "                    0.5 * len(percent_data.index) + 3))\n",
    "\n",
    "\n",
    "sns.heatmap(\n",
    "    percent_data,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    cbar_kws={\"label\": \"Percentage (%)\"},\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"white\",\n",
    "    cbar=False,\n",
    "    square=True,\n",
    ")\n",
    "plt.title(\"Facial Region Frequency per Explainer (Percentage)\")\n",
    "plt.xlabel(\"Explainer\")\n",
    "plt.ylabel(\"Facial Region\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(f\"RGU\\\\XAI_EXPLAINERS\\\\imgs\\\\region_frequency_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}_kpixel_{k_pixel}_kregion_{k_region}.pdf\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a7178",
   "metadata": {},
   "source": [
    "## Pixel Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_pairwise_pixel(agreements)\n",
    "\n",
    "order = [\n",
    "    \"Saliency\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"DeepLift\",\n",
    "    \"DeepLiftShap\",\n",
    "    \"GradientShap\",\n",
    "    \"GradCAM\",\n",
    "    \"GuidedGradCAM\",\n",
    "    \"Deconvolution\",\n",
    "    \"Occlusion\",\n",
    "    \"Lime\",\n",
    "    \"EdgeDetector\",\n",
    "    \"Noise\",\n",
    "    \"RANDOM_Saliency\",\n",
    "    \"RANDOM_IntegratedGradients\",\n",
    "    \"RANDOM_DeepLift\",\n",
    "    \"RANDOM_DeepLiftShap\",\n",
    "    \"RANDOM_GradientShap\",\n",
    "    \"RANDOM_GradCAM\",\n",
    "    \"RANDOM_GuidedGradCAM\",\n",
    "    \"RANDOM_Deconvolution\",\n",
    "    \"RANDOM_Occlusion\",\n",
    "    \"RANDOM_Lime\",\n",
    "]\n",
    "\n",
    "present = set(summary[\"method_a\"]).union(summary[\"method_b\"])\n",
    "ordered = [m for m in order if m in present]\n",
    "extras = sorted(present.difference(ordered))\n",
    "labels = ordered + extras  # append any unexpected explainers\n",
    "\n",
    "matrix = (\n",
    "    summary.pivot(index=\"method_a\", columns=\"method_b\", values=\"mean_pixel\")\n",
    "           .reindex(index=labels, columns=labels)\n",
    ")\n",
    "\n",
    "matrix = matrix.combine_first(matrix.T)\n",
    "np.fill_diagonal(matrix.values, 1.0)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(matrix, vmin=0, vmax=1, cmap=\"RdBu_r\", annot=True, fmt=\".2f\",\n",
    "            cbar=False, linewidths=0.5, linecolor=\"white\", square=True)\n",
    "plt.title(\"Mean Feature Agreement Between Explainers\")\n",
    "plt.ylabel(\"Explainer A\")\n",
    "plt.xlabel(\"Explainer B\")\n",
    "plt.savefig(Path(f\"RGU\\XAI_EXPLAINERS\\imgs\\pixel_agreement_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}_kpixel_{k_pixel}_kregion_{k_region}.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ffaaf9",
   "metadata": {},
   "source": [
    "### Overall Agreed Pixel by Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0d9b8",
   "metadata": {},
   "source": [
    "### Overall Agreed Pixels by All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74321abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_heatmaps = aggregate_top_masks_per_sample(samples)\n",
    "\n",
    "def average_heatmap(aggregated_masks, target_label=None):\n",
    "    \"\"\"Mean of per-explainer heatmaps; optionally filter by suffix label.\"\"\"\n",
    "    selected = []\n",
    "    for sample_id, heatmap in aggregated_masks.items():\n",
    "        label = sample_id.rsplit(\"_\", 1)[-1].lower()\n",
    "\n",
    "        if label == \"pain\":\n",
    "            parsed_label = \"pain\"\n",
    "        elif label == \"nopain\":\n",
    "            parsed_label = \"nopain\"\n",
    "        else:\n",
    "            continue  # skip IDs without a known suffix\n",
    "\n",
    "        if target_label and parsed_label != target_label:\n",
    "            continue\n",
    "\n",
    "        selected.append(heatmap.astype(np.float32))\n",
    "\n",
    "    if not selected:\n",
    "        return None\n",
    "    return np.mean(selected, axis=0)\n",
    "\n",
    "overall_heatmap = average_heatmap(pixel_heatmaps)\n",
    "pain_heatmap = average_heatmap(pixel_heatmaps, target_label=\"pain\")\n",
    "no_pain_heatmap = average_heatmap(pixel_heatmaps, target_label=\"nopain\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (heatmap, title) in enumerate(zip(\n",
    "    [overall_heatmap, pain_heatmap, no_pain_heatmap],\n",
    "    [\"Overall Average Heatmap\", \"Pain Average Heatmap\", \"No Pain Average Heatmap\"],\n",
    ")):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    if heatmap is not None:\n",
    "        plt.imshow(heatmap, cmap=\"jet\")\n",
    "        plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef81ef8",
   "metadata": {},
   "source": [
    "## Region Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_regions = summarize_pairwise_region(agreements)\n",
    "\n",
    "order = [\n",
    "    \"Saliency\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"DeepLift\",\n",
    "    \"DeepLiftShap\",\n",
    "    \"GradientShap\",\n",
    "    \"GradCAM\",\n",
    "    \"GuidedGradCAM\",\n",
    "    \"Deconvolution\",\n",
    "    \"Occlusion\",\n",
    "    \"Lime\",\n",
    "    \"EdgeDetector\",\n",
    "    \"Noise\",\n",
    "    \"RANDOM_Saliency\",\n",
    "    \"RANDOM_IntegratedGradients\",\n",
    "    \"RANDOM_DeepLift\",\n",
    "    \"RANDOM_DeepLiftShap\",\n",
    "    \"RANDOM_GradientShap\",\n",
    "    \"RANDOM_GradCAM\",\n",
    "    \"RANDOM_GuidedGradCAM\",\n",
    "    \"RANDOM_Deconvolution\",\n",
    "    \"RANDOM_Occlusion\",\n",
    "    \"RANDOM_Lime\",\n",
    "]\n",
    "\n",
    "present = set(summary_regions[\"method_a\"]).union(summary_regions[\"method_b\"])\n",
    "ordered = [m for m in order if m in present]\n",
    "extras = sorted(present.difference(ordered))\n",
    "labels = ordered + extras  # append any unexpected explainers\n",
    "\n",
    "matrix = (\n",
    "    summary_regions.pivot(index=\"method_a\", columns=\"method_b\", values=\"mean_region\")\n",
    "           .reindex(index=labels, columns=labels)\n",
    ")\n",
    "\n",
    "matrix = matrix.combine_first(matrix.T)\n",
    "np.fill_diagonal(matrix.values, 1.0)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(matrix, vmin=0, vmax=1, cmap=\"RdBu_r\", annot=True, fmt=\".2f\",\n",
    "            cbar=False, linewidths=0.5, linecolor=\"white\", square=True)\n",
    "plt.title(\"Mean Region Agreement Between Explainers\")\n",
    "plt.ylabel(\"Explainer A\")\n",
    "plt.xlabel(\"Explainer B\")\n",
    "plt.savefig(Path(rf\"RGU\\XAI_EXPLAINERS\\imgs\\region_agreement_{MODEL_NAME_AUX}_align_{ALIGN}_method_{METHOD_VIZU}_kpixel_{k_pixel}_kregion_{k_region}.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e572183",
   "metadata": {},
   "source": [
    "### Overall Agreed Regions by Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f460a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse stringified lists\n",
    "#agreements[\"agreed_regions\"] = agreements[\"agreed_regions\"].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Expand regions into long format\n",
    "exploded = agreements.explode(\"agreed_regions\")\n",
    "\n",
    "# Count frequencies per explainer pair + region\n",
    "region_counts = (\n",
    "    exploded.groupby([\"method_a\", \"method_b\", \"agreed_regions\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Pivot into matrix (rows=pairs, cols=regions)\n",
    "heatmap_data = region_counts.pivot_table(\n",
    "    index=[\"method_a\", \"method_b\"],\n",
    "    columns=\"agreed_regions\",\n",
    "    values=\"count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# ---- Convert to percentage (row-normalized) ----\n",
    "heatmap_data_pct = heatmap_data.div(heatmap_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# ---- Heatmap ----\n",
    "plt.figure(figsize=(50,50))\n",
    "sns.heatmap(heatmap_data_pct, annot=True, cmap=\"RdBu_r\", fmt=\".1f\", cbar=False)\n",
    "plt.title(\"Region Agreement (%) per Explainer Pair\")\n",
    "plt.ylabel(\"Explainer Pair\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790edf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_regions(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x) or x.strip() == \"\" or x.strip() == \"[]\":\n",
    "        return []\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "agreements[\"agreed_regions\"] = agreements[\"agreed_regions\"].apply(parse_regions)\n",
    "\n",
    "agreements[\"condition\"] = (\n",
    "    agreements[\"sample\"]\n",
    "    .str.extract(r\"(nopain|pain)\", expand=False)\n",
    "    .map({\"pain\": \"Pain\", \"nopain\": \"No Pain\"})\n",
    ")\n",
    "\n",
    "exploded = agreements.explode(\"agreed_regions\").dropna(subset=[\"agreed_regions\", \"condition\"])\n",
    "\n",
    "condition_counts = (\n",
    "    exploded.groupby([\"condition\", \"agreed_regions\"])\n",
    "    .size()\n",
    "    .rename(\"count\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "condition_counts[\"percent\"] = (\n",
    "    condition_counts[\"count\"]\n",
    "    / condition_counts.groupby(\"condition\")[\"count\"].transform(\"sum\")\n",
    "    * 100\n",
    ").round(2)\n",
    "\n",
    "pivot = (\n",
    "    condition_counts\n",
    "    .pivot(index=\"agreed_regions\", columns=\"condition\", values=\"percent\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "pivot[\"Total\"] = pivot.sum(axis=1)\n",
    "pivot = pivot.sort_values(\"Total\", ascending=True)\n",
    "pivot = pivot.drop(columns=\"Total\")\n",
    "\n",
    "top_n = 15  # change or remove this line to plot all regions\n",
    "plot_data = pivot.head(top_n)\n",
    "\n",
    "plot_data.plot(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    figsize=(10, max(5, 0.4 * len(plot_data))),\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Percentage of agreements (%)\")\n",
    "plt.ylabel(\"Region\")\n",
    "plt.title(\"Agreed Regions Across Explainer Pairs (stacked Pain vs No Pain)\")\n",
    "plt.legend(title=\"Class\", loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb80aa",
   "metadata": {},
   "source": [
    "### Overall Agreed Regions by All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac372982",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_scores = aggregate_region_scores(samples)\n",
    "\n",
    "def region_summary(region_scores):\n",
    "    \"\"\"Average region importance overall and per class using ID suffixes.\"\"\"\n",
    "    rows = []\n",
    "    for sample_id, regions in region_scores.items():\n",
    "        label = sample_id.rsplit(\"_\", 1)[-1].lower()\n",
    "        if label == \"pain\":\n",
    "            parsed_label = \"pain\"\n",
    "        elif label == \"nopain\":\n",
    "            parsed_label = \"nopain\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for region, score in regions.items():\n",
    "            rows.append(\n",
    "                {\"sample\": sample_id, \"label\": parsed_label, \"region\": region, \"score\": score}\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    overall = df.groupby(\"region\")[\"score\"].mean().sort_values(ascending=False)\n",
    "    per_label = df.groupby([\"label\", \"region\"])[\"score\"].mean().unstack(0).sort_values(\n",
    "        by=\"pain\", ascending=False\n",
    "    )\n",
    "    return overall, per_label\n",
    "\n",
    "overall_region_mean, per_label_region_mean = region_summary(region_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b365e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e7c8",
   "metadata": {},
   "source": [
    "# 7. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36a1ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64247c2",
   "metadata": {},
   "source": [
    "# 8. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cdfd4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
