{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d576b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from models import *\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from captum.attr import visualization as viz\n",
    "from dataloaders import presets\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utils.utils import create_folder, load_config\n",
    "from dataloaders.presets import PresetTransform\n",
    "from captum.attr import *\n",
    "\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "from captum.metrics import infidelity, sensitivity_max\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e3449",
   "metadata": {},
   "source": [
    "# Load Folds Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_name = 'ViT_B_32'\n",
    "\n",
    "path_experiments = f'experiments\\\\{model_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e2c8b",
   "metadata": {},
   "source": [
    "# Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_fn(inputs):\n",
    "    noise = torch.tensor(np.random.normal(0, 0.1, inputs.shape)).float().to(inputs.device)\n",
    "    return noise, torch.clip(inputs - noise, 0, 1)\n",
    "\n",
    "def rgb_to_gray_and_scale(x):\n",
    "    x = np.asarray(x)\n",
    "    # Shape must be in (H, W, C)\n",
    "    x_combined = np.sum(x, axis=2)\n",
    "\n",
    "    sorted_vals = np.sort(np.abs(x_combined).flatten())\n",
    "    cum_sums = np.cumsum(sorted_vals)\n",
    "    threshold_id: int = np.where(cum_sums >= cum_sums[-1] * 0.01 * 98)[0][0]\n",
    "    threshold = sorted_vals[threshold_id]\n",
    "\n",
    "    attr_norm = x_combined / threshold\n",
    "    \n",
    "    return np.clip(attr_norm, -1, 1)\n",
    "\n",
    "# --- Create superpixel feature mask for Captum ---\n",
    "def make_feature_mask(img_tensor, n_segments=100):\n",
    "    x = img_tensor.detach().cpu().squeeze(0)  # 3 x H x W\n",
    "    x_np = x.numpy()\n",
    "    x_np = (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-8)\n",
    "    x_np = np.transpose(x_np, (1, 2, 0))  # H, W, 3\n",
    "\n",
    "    seg = slic(x_np, n_segments=n_segments, compactness=10.0, sigma=0.0,\n",
    "               start_label=0, channel_axis=2)\n",
    "\n",
    "    seg_t = torch.from_numpy(seg).long().unsqueeze(0).unsqueeze(0)\n",
    "    return seg_t, seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958291f",
   "metadata": {},
   "source": [
    "# Run for each Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data ={'img_path': [], 'fold': [], 'label': [], \n",
    "           'prediction': [], 'probability': [],\n",
    "           'sensitivity': [], 'infidelity': [],\n",
    "           'mask_path': []}\n",
    "\n",
    "\n",
    "for exp in os.listdir(path_experiments):\n",
    "    # Skip non-experiment folders/files.\n",
    "    if any(ext in exp for ext in ['.pkl', 'masks', '.png', '.pdf']):\n",
    "        continue\n",
    "\n",
    "    # Load model and configuration.\n",
    "    path_model = os.path.join(path_experiments, exp, 'Model', 'best_model.pt')\n",
    "    path_yaml  = os.path.join(path_experiments, exp, 'Model', 'config.yaml')\n",
    "    config     = load_config(path_yaml)\n",
    "    test_path  = config['path_test']\n",
    "\n",
    "    # Set up the model, transforms, and attribution objects based on experiment type.\n",
    "    if \"NCNN\" in exp:\n",
    "        model = NCNN().to(device)\n",
    "        img_size = 120\n",
    "        transform = PresetTransform(\"NCNN\").transforms\n",
    "        layer =  model.merge_branch[0]\n",
    "    elif \"VGGFace\" in exp:\n",
    "        model = VGGFace().to(device)\n",
    "        img_size = 224\n",
    "        transform = PresetTransform(\"VGGFace\").transforms\n",
    "        layer = model.VGGFace.features.conv5_3\n",
    "    elif \"ViT\" in exp:\n",
    "        model = ViT().to(device)\n",
    "        img_size = 224\n",
    "        transform = PresetTransform(\"ViT\").transforms\n",
    "        layer = model.ViT.conv_proj\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Load the trained weights and set the model to evaluation mode.\n",
    "    model.load_state_dict(torch.load(path_model))\n",
    "    model.eval()\n",
    "\n",
    "    # Get list of test images.\n",
    "    image_files = [f for f in os.listdir(test_path) if f.endswith('.jpg')]\n",
    "\n",
    "    for image_file in tqdm(image_files):\n",
    "        full_img_path = os.path.join(test_path, image_file)\n",
    "\n",
    "        img_rgb = Image.open(os.path.join(full_img_path)).convert(\"RGB\")\n",
    "        img_rgb = img_rgb.resize((img_size, img_size))\n",
    "        \n",
    "        img_name = image_file.split(\".jpg\")[0]\n",
    "        # Extract label from the filename.\n",
    "        label = 1 if image_file.split(\".jpg\")[0].split(\"_\")[3] == 'pain' else 0\n",
    "\n",
    "        if \"VGGFace\" in exp:\n",
    "            img_input = Image.fromarray(np.array(img_rgb)[:, :, ::-1])\n",
    "        else:\n",
    "            img_input = img_rgb\n",
    "\n",
    "        blurred_image = img_input.filter(ImageFilter.GaussianBlur(radius=5))\n",
    "\n",
    "        transformed = transform(img_input)\n",
    "        transformed_blurred = transform(blurred_image)\n",
    "\n",
    "        input_tensor = transformed.unsqueeze(0).to(device)\n",
    "        input_tensor_blurred = transformed_blurred.unsqueeze(0).to(device)\n",
    "\n",
    "        XAI_name = \"LIME\"  # Change as needed\n",
    "\n",
    "        create_folder(os.path.join(\"RGU\", model_name, XAI_name))\n",
    "\n",
    "        ## IntegratedGradients\n",
    "        #attribution_method = IntegratedGradients(model)\n",
    "        #kwargs = {'internal_batch_size':10}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## Saliency\n",
    "        #attribution_method = Saliency(model)\n",
    "        #kwargs = {}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## DeepLift\n",
    "        #attribution_method = DeepLift(model)\n",
    "        #kwargs = {'baselines': input_tensor_blurred}\n",
    "        #input_tensor = input_tensor.contiguous()\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## Occlusion\n",
    "        #attribution_method = Occlusion(model)\n",
    "        #kwargs = {'sliding_window_shapes':(3,20,20),'strides': (3,10,10)}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "\n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## GradCAM\n",
    "        #attribution_method = LayerGradCam(model,layer)\n",
    "        #kwargs = {}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        #attributions = LayerAttribution.interpolate(attributions, (img_size, img_size), interpolate_mode=\"bilinear\")\n",
    "        #attributions = attributions.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## GuidedGradCAM\n",
    "        #attribution_method = GuidedGradCam(model, layer)\n",
    "        #kwargs = {}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## Deconvolution\n",
    "        #attribution_method = Deconvolution(model)\n",
    "        #kwargs = {}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## GradientSHAP\n",
    "        #attribution_method = GradientShap(model)\n",
    "        #kwargs = {'baselines': torch.zeros_like(input_tensor).to(device), 'n_samples': 5, 'stdevs': 0.1}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## DeepLiftSHAP\n",
    "        #attribution_method = DeepLiftShap(model)\n",
    "        #kwargs = {'baselines': input_tensor_blurred.repeat(3,1,1,1)}\n",
    "        #attributions = attribution_method.attribute(input_tensor, **kwargs)\n",
    "\n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        ## LIME\n",
    "        feature_mask_t, _ = make_feature_mask(input_tensor, n_segments=100)\n",
    "        feature_mask_t = feature_mask_t.to(device).contiguous()\n",
    "        input_tensor = input_tensor.contiguous()\n",
    "        attribution_method = Lime(model)\n",
    "        kwargs = {'baselines': torch.zeros_like(input_tensor).to(device),\n",
    "                  'feature_mask': feature_mask_t.to(device), 'n_samples': 1000,\n",
    "                  'perturbations_per_eval': 64, 'show_progress': False}\n",
    "        attributions = attribution_method.attribute(input_tensor, **kwargs).contiguous()\n",
    "        \n",
    "        # ------------------------------------------------------------ #\n",
    "\n",
    "        # Compute sensitivity and infidelity metrics.\n",
    "        sens = sensitivity_max(attribution_method.attribute, input_tensor, **kwargs)\n",
    "        infid = infidelity(model, perturb_fn, input_tensor, attributions, normalize=True)\n",
    "\n",
    "        # Normalize and save the attributions.\n",
    "        attributions_normalized = rgb_to_gray_and_scale(attributions.squeeze().cpu().detach().numpy().transpose(1,2,0))\n",
    "        output_path = os.path.join(\"RGU\", model_name, XAI_name,img_name+\".npz\")\n",
    "        np.savez_compressed(output_path, mask=attributions_normalized)\n",
    "\n",
    "        # Store all relevant data.\n",
    "        all_data['img_path'].append(full_img_path)\n",
    "        all_data['fold'].append(test_path.split(\"\\\\\")[-2])\n",
    "        all_data['label'].append(label)\n",
    "        all_data['sensitivity'].append(sens.item())\n",
    "        all_data['infidelity'].append(infid.item())\n",
    "        all_data['mask_path'].append(output_path)\n",
    "\n",
    "        probs = model.predict(input_tensor)\n",
    "        pred = (probs >= 0.5).int()\n",
    "        all_data['probability'].append(probs.item())\n",
    "        all_data['prediction'].append(pred.item())\n",
    "\n",
    "dataframe = pd.DataFrame(all_data)\n",
    "dataframe.to_csv(os.path.join(\"RGU\", model_name, f'{XAI_name}.csv'), index=False)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c27fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attributions_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109aa5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(output_path)['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.min(), data.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf70249",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(os.path.join(\"RGU\", model_name, f'{XAI_name}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e594875",
   "metadata": {},
   "source": [
    "# LIME With Facial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea02ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Lime\n",
    "from XAI.metrics import  create_face_regions_masks\n",
    "import pickle\n",
    "from utils import resize_landmarks\n",
    "import cv2\n",
    "import torch\n",
    "from captum.attr import visualization as viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'Datasets/DatasetFaces/Landmarks/{id}.pkl', 'rb') as f:\n",
    "    landmarks = pickle.load(f)\n",
    "\n",
    "test = create_face_regions_masks(landmarks)\n",
    "\n",
    "test['eyes'] = test['right_eye'] + test['left_eye']\n",
    "test['eyebrown'] = test['right_eyebrown'] + test['left_eyebrown']\n",
    "test['nasolabial_fold'] = test['right_nasolabial_fold'] + test['left_nasolabial_fold']\n",
    "test['cheek'] = test['right_cheek'] + test['left_cheek']\n",
    "\n",
    "test.pop('right_eye')\n",
    "test.pop('left_eye')\n",
    "test.pop('right_eyebrown')\n",
    "test.pop('left_eyebrown')\n",
    "test.pop('right_nasolabial_fold')\n",
    "test.pop('left_nasolabial_fold')\n",
    "test.pop('right_cheek')\n",
    "test.pop('left_cheek')\n",
    "test.pop('outside')\n",
    "\n",
    "features = np.zeros((img_size,img_size))\n",
    "\n",
    "for i,mask in enumerate(test):\n",
    "    features[np.where(cv2.resize(test[mask], (img_size,img_size), interpolation=cv2.INTER_NEAREST) == 1)] = i\n",
    "\n",
    "features = np.stack([features] * 3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee955707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_eucl_distance = get_exp_kernel_similarity_function('euclidean', kernel_width=1000)\n",
    "\n",
    "def iter_combinations(*args, **kwargs):\n",
    "    for i in range(2 **  len(test)):\n",
    "        yield torch.tensor([int(d) for d in bin(i)[2:].zfill( len(test))]).unsqueeze(0)\n",
    "\n",
    "\n",
    "lr_lime = Lime(\n",
    "    model, \n",
    "    interpretable_model=SkLearnLasso(alpha=0.08),  # build-in wrapped sklearn Linear Regression\n",
    "    similarity_func=exp_eucl_distance,\n",
    "    perturb_func=iter_combinations\n",
    ")\n",
    "\n",
    "\n",
    "attrs = lr_lime.attribute(\n",
    "    img_transformed,\n",
    "    feature_mask=torch.tensor(features, dtype=torch.int64),\n",
    "    n_samples=2 ** len(test),\n",
    "    perturbations_per_eval=16,\n",
    "    show_progress=True\n",
    ").squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19cc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attr(attr_map):\n",
    "    viz.visualize_image_attr(\n",
    "        attr_map.permute(1, 2, 0).numpy(),  # adjust shape to height, width, channels \n",
    "        method='heat_map',\n",
    "        sign='all',\n",
    "        show_colorbar=True\n",
    "    )\n",
    "    \n",
    "show_attr(attrs.cpu().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ff32c",
   "metadata": {},
   "source": [
    "# RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25827c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XAI import RISE\n",
    "from models import NCNN, VGGFace\n",
    "import torch\n",
    "from dataloaders import presets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "rise = RISE(model, input_size=(224,224), gpu_batch=10, device=device)\n",
    "\n",
    "rise.generate_masks(1000, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rise.masks[0].cpu()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb61791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img_transformed.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b07b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask.cpu())\n",
    "plt.imshow(img.resize((224,224)), alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doutorado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
